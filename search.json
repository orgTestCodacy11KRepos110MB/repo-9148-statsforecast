[
  {
    "objectID": "ets.html",
    "href": "ets.html",
    "title": "ETS Model",
    "section": "",
    "text": "source\n\n\n\n ets_target_fn (par, p_y, p_nstate, p_errortype, p_trendtype,\n                p_seasontype, p_damped, p_lower, p_upper, p_opt_crit,\n                p_nmse, p_bounds, p_m, p_optAlpha, p_optBeta, p_optGamma,\n                p_optPhi, p_givenAlpha, p_givenBeta, p_givenGamma,\n                p_givenPhi, alpha, beta, gamma, phi)\n\n\nsource\n\n\n\n\n is_constant (x)\nGive us a ⭐ on Github"
  },
  {
    "objectID": "theta.html",
    "href": "theta.html",
    "title": "Theta Model",
    "section": "",
    "text": "source\n\n\n\n theta_target_fn (optimal_param, init_level, init_alpha, init_theta,\n                  opt_level, opt_alpha, opt_theta, y, modeltype, nmse)\n\n\nsource\n\n\n\n\n is_constant (x)\n\n\nis_constant(ap)\n\n\nforecast_theta(res, 12, level=[90, 80])\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.multiprocess.html",
    "href": "distributed.multiprocess.html",
    "title": "MultiprocessBackend",
    "section": "",
    "text": "source\n\nMultiprocessBackend\n\n MultiprocessBackend (n_jobs:int)\n\nMultiprocessBackend Parent Class for Distributed Computation.\nParameters: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nNotes:\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "mstl.html",
    "href": "mstl.html",
    "title": "MSTL model",
    "section": "",
    "text": "source\n\nmstl\n\n mstl (x:numpy.ndarray, period:Union[int,List[int]],\n       blambda:Optional[float]=None, iterate:int=1,\n       s_window:Optional[numpy.ndarray]=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\ntime series\n\n\nperiod\ntyping.Union[int, typing.List[int]]\n\nseasom length\n\n\nblambda\ntyping.Optional[float]\nNone\nbox-cox transform\n\n\niterate\nint\n1\nnumber of iterations\n\n\ns_window\ntyping.Optional[numpy.ndarray]\nNone\nseasonal window\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Models",
    "section": "",
    "text": "StatsForecast offers a wide variety of models grouped in the following categories:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "models.html#autoarima",
    "href": "models.html#autoarima",
    "title": "Models",
    "section": "AutoARIMA",
    "text": "AutoARIMA\n\nsource\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=False, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=False, allowmean:bool=False,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            parallel:bool=False, num_cores:int=2, season_length:int=1,\n            alias:str='AutoARIMA')\n\nAutoARIMA model.\nAutomatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Default is Akaike Information Criterion (AICc).\nNote: This implementation is a mirror of Hyndman’s forecast::auto.arima.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd\ntyping.Optional[int]\nNone\nOrder of first-differencing.\n\n\nD\ntyping.Optional[int]\nNone\nOrder of seasonal-differencing.\n\n\nmax_p\nint\n5\nMax autorregresives p.\n\n\nmax_q\nint\n5\nMax moving averages q.\n\n\nmax_P\nint\n2\nMax seasonal autorregresives P.\n\n\nmax_Q\nint\n2\nMax seasonal moving averages Q.\n\n\nmax_order\nint\n5\nMax p+q+P+Q value if not stepwise selection.\n\n\nmax_d\nint\n2\nMax non-seasonal differences.\n\n\nmax_D\nint\n1\nMax seasonal differences.\n\n\nstart_p\nint\n2\nStarting value of p in stepwise procedure.\n\n\nstart_q\nint\n2\nStarting value of q in stepwise procedure.\n\n\nstart_P\nint\n1\nStarting value of P in stepwise procedure.\n\n\nstart_Q\nint\n1\nStarting value of Q in stepwise procedure.\n\n\nstationary\nbool\nFalse\nIf True, restricts search to stationary models.\n\n\nseasonal\nbool\nTrue\nIf False, restricts search to non-seasonal models.\n\n\nic\nstr\naicc\nInformation criterion to be used in model selection.\n\n\nstepwise\nbool\nTrue\nIf True, will do stepwise selection (faster).\n\n\nnmodels\nint\n94\nNumber of models considered in stepwise search.\n\n\ntrace\nbool\nFalse\nIf True, the searched ARIMA models is reported.\n\n\napproximation\ntyping.Optional[bool]\nFalse\nIf True, conditional sums-of-squares estimation, final MLE.\n\n\nmethod\ntyping.Optional[str]\nNone\nFitting method between maximum likelihood or sums-of-squares.\n\n\ntruncate\ntyping.Optional[bool]\nNone\nObservations truncated series used in model selection.\n\n\ntest\nstr\nkpss\nUnit root test to use. See ndiffs for details.\n\n\ntest_kwargs\ntyping.Optional[str]\nNone\nUnit root test additional arguments.\n\n\nseasonal_test\nstr\nseas\nSelection method for seasonal differences.\n\n\nseasonal_test_kwargs\ntyping.Optional[typing.Dict]\nNone\nSeasonal unit root test arguments.\n\n\nallowdrift\nbool\nFalse\nIf True, drift models terms considered.\n\n\nallowmean\nbool\nFalse\nIf True, non-zero mean models considered.\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nparallel\nbool\nFalse\nIf True and stepwise=False, then parallel search.\n\n\nnum_cores\nint\n2\nAmount of parallel processes to be used if parallel=True.\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalias\nstr\nAutoARIMA\nCustom name of the model.\n\n\n\n\nsource\n\n\nAutoARIMA.fit\n\n AutoARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoARIMA model.\nFit an AutoARIMA to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoARIMA fitted model.\n\n\n\n\nsource\n\n\nAutoARIMA.predict\n\n AutoARIMA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted AutoArima.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.predict_in_sample\n\n AutoARIMA.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoArima insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.forecast\n\n AutoARIMA.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoARIMA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.forward\n\n AutoARIMA.forward (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted ARIMA model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoARIMA's usage example\n\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.utils import AirPassengers as ap\n\n\narima = AutoARIMA(season_length=4)\narima = arima.fit(y=ap)\ny_hat_dict = arima.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([497.95290378, 486.7806859 , 500.08214752, 494.10983682]),\n 'lo-80': 0    467.167462\n 1    442.112227\n 2    450.786736\n 3    440.963293\n Name: 80%, dtype: float64,\n 'hi-80': 0    528.738346\n 1    531.449145\n 2    549.377559\n 3    547.256381\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "models.html#autoets",
    "href": "models.html#autoets",
    "title": "Models",
    "section": "AutoETS",
    "text": "AutoETS\n\nsource\n\nAutoETS\n\n AutoETS (season_length:int=1, model:str='ZZZ',\n          damped:Optional[bool]=None, alias:str='AutoETS')\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‘ANN’ (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‘Z’, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nNote: This implementation is a mirror of Hyndman’s forecast::ets.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\nHyndman, Rob, et al (2008). “Forecasting with exponential smoothing: the state space approach”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZZZ\nControlling state-space-equations.\n\n\ndamped\ntyping.Optional[bool]\nNone\nA parameter that ‘dampens’ the trend.\n\n\nalias\nstr\nAutoETS\nCustom name of the model.\n\n\n\n\nsource\n\n\nAutoETS.fit\n\n AutoETS.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nAutoETS.predict\n\n AutoETS.predict (h:int, X:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.predict_in_sample\n\n AutoETS.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.forecast\n\n AutoETS.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.forward\n\n AutoETS.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                  X_future:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoETS' usage example\n\nfrom statsforecast.models import AutoETS\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nautoets = AutoETS(model='ZMZ',  \n              season_length=4)\nautoets = autoets.fit(y=ap)\ny_hat_dict = autoets.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.63294737, 419.65915384, 442.66309931, 457.33314074])}\n\n\n\nsource\n\n\nETS\n\n ETS (season_length:int=1, model:str='ZZZ', damped:Optional[bool]=None,\n      alias:str='ETS')\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‘ANN’ (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‘Z’, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nNote: This implementation is a mirror of Hyndman’s forecast::ets.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\nHyndman, Rob, et al (2008). “Forecasting with exponential smoothing: the state space approach”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZZZ\nControlling state-space-equations.\n\n\ndamped\ntyping.Optional[bool]\nNone\nA parameter that ‘dampens’ the trend.\n\n\nalias\nstr\nETS\nCustom name of the model.\n\n\n\n\nets = ETS(model='ZMZ', season_length=4)"
  },
  {
    "objectID": "models.html#autoces",
    "href": "models.html#autoces",
    "title": "Models",
    "section": "AutoCES",
    "text": "AutoCES\n\nsource\n\nAutoCES\n\n AutoCES (season_length:int=1, model:str='Z', alias:str='CES')\n\nComplex Exponential Smoothing model.\nAutomatically selects the best Complex Exponential Smoothing model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(S\\) simple, \\(P\\) parial, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the kind of CES model: \\(N\\) for simple CES (withous seasonality), \\(S\\) for simple seasonality (lagged CES), \\(P\\) for partial seasonality (without complex part), \\(F\\) for full seasonality (lagged CES with real and complex seasonal parts).\nIf the component is selected as ‘Z’, it operates as a placeholder to ask the AutoCES model to figure out the best parameter.\nReferences: Svetunkov, Ivan & Kourentzes, Nikolaos. (2015). “Complex Exponential Smoothing”. 10.13140/RG.2.1.3757.2562..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZ\nControlling state-space-equations.\n\n\nalias\nstr\nCES\nCustom name of the model.\n\n\n\n\nsource\n\n\nAutoCES.fit\n\n AutoCES.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Complex Exponential Smoothing model.\nFit the Complex Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nComplex Exponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nAutoCES.predict\n\n AutoCES.predict (h:int, X:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.predict_in_sample\n\n AutoCES.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.forecast\n\n AutoCES.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Complex Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.forward\n\n AutoCES.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                  X_future:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Complex Exponential Smoothing to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# CES' usage example\n\nfrom statsforecast.models import AutoCES\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nces = AutoCES(model='Z',  \n              season_length=4)\nces = ces.fit(y=ap)\ny_hat_dict = ces.predict(h=4)\ny_hat_dict\n\n{'mean': array([424.30716324, 405.69589186, 442.02640533, 443.63488996])}"
  },
  {
    "objectID": "models.html#autotheta",
    "href": "models.html#autotheta",
    "title": "Models",
    "section": "AutoTheta",
    "text": "AutoTheta\n\nsource\n\nAutoTheta\n\n AutoTheta (season_length:int=1, decomposition_type:str='multiplicative',\n            model:Optional[str]=None, alias:str='AutoTheta')\n\nAutoTheta model.\nAutomatically selects the best Theta (Standard Theta Model (‘STM’), Optimized Theta Model (‘OTM’), Dynamic Standard Theta Model (‘DSTM’), Dynamic Optimized Theta Model (‘DOTM’)) model using mse.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nmodel\ntyping.Optional[str]\nNone\nControlling Theta Model. By default searchs the best model.\n\n\nalias\nstr\nAutoTheta\nCustom name of the model.\n\n\n\n\nsource\n\n\nAutoTheta.fit\n\n AutoTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nAutoTheta.predict\n\n AutoTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.predict_in_sample\n\n AutoTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.forecast\n\n AutoTheta.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.forward\n\n AutoTheta.forward (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoTheta's usage example\n\nfrom statsforecast.models import AutoTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\ntheta = AutoTheta(season_length=4)\ntheta = theta.fit(y=ap)\ny_hat_dict = theta.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.86262032, 410.60532872, 429.59124482, 440.16565301])}"
  },
  {
    "objectID": "models.html#arima",
    "href": "models.html#arima",
    "title": "Models",
    "section": "ARIMA",
    "text": "ARIMA\n\nsource\n\nARIMA\n\n ARIMA (order:Tuple[int,int,int]=(0, 0, 0), season_length:int=1,\n        seasonal_order:Tuple[int,int,int]=(0, 0, 0),\n        include_mean:bool=True, include_drift:bool=False,\n        include_constant:Optional[bool]=None,\n        blambda:Optional[float]=None, biasadj:bool=False, method:str='CSS-\n        ML', fixed:Optional[dict]=None, alias:str='ARIMA')\n\nARIMA model.\nAutoRegressive Integrated Moving Average model.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norder\ntyping.Tuple[int, int, int]\n(0, 0, 0)\nA specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order.\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nseasonal_order\ntyping.Tuple[int, int, int]\n(0, 0, 0)\nA specification of the seasonal part of the ARIMA model.(P, D, Q) for the AR order, the degree of differencing, the MA order.\n\n\ninclude_mean\nbool\nTrue\nShould the ARIMA model include a mean term? The default is True for undifferenced series, False for differenced ones (where a mean would not affect the fit nor predictions).\n\n\ninclude_drift\nbool\nFalse\nShould the ARIMA model include a linear drift term? (i.e., a linear regression with ARIMA errors is fitted.)\n\n\ninclude_constant\ntyping.Optional[bool]\nNone\nIf True, then includ_mean is set to be True for undifferenced series and include_drift is set to be True for differenced series. Note that if there is more than one difference taken, no constant is included regardless of the value of this argument. This is deliberate as otherwise quadratic and higher order polynomial trends would be induced.\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nmethod\nstr\nCSS-ML\nFitting method: maximum likelihood or minimize conditional sum-of-squares. The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood.\n\n\nfixed\ntyping.Optional[dict]\nNone\nDictionary containing fixed coefficients for the arima model. Example: {'ar1': 0.5, 'ma2': 0.75}.For autoregressive terms use the ar{i} keys. For its seasonal version use sar{i}.For moving average terms use the ma{i} keys. For its seasonal version use sma{i}.For intercept and drift use the intercept and drift keys.For exogenous variables use the ex_{i} keys.\n\n\nalias\nstr\nARIMA\nCustom name of the model.\n\n\n\n\nsource\n\n\nARIMA.fit\n\n ARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nFitted model.\n\n\n\n\nsource\n\n\nARIMA.predict\n\n ARIMA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.predict_in_sample\n\n ARIMA.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.forecast\n\n ARIMA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.forward\n\n ARIMA.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# ARIMA's usage example\n\nfrom statsforecast.models import ARIMA\nfrom statsforecast.utils import AirPassengers as ap\n\n\narima = ARIMA(order=(1, 0, 0), season_length=12)\narima = arima.fit(y=ap)\ny_hat_dict = arima.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([426.54529705, 421.28383474, 416.20876726, 411.31349129]),\n 'lo-80': 0    383.228999\n 1    361.100640\n 2    343.777875\n 3    329.110028\n Name: 80%, dtype: float64,\n 'hi-80': 0    469.861595\n 1    481.467029\n 2    488.639659\n 3    493.516954\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "models.html#autoregressive",
    "href": "models.html#autoregressive",
    "title": "Models",
    "section": "AutoRegressive",
    "text": "AutoRegressive\n\nsource\n\nAutoRegressive\n\n AutoRegressive (lags:Tuple[int,List], include_mean:bool=True,\n                 include_drift:bool=False, blambda:Optional[float]=None,\n                 biasadj:bool=False, method:str='CSS-ML',\n                 fixed:Optional[dict]=None, alias:str='AutoRegressive')\n\nSimple Autoregressive model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlags\ntyping.Tuple[int, typing.List]\n\nNumber of lags to include in the model. If an int is passed then all lags up to lags are considered.If a list, only the elements of the list are considered as lags.\n\n\ninclude_mean\nbool\nTrue\nShould the AutoRegressive model include a mean term? The default is True for undifferenced series, False for differenced ones (where a mean would not affect the fit nor predictions).\n\n\ninclude_drift\nbool\nFalse\nShould the AutoRegressive model include a linear drift term? (i.e., a linear regression with AutoRegressive errors is fitted.)\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nmethod\nstr\nCSS-ML\nFitting method: maximum likelihood or minimize conditional sum-of-squares. The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood.\n\n\nfixed\ntyping.Optional[dict]\nNone\nDictionary containing fixed coefficients for the AutoRegressive model. Example: {'ar1': 0.5, 'ar5': 0.75}.For autoregressive terms use the ar{i} keys.\n\n\nalias\nstr\nAutoRegressive\nCustom name of the model.\n\n\n\n\nsource\n\n\nAutoRegressive.fit\n\n AutoRegressive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nFitted model.\n\n\n\n\nsource\n\n\nAutoRegressive.predict\n\n AutoRegressive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None)\n\nPredict with fitted model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.predict_in_sample\n\n AutoRegressive.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.forecast\n\n AutoRegressive.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.forward\n\n AutoRegressive.forward (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nApply fitted model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoRegressive's usage example\n\nfrom statsforecast.models import AutoRegressive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nar = AutoRegressive(lags=[12])\nar = ar.fit(y=ap)\ny_hat_dict = ar.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([460.01874698, 432.12629561, 462.16432016, 507.22135699]),\n 'lo-80': 0    439.539875\n 1    411.647423\n 2    441.685448\n 3    486.742485\n Name: 80%, dtype: float64,\n 'hi-80': 0    480.497619\n 1    452.605168\n 2    482.643192\n 3    527.700229\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "models.html#simplesmooth",
    "href": "models.html#simplesmooth",
    "title": "Models",
    "section": "SimpleSmooth",
    "text": "SimpleSmooth\n\nsource\n\nSimpleExponentialSmoothing\n\n SimpleExponentialSmoothing (alpha:float, alias:str='SES')\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe rate \\(0 \\leq \\alpha \\leq 1\\) at which the weights decrease is called the smoothing parameter. When \\(\\alpha = 1\\), SES is equal to the naive method.\nReferences: Charles C Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha\nfloat\n\nSmoothing parameter.\n\n\nalias\nstr\nSES\nCustom name of the model.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.forecast\n\n SimpleExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                      X:Optional[numpy.ndarray]=None, X_fu\n                                      ture:Optional[numpy.ndarray]=None,\n                                      fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.fit\n\n SimpleExponentialSmoothing.fit (y:numpy.ndarray,\n                                 X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothing model.\nFit an SimpleExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSimpleExponentialSmoothing fitted model.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.predict\n\n SimpleExponentialSmoothing.predict (h:int,\n                                     X:Optional[numpy.ndarray]=None)\n\nPredict with fitted SimpleExponentialSmoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.predict_in_sample\n\n SimpleExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothing insample predictions.\n\n# SimpleExponentialSmoothing's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nses = SimpleExponentialSmoothing(alpha=0.5)\nses = ses.fit(y=ap)\ny_hat_dict = ses.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "models.html#simplesmoothoptimized",
    "href": "models.html#simplesmoothoptimized",
    "title": "Models",
    "section": "SimpleSmoothOptimized",
    "text": "SimpleSmoothOptimized\n\nsource\n\nSimpleExponentialSmoothingOptimized\n\n SimpleExponentialSmoothingOptimized (alias:str='SESOpt')\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nReferences: Charles C Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.fit\n\n SimpleExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                          X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothingOptimized model.\nFit an SimpleExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSimpleExponentialSmoothingOptimized fitted model.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict\n\n SimpleExponentialSmoothingOptimized.predict (h:int,\n                                              X:Optional[numpy.ndarray]=No\n                                              ne)\n\nPredict with fitted SimpleExponentialSmoothingOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict_in_sample\n\n SimpleExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothingOptimized insample predictions.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.forecast\n\n SimpleExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                               X:Optional[numpy.ndarray]=N\n                                               one, X_future:Optional[nump\n                                               y.ndarray]=None,\n                                               fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SimpleExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nseso = SimpleExponentialSmoothingOptimized()\nseso = seso.fit(y=ap)\ny_hat_dict = seso.predict(h=4)\ny_hat_dict\n\n{'mean': array([431.58716, 431.58716, 431.58716, 431.58716], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalsmooth",
    "href": "models.html#seasonalsmooth",
    "title": "Models",
    "section": "SeasonalSmooth",
    "text": "SeasonalSmooth\n\nsource\n\nSeasonalExponentialSmoothing\n\n SeasonalExponentialSmoothing (season_length:int, alpha:float,\n                               alias:str='SeasonalES')\n\nSeasonalExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nNote: This method is an extremely simplified of Holt-Winter’s method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nReferences: Charles. C. Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”, ONR Research Memorandum, Carnegie Institute of Technology 52..\nPeter R. Winters (1960). “Forecasting sales by exponentially weighted moving averages”. Management Science.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalpha\nfloat\n\nSmoothing parameter.\n\n\nalias\nstr\nSeasonalES\nCustom name of the model.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.fit\n\n SeasonalExponentialSmoothing.fit (y:numpy.ndarray,\n                                   X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalExponentialSmoothing model.\nFit an SeasonalExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalExponentialSmoothing fitted model.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.predict\n\n SeasonalExponentialSmoothing.predict (h:int,\n                                       X:Optional[numpy.ndarray]=None)\n\nPredict with fitted SeasonalExponentialSmoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.predict_in_sample\n\n SeasonalExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothing insample predictions.\n\nsource\n\n\nSeasonalExponentialSmoothing.forecast\n\n SeasonalExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                        X:Optional[numpy.ndarray]=None, X_\n                                        future:Optional[numpy.ndarray]=Non\n                                        e, fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SeasonalExponentialSmoothing's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothing(alpha=0.5, season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([376.28955, 354.71094, 396.02002, 412.06738], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalsmoothoptimized",
    "href": "models.html#seasonalsmoothoptimized",
    "title": "Models",
    "section": "SeasonalSmoothOptimized",
    "text": "SeasonalSmoothOptimized\n\nsource\n\nSeasonalExponentialSmoothingOptimized\n\n SeasonalExponentialSmoothingOptimized (season_length:int,\n                                        alias:str='SeasESOpt')\n\nSeasonalExponentialSmoothingOptimized model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nNote: This method is an extremely simplified of Holt-Winter’s method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nReferences: Charles. C. Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”, ONR Research Memorandum, Carnegie Institute of Technology 52..\nPeter R. Winters (1960). “Forecasting sales by exponentially weighted moving averages”. Management Science.\nParameters season_length : int\nNumber of observations per unit of time. Ex: 24 Hourly data. alias : str Custom name of the model.\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.forecast\n\n SeasonalExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                                 X:Optional[numpy.ndarray]\n                                                 =None, X_future:Optional[\n                                                 numpy.ndarray]=None,\n                                                 fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.fit\n\n SeasonalExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                            X:Optional[numpy.ndarray]=None\n                                            )\n\nFit the SeasonalExponentialSmoothingOptimized model.\nFit an SeasonalExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalExponentialSmoothingOptimized fitted model.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict\n\n SeasonalExponentialSmoothingOptimized.predict (h:int,\n                                                X:Optional[numpy.ndarray]=\n                                                None)\n\nPredict with fitted SeasonalExponentialSmoothingOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict_in_sample\n\n SeasonalExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothingOptimized insample predictions.\n\n# SeasonalExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothingOptimized(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.42798, 390.50757, 418.8656 , 460.3452 ], dtype=float32)}"
  },
  {
    "objectID": "models.html#holts-method",
    "href": "models.html#holts-method",
    "title": "Models",
    "section": "Holt’s method",
    "text": "Holt’s method\n\nsource\n\nHolt\n\n Holt (season_length:int=1, error_type:str='A', alias:str='Holt')\n\nHolt’s method.\nAlso known as double exponential smoothing, Holt’s method is an extension of exponential smoothing for series with a trend. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‘AAN’ or ‘MAN’).\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Methods with trend”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 12 Monthly data.\n\n\nerror_type\nstr\nA\nThe type of error of the ETS model. Can be additive (A) or multiplicative (M).\n\n\nalias\nstr\nHolt\nCustom name of the model.\n\n\n\n\nsource\n\n\nHolt.forecast\n\n Holt.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.fit\n\n Holt.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nHolt.predict\n\n Holt.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.predict_in_sample\n\n Holt.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.forward\n\n Holt.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Holt's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Holt(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.27026573, 436.5445037 , 438.81874167, 441.09297963])}"
  },
  {
    "objectID": "models.html#holt-winters-method",
    "href": "models.html#holt-winters-method",
    "title": "Models",
    "section": "Holt-Winters’ method",
    "text": "Holt-Winters’ method\n\nsource\n\nHoltWinters\n\n HoltWinters (season_length:int=1, error_type:str='A',\n              alias:str='HoltWinters')\n\nHolt-Winters’ method.\nAlso known as triple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‘AAA’ or ‘MAM’).\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Methods with seasonality”.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nseason length\n\n\nerror_type\nstr\nA\nerror type\n\n\nalias\nstr\nHoltWinters\nCustom name of the model.\n\n\n\n\nsource\n\n\nHoltWinters.forecast\n\n HoltWinters.forecast (y:numpy.ndarray, h:int,\n                       X:Optional[numpy.ndarray]=None,\n                       X_future:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.fit\n\n HoltWinters.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nHoltWinters.predict\n\n HoltWinters.predict (h:int, X:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.predict_in_sample\n\n HoltWinters.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.forward\n\n HoltWinters.forward (y:numpy.ndarray, h:int,\n                      X:Optional[numpy.ndarray]=None,\n                      X_future:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Holt-Winters' usage example\n\n#from statsforecast.models import HoltWinters\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HoltWinters(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.4192839 , 414.82970205, 449.80384898, 493.35229084])}"
  },
  {
    "objectID": "models.html#historicaverage",
    "href": "models.html#historicaverage",
    "title": "Models",
    "section": "HistoricAverage",
    "text": "HistoricAverage\n\nsource\n\nHistoricAverage\n\n HistoricAverage (alias:str='HistoricAverage')\n\nHistoricAverage model.\nAlso known as mean method. Uses a simple average of all past observations. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\[ \\hat{y}_{t+1} = \\frac{1}{t} \\sum_{j=1}^t y_j \\]\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Simple Methods”.\n\nsource\n\n\nHistoricAverage.forecast\n\n HistoricAverage.forecast (y:numpy.ndarray, h:int,\n                           X:Optional[numpy.ndarray]=None,\n                           X_future:Optional[numpy.ndarray]=None,\n                           level:Optional[Tuple[int]]=None,\n                           fitted:bool=False)\n\nMemory Efficient HistoricAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHistoricAverage.fit\n\n HistoricAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the HistoricAverage model.\nFit an HistoricAverage to a time series (numpy array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself\n\nHistoricAverage fitted model.\n\n\n\n\nsource\n\n\nHistoricAverage.predict\n\n HistoricAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                          level:Optional[Tuple[int]]=None)\n\nPredict with fitted HistoricAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHistoricAverage.predict_in_sample\n\n HistoricAverage.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted HistoricAverage insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# HistoricAverage's usage example\n\nfrom statsforecast.models import HistoricAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HistoricAverage()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([280.2986, 280.2986, 280.2986, 280.2986], dtype=float32)}"
  },
  {
    "objectID": "models.html#naive",
    "href": "models.html#naive",
    "title": "Models",
    "section": "Naive",
    "text": "Naive\n\nsource\n\nNaive\n\n Naive (alias:str='Naive')\n\nNaive model.\nA random walk model, defined as \\(\\hat{y}_{t+1} = y_t\\) $\forall t$\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\nsource\n\n\nNaive.forecast\n\n Naive.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[Tuple[int]]=None, fitted:bool=False)\n\nMemory Efficient Naive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaive.fit\n\n Naive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Naive model.\nFit an Naive to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nNaive fitted model.\n\n\n\n\nsource\n\n\nNaive.predict\n\n Naive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[Tuple[int]]=None)\n\nPredict with fitted Naive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaive.predict_in_sample\n\n Naive.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Naive insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Naive's usage example\n\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Naive()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([432., 432., 432., 432.], dtype=float32)}"
  },
  {
    "objectID": "models.html#randomwalkwithdrift",
    "href": "models.html#randomwalkwithdrift",
    "title": "Models",
    "section": "RandomWalkWithDrift",
    "text": "RandomWalkWithDrift\n\nsource\n\nRandomWalkWithDrift\n\n RandomWalkWithDrift (alias:str='RWD')\n\nRandomWalkWithDrift model.\nA variation of the naive method allows the forecasts to change over time. The amout of change, called drift, is the average change seen in the historical data.\n\\[ \\hat{y}_{t+1} = y_t+\\frac{1}{t-1}\\sum_{j=1}^t (y_j-y_{j-1}) = y_t+ \\frac{y_t-y_1}{t-1} \\]\nFrom the previous equation, we can see that this is equivalent to extrapolating a line between the first and the last observation.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nRWD\nCustom name of the model.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.forecast\n\n RandomWalkWithDrift.forecast (y:numpy.ndarray, h:int,\n                               X:Optional[numpy.ndarray]=None,\n                               X_future:Optional[numpy.ndarray]=None,\n                               level:Optional[Tuple[int]]=None,\n                               fitted:bool=False)\n\nMemory Efficient RandomWalkWithDrift predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\nforecasts: dict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.fit\n\n RandomWalkWithDrift.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the RandomWalkWithDrift model.\nFit an RandomWalkWithDrift to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nRandomWalkWithDrift fitted model.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.predict\n\n RandomWalkWithDrift.predict (h:int, X:Optional[numpy.ndarray]=None,\n                              level:Optional[Tuple[int]]=None)\n\nPredict with fitted RandomWalkWithDrift.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.predict_in_sample\n\n RandomWalkWithDrift.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted RandomWalkWithDrift insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# RandomWalkWithDrift's usage example\n\nfrom statsforecast.models import RandomWalkWithDrift\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = RandomWalkWithDrift()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.23776, 436.47552, 438.7133 , 440.95105], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalnaive",
    "href": "models.html#seasonalnaive",
    "title": "Models",
    "section": "SeasonalNaive",
    "text": "SeasonalNaive\n\nsource\n\nSeasonalNaive\n\n SeasonalNaive (season_length:int, alias:str='SeasonalNaive')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSeasonalNaive.forecast\n\n SeasonalNaive.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[Tuple[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient SeasonalNaive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalNaive.fit\n\n SeasonalNaive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalNaive model.\nFit an SeasonalNaive to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nSeasonalNaive fitted model.\n\n\n\n\nsource\n\n\nSeasonalNaive.predict\n\n SeasonalNaive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[Tuple[int]]=None)\n\nPredict with fitted Naive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalNaive.predict_in_sample\n\n SeasonalNaive.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted SeasonalNaive insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SeasonalNaive's usage example\n\nfrom statsforecast.models import SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalNaive(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([417., 391., 419., 461.], dtype=float32)}"
  },
  {
    "objectID": "models.html#windowaverage",
    "href": "models.html#windowaverage",
    "title": "Models",
    "section": "WindowAverage",
    "text": "WindowAverage\n\nsource\n\nWindowAverage\n\n WindowAverage (window_size:int, alias:str='WindowAverage')\n\nWindowAverage model.\nUses the average of the last \\(k\\) observations, with \\(k\\) the length of the window. Wider windows will capture global trends, while narrow windows will reveal local trends. The length of the window selected should take into account the importance of past observations and how fast the series changes.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwindow_size\nint\n\nSize of truncated series on which average is estimated.\n\n\nalias\nstr\nWindowAverage\nCustom name of the model.\n\n\n\n\nsource\n\n\nWindowAverage.forecast\n\n WindowAverage.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         fitted:bool=False)\n\nMemory Efficient WindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nWindowAverage.fit\n\n WindowAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the WindowAverage model.\nFit an WindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nWindowAverage fitted model.\n\n\n\n\nsource\n\n\nWindowAverage.predict\n\n WindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted WindowAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nWindowAverage.predict_in_sample\n\n WindowAverage.predict_in_sample ()\n\nAccess fitted WindowAverage insample predictions.\n\n# WindowAverage's usage example\n\nfrom statsforecast.models import WindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = WindowAverage(window_size=12*4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.47916, 413.47916, 413.47916, 413.47916], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalwindowaverage",
    "href": "models.html#seasonalwindowaverage",
    "title": "Models",
    "section": "SeasonalWindowAverage",
    "text": "SeasonalWindowAverage\n\nsource\n\nSeasonalWindowAverage\n\n SeasonalWindowAverage (season_length:int, window_size:int,\n                        alias:str='SeasWA')\n\nSeasonalWindowAverage model.\nAn average of the last \\(k\\) observations of the same period, with \\(k\\) the length of the window.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\n\n\n\nwindow_size\nint\n\nSize of truncated series on which average is estimated.\n\n\nalias\nstr\nSeasWA\nCustom name of the model.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.forecast\n\n SeasonalWindowAverage.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 fitted:bool=False)\n\nMemory Efficient SeasonalWindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.fit\n\n SeasonalWindowAverage.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalWindowAverage model.\nFit an SeasonalWindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalWindowAverage fitted model.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.predict\n\n SeasonalWindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted SeasonalWindowAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.predict_in_sample\n\n SeasonalWindowAverage.predict_in_sample ()\n\nAccess fitted SeasonalWindowAverage insample predictions.\n\n# SeasonalWindowAverage's usage example\n\nfrom statsforecast.models import SeasonalWindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalWindowAverage(season_length=12, window_size=4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([358.  , 338.  , 385.75, 388.25], dtype=float32)}"
  },
  {
    "objectID": "models.html#adida",
    "href": "models.html#adida",
    "title": "Models",
    "section": "ADIDA",
    "text": "ADIDA\n\nsource\n\nADIDA\n\n ADIDA (alias:str='ADIDA')\n\nADIDA model.\nAggregate-Dissagregate Intermittent Demand Approach: Uses temporal aggregation to reduce the number of zero observations. Once the data has been agregated, it uses the optimized SES to generate the forecasts at the new level. It then breaks down the forecast to the original level using equal weights.\nADIDA specializes on sparse or intermittent series are series with very few non-zero observations. They are notoriously hard to forecast, and so, different methods have been developed especifically for them.\nReferences: Nikolopoulos, K., Syntetos, A. A., Boylan, J. E., Petropoulos, F., & Assimakopoulos, V. (2011). An aggregate–disaggregate intermittent demand approach (ADIDA) to forecasting: an empirical proposition and analysis. Journal of the Operational Research Society, 62(3), 544-554..\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nADIDA\nCustom name of the model.\n\n\n\n\nsource\n\n\nADIDA.forecast\n\n ADIDA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None, fitted:bool=False)\n\nMemory Efficient ADIDA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nADIDA.fit\n\n ADIDA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the ADIDA model.\nFit an ADIDA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nADIDA fitted model.\n\n\n\n\nsource\n\n\nADIDA.predict\n\n ADIDA.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted ADIDA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nADIDA.predict_in_sample\n\n ADIDA.predict_in_sample ()\n\nAccess fitted ADIDA insample predictions.\n\n# ADIDA's usage example\n\nfrom statsforecast.models import ADIDA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ADIDA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "models.html#crostonclassic",
    "href": "models.html#crostonclassic",
    "title": "Models",
    "section": "CrostonClassic",
    "text": "CrostonClassic\n\nsource\n\nCrostonClassic\n\n CrostonClassic (alias:str='CrostonClassic')\n\nCrostonClassic model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nwhere \\(\\hat{z}_t\\) and \\(\\hat{p}_t\\) are forecasted using SES. The smoothing parameter of both components is set equal to 0.1\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonClassic\nCustom name of the model.\n\n\n\n\nsource\n\n\nCrostonClassic.forecast\n\n CrostonClassic.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          fitted:bool=False)\n\nMemory Efficient CrostonClassic predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonClassic.fit\n\n CrostonClassic.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonClassic model.\nFit an CrostonClassic to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonClassic fitted model.\n\n\n\n\nsource\n\n\nCrostonClassic.predict\n\n CrostonClassic.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted CrostonClassic.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonClassic.predict_in_sample\n\n CrostonClassic.predict_in_sample (level)\n\nAccess fitted CrostonClassic insample predictions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nlevel\n\n\n\n\nReturns\ndict\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# CrostonClassic's usage example\n\nfrom statsforecast.models import CrostonClassic\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonClassic()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([460.30276, 460.30276, 460.30276, 460.30276], dtype=float32)}"
  },
  {
    "objectID": "models.html#crostonoptimized",
    "href": "models.html#crostonoptimized",
    "title": "Models",
    "section": "CrostonOptimized",
    "text": "CrostonOptimized\n\nsource\n\nCrostonOptimized\n\n CrostonOptimized (alias:str='CrostonOptimized')\n\nCrostonOptimized model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston’s method where the smooting paramater is optimally selected from the range \\([0.1,0.3]\\). Both the non-zero demand \\(z_t\\) and the inter-demand intervals \\(p_t\\) are smoothed separately, so their smoothing parameters can be different.\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonOptimized\nCustom name of the model.\n\n\n\n\nsource\n\n\nCrostonOptimized.forecast\n\n CrostonOptimized.forecast (y:numpy.ndarray, h:int,\n                            X:Optional[numpy.ndarray]=None,\n                            X_future:Optional[numpy.ndarray]=None,\n                            fitted:bool=False)\n\nMemory Efficient CrostonOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonOptimized.fit\n\n CrostonOptimized.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonOptimized model.\nFit an CrostonOptimized to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonOptimized fitted model.\n\n\n\n\nsource\n\n\nCrostonOptimized.predict\n\n CrostonOptimized.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted CrostonOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonOptimized.predict_in_sample\n\n CrostonOptimized.predict_in_sample ()\n\nAccess fitted CrostonOptimized insample predictions.\n\n# CrostonOptimized's usage example\n\nfrom statsforecast.models import CrostonOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonOptimized()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "models.html#crostonsba",
    "href": "models.html#crostonsba",
    "title": "Models",
    "section": "CrostonSBA",
    "text": "CrostonSBA\n\nsource\n\nCrostonSBA\n\n CrostonSBA (alias:str='CrostonSBA')\n\nCrostonSBA model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston’s method that uses a debiasing factor, so that the forecast is given by: \\[ \\hat{y}_t = 0.95  \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonSBA\nCustom name of the model.\n\n\n\n\nsource\n\n\nCrostonSBA.forecast\n\n CrostonSBA.forecast (y:numpy.ndarray, h:int,\n                      X:Optional[numpy.ndarray]=None,\n                      X_future:Optional[numpy.ndarray]=None,\n                      fitted:bool=False)\n\nMemory Efficient CrostonSBA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonSBA.fit\n\n CrostonSBA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonSBA model.\nFit an CrostonSBA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonSBA fitted model.\n\n\n\n\nsource\n\n\nCrostonSBA.predict\n\n CrostonSBA.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted CrostonSBA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonSBA.predict_in_sample\n\n CrostonSBA.predict_in_sample ()\n\nAccess fitted CrostonSBA insample predictions.\n\n# CrostonSBA's usage example\n\nfrom statsforecast.models import CrostonSBA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonSBA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([437.28763, 437.28763, 437.28763, 437.28763], dtype=float32)}"
  },
  {
    "objectID": "models.html#imapa",
    "href": "models.html#imapa",
    "title": "Models",
    "section": "IMAPA",
    "text": "IMAPA\n\nsource\n\nIMAPA\n\n IMAPA (alias:str='IMAPA')\n\nIMAPA model.\nIntermittent Multiple Aggregation Prediction Algorithm: Similar to ADIDA, but instead of using a single aggregation level, it considers multiple in order to capture different dynamics of the data. Uses the optimized SES to generate the forecasts at the new levels and then combines them using a simple average.\nReferences: Syntetos, A. A., & Boylan, J. E. (2021). Intermittent demand forecasting: Context, methods and applications. John Wiley & Sons..\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nIMAPA\nCustom name of the model.\n\n\n\n\nsource\n\n\nIMAPA.forecast\n\n IMAPA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None, fitted:bool=False)\n\nMemory Efficient IMAPA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nIMAPA.fit\n\n IMAPA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the IMAPA model.\nFit an IMAPA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nIMAPA fitted model.\n\n\n\n\nsource\n\n\nIMAPA.predict\n\n IMAPA.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted IMAPA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nIMAPA.predict_in_sample\n\n IMAPA.predict_in_sample ()\n\nAccess fitted IMAPA insample predictions.\n\n# IMAPA's usage example\n\nfrom statsforecast.models import IMAPA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = IMAPA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "models.html#tsb",
    "href": "models.html#tsb",
    "title": "Models",
    "section": "TSB",
    "text": "TSB\n\nsource\n\nTSB\n\n TSB (alpha_d:float, alpha_p:float, alias:str='TSB')\n\nTSB model.\nTeunter-Syntetos-Babai: A modification of Croston’s method that replaces the inter-demand intervals with the demand probability \\(d_t\\), which is defined as follows.\n\\[\nd_t = \\begin{cases}\n    1  & \\text{if demand occurs at time t} \\\\\n    0  & \\text{otherwise.}\n\\end{cases}\n\\]\nHence, the forecast is given by\n\\[\\hat{y}_t= \\hat{d}_t\\hat{z_t}\\]\nBoth \\(d_t\\) and \\(z_t\\) are forecasted using SES. The smooting paramaters of each may differ, like in the optimized Croston’s method.\nReferences: Teunter, R. H., Syntetos, A. A., & Babai, M. Z. (2011). Intermittent demand: Linking forecasting to inventory obsolescence. European Journal of Operational Research, 214(3), 606-615.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha_d\nfloat\n\nSmoothing parameter for demand.\n\n\nalpha_p\nfloat\n\nSmoothing parameter for probability.\n\n\nalias\nstr\nTSB\nCustom name of the model.\n\n\n\n\nsource\n\n\nTSB.forecast\n\n TSB.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None, fitted:bool=False)\n\nMemory Efficient TSB predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTSB.fit\n\n TSB.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the TSB model.\nFit an TSB to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nTSB fitted model.\n\n\n\n\nsource\n\n\nTSB.predict\n\n TSB.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted TSB.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTSB.predict_in_sample\n\n TSB.predict_in_sample ()\n\nAccess fitted TSB insample predictions.\n\n# TSB's usage example\n\nfrom statsforecast.models import TSB\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = TSB(alpha_d=0.5, alpha_p=0.5)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "models.html#standard-theta-method",
    "href": "models.html#standard-theta-method",
    "title": "Models",
    "section": "Standard Theta Method",
    "text": "Standard Theta Method\n\nsource\n\nTheta\n\n Theta (season_length:int=1, decomposition_type:str='multiplicative',\n        alias:str='Theta')\n\nStandard Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nTheta\nCustom name of the model.\n\n\n\n\nsource\n\n\nTheta.forecast\n\n Theta.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.fit\n\n Theta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nTheta.predict\n\n Theta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.predict_in_sample\n\n Theta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.forward\n\n Theta.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Theta's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Theta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.96918204, 429.24926382, 490.69323393, 476.65998055])}"
  },
  {
    "objectID": "models.html#optimized-theta-method",
    "href": "models.html#optimized-theta-method",
    "title": "Models",
    "section": "Optimized Theta Method",
    "text": "Optimized Theta Method\n\nsource\n\nOptimizedTheta\n\n OptimizedTheta (season_length:int=1,\n                 decomposition_type:str='multiplicative',\n                 alias:str='OptimizedTheta')\n\nOptimized Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nOptimizedTheta\nCustom name of the model. Default OptimizedTheta.\n\n\n\n\nsource\n\n\nOptimizedTheta.forecast\n\n OptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.fit\n\n OptimizedTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nOptimizedTheta.predict\n\n OptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.predict_in_sample\n\n OptimizedTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.forward\n\n OptimizedTheta.forward (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# OptimzedThetA's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = OptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94078295, 432.22936898, 495.30609727, 482.30625563])}"
  },
  {
    "objectID": "models.html#dynamic-standard-theta-method",
    "href": "models.html#dynamic-standard-theta-method",
    "title": "Models",
    "section": "Dynamic Standard Theta Method",
    "text": "Dynamic Standard Theta Method\n\nsource\n\nDynamicTheta\n\n DynamicTheta (season_length:int=1,\n               decomposition_type:str='multiplicative',\n               alias:str='DynamicTheta')\n\nDynamic Standard Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nDynamicTheta\nCustom name of the model.\n\n\n\n\nsource\n\n\nDynamicTheta.forecast\n\n DynamicTheta.forecast (y:numpy.ndarray, h:int,\n                        X:Optional[numpy.ndarray]=None,\n                        X_future:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.fit\n\n DynamicTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nDynamicTheta.predict\n\n DynamicTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                       level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.predict_in_sample\n\n DynamicTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.forward\n\n DynamicTheta.forward (y:numpy.ndarray, h:int,\n                       X:Optional[numpy.ndarray]=None,\n                       X_future:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# DynStandardThetaMethod's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.77412474, 429.06190332, 490.48332496, 476.46133269])}"
  },
  {
    "objectID": "models.html#dynamic-optimized-theta-method",
    "href": "models.html#dynamic-optimized-theta-method",
    "title": "Models",
    "section": "Dynamic Optimized Theta Method",
    "text": "Dynamic Optimized Theta Method\n\nsource\n\nDynamicOptimizedTheta\n\n DynamicOptimizedTheta (season_length:int=1,\n                        decomposition_type:str='multiplicative',\n                        alias:str='DynamicOptimizedTheta')\n\nDynamic Optimized Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nDynamicOptimizedTheta\nCustom name of the model.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.forecast\n\n DynamicOptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.fit\n\n DynamicOptimizedTheta.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.predict\n\n DynamicOptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                                level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.predict_in_sample\n\n DynamicOptimizedTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.forward\n\n DynamicOptimizedTheta.forward (y:numpy.ndarray, h:int,\n                                X:Optional[numpy.ndarray]=None,\n                                X_future:Optional[numpy.ndarray]=None,\n                                level:Optional[List[int]]=None,\n                                fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# OptimzedThetaMethod's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicOptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94256075, 432.31255941, 495.49774527, 482.58930649])}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StatsForecast ⚡️",
    "section": "",
    "text": "You can install StatsForecast with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\nVist our Installation Guide for further instructions.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "StatsForecast ⚡️",
    "section": "Quick Start",
    "text": "Quick Start\nMinimal Example\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\nsf.predict(h=12, level=[95])\nGet Started with this quick guide.\nFollow this end-to-end walkthrough for best practices."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "StatsForecast ⚡️",
    "section": "Why?",
    "text": "Why?\nCurrent Python alternatives for statistical models are slow, inaccurate and don’t scale well. So we created a library that can be used to forecast in production environments or as benchmarks. StatsForecast includes an extensive battery of models that can efficiently fit millions of time series."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "StatsForecast ⚡️",
    "section": "Features",
    "text": "Features\n\nFastest and most accurate implementations of AutoARIMA, AutoETS, AutoCES, MSTL and Theta in Python.\nOut-of-the-box compatibility with Spark, Dask, and Ray.\nProbabilistic Forecasting and Confidence Intervals.\nSupport for exogenous Variables and static covariates.\nAnomaly Detection.\nFamiliar sklearn syntax: .fit and .predict."
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "StatsForecast ⚡️",
    "section": "Highlights",
    "text": "Highlights\n\nInclusion of exogenous variables and prediction intervals for ARIMA.\n20x faster than pmdarima.\n1.5x faster than R.\n500x faster than Prophet.\n4x faster than statsmodels.\nCompiled to high performance machine code through numba.\n1,000,000 series in 30 min with ray.\nReplace FB-Prophet in two lines of code and gain speed and accuracy. Check the experiments here.\nFit 10 benchmark models on 1,000,000 series in under 5 min.\n\nMissing something? Please open an issue or write us in"
  },
  {
    "objectID": "index.html#examples-and-guides",
    "href": "index.html#examples-and-guides",
    "title": "StatsForecast ⚡️",
    "section": "Examples and Guides",
    "text": "Examples and Guides\n📚 End to End Walkthrough: Model training, evaluation and selection for multiple time series\n🔎 Anomaly Detection: detect anomalies for time series using in-sample prediction intervals.\n👩‍🔬 Cross Validation: robust model’s performance evaluation.\n❄️ Multiple Seasonalities: how to forecast data with multiple seasonalities using an MSTL.\n🔌 Predict Demand Peaks: electricity load forecasting for detecting daily peaks and reducing electric bills.\n📈 Intermittent Demand: forecast series with very few non-zero observations.\n🌡️ Exogenous Regressors: like weather or prices"
  },
  {
    "objectID": "index.html#models",
    "href": "index.html#models",
    "title": "StatsForecast ⚡️",
    "section": "Models",
    "text": "Models\n\nAutomatic Forecasting\nAutomatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n✅\n✅\n✅\n✅\n\n\nAutoETS\n✅\n✅\n✅\n✅\n\n\nAutoCES\n✅\n✅\n✅\n✅\n\n\nAutoTheta\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "index.html#arima-family",
    "href": "index.html#arima-family",
    "title": "StatsForecast ⚡️",
    "section": "ARIMA Family",
    "text": "ARIMA Family\nThese models exploit the existing autocorrelations in the time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nARIMA\n✅\n✅\n✅\n✅\n\n\nAutoRegressive\n✅\n✅\n✅\n✅\n\n\n\n\nTheta Family\nFit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n✅\n✅\n✅\n✅\n\n\nOptimizedTheta\n✅\n✅\n✅\n✅\n\n\nDynamicTheta\n✅\n✅\n✅\n✅\n\n\nDynamicOptimizedTheta\n✅\n✅\n✅\n✅\n\n\n\n\n\nMultiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n✅\n✅\n✅\n✅\n\n\n\n\n\nBaseline Models\nClassical models for establishing baseline.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n✅\n✅\n✅\n✅\n\n\nNaive\n✅\n✅\n✅\n✅\n\n\nRandomWalkWithDrift\n✅\n✅\n✅\n✅\n\n\nSeasonalNaive\n✅\n✅\n✅\n✅\n\n\nWindowAverage\n✅\n\n\n\n\n\nSeasonalWindowAverage\n✅\n\n\n\n\n\n\n\n\nExponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with clear trend and/or seasonality. Use the SimpleExponential family for data with no clear trend or seasonality.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n✅\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n✅\n\n\n\n\n\nSeasonalExponentialSmoothing\n✅\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n✅\n\n\n\n\n\nHolt\n✅\n✅\n✅\n✅\n\n\nHoltWinters\n✅\n✅\n✅\n✅\n\n\n\n\n\nSparse of Inttermitent\nSuited for series with very few non-zero observations\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n✅\n\n\n\n\n\nCrostonClassic\n✅\n\n\n\n\n\nCrostonOptimized\n✅\n\n\n\n\n\nCrostonSBA\n✅\n\n\n\n\n\nIMAPA\n✅\n\n\n\n\n\nTSB\n✅"
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "StatsForecast ⚡️",
    "section": "How to contribute",
    "text": "How to contribute\nSee CONTRIBUTING.md."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "StatsForecast ⚡️",
    "section": "Citing",
    "text": "Citing\n@misc{garza2022statsforecast,\n    author={Federico Garza, Max Mergenthaler Canseco, Cristian Challú, Kin G. Olivares},\n    title = {{StatsForecast}: Lightning fast forecasting with statistical and econometric models},\n    year={2022},\n    howpublished={{PyCon} Salt Lake City, Utah, US 2022},\n    url={https://github.com/Nixtla/statsforecast}\n}"
  },
  {
    "objectID": "distributed.fugue.html",
    "href": "distributed.fugue.html",
    "title": "FugueBackend",
    "section": "",
    "text": "source\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.fugue.html#dask-distributed-predictions",
    "href": "distributed.fugue.html#dask-distributed-predictions",
    "title": "FugueBackend",
    "section": "Dask Distributed Predictions",
    "text": "Dask Distributed Predictions\nHere we provide an example for the distribution of the StatsForecast predictions using Fugue to execute the code in a Dask cluster.\nTo do it we instantiate the FugueBackend class with a DaskExecutionEngine.\n\nfrom dask.distributed import Client\nfrom fugue_dask import DaskExecutionEngine\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import generate_series\n\n# Generate Synthetic Panel Data\ndf = generate_series(10).reset_index()\ndf['unique_id'] = df['unique_id'].astype(str)\n\n# Instantiate FugueBackend with DaskExecutionEngine\ndask_client = Client()\nengine = DaskExecutionEngine(dask_client=dask_client)\nbackend = FugueBackend(engine=engine, as_local=True)\n\nWe have simply pass backend to the usual StatsForecast instantiation.\n\nfcst = StatsForecast(models=[Naive()], freq='D', backend=backend)\n\n\nDistributed Forecast\nFor extremely fast distributed predictions we use FugueBackend as backend that operates like the original StatsForecast.forecast method.\nIt receives as input a pandas.DataFrame with columns [unique_id,ds,y] and exogenous, where the ds (datestamp) column should be of a format expected by Pandas. The y column must be numeric, and represents the measurement we wish to forecast. And the unique_id uniquely identifies the series in the panel data.\n\n# Distributed predictions with FugueBackend.\nfcst.forecast(df=df, h=12)\n\n\n\nDistributed Cross-Validation\nFor extremely fast distributed temporcal cross-validation we use cross_validation method that operates like the original StatsForecast.cross_validation method.\n\n# Distributed cross-validation with FugueBackend.\nfcst.cross_validation(df=df, h=12, n_windows=2)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_static_features:int=0,\n                  equal_ends:bool=False, seed:int=0)\n\nGenerate Synthetic Panel Series.\nGenerates n_series of frequency freq of different lengths in the interval [min_length, max_length]. If n_static_features > 0, then each serie gets static features with random values. If equal_ends == True then all series end at the same date.\nParameters: n_series: int, number of series for synthetic panel. min_length: int, minimal length of synthetic panel’s series. max_length: int, minimal length of synthetic panel’s series. n_static_features: int, default=0, number of static exogenous variables for synthetic panel’s series. equal_ends: bool, if True, series finish in the same date stamp ds. freq: str, frequency of the data, panda’s available frequencies.\nReturns: freq: pandas.DataFrame, synthetic panel with columns [unique_id, ds, y] and exogenous.\n\nfrom statsforecast.utils import generate_series\n\nsynthetic_panel = generate_series(n_series=2)\nsynthetic_panel.groupby('unique_id').head(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      y\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      0\n      2000-01-01\n      0.357595\n    \n    \n      0\n      2000-01-02\n      1.301382\n    \n    \n      0\n      2000-01-03\n      2.272442\n    \n    \n      0\n      2000-01-04\n      3.211827\n    \n    \n      1\n      2000-01-01\n      5.399023\n    \n    \n      1\n      2000-01-02\n      6.092818\n    \n    \n      1\n      2000-01-03\n      0.476396\n    \n    \n      1\n      2000-01-04\n      1.343744\nGive us a ⭐ on Github"
  },
  {
    "objectID": "utils.html#model-utils",
    "href": "utils.html#model-utils",
    "title": "Utils",
    "section": "Model utils",
    "text": "Model utils"
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html",
    "href": "examples/autoarima_vs_prophet.html",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html#motivation",
    "href": "examples/autoarima_vs_prophet.html#motivation",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Motivation",
    "text": "Motivation\nThe AutoARIMA model is widely used to forecast time series in production and as a benchmark. However, the python implementation (pmdarima) is so slow that prevent data scientist practioners from quickly iterating and deploying AutoARIMA in production for a large number of time series. In this notebook we present Nixtla’s AutoARIMA based on the R implementation (developed by Rob Hyndman) and optimized using numba."
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html#example",
    "href": "examples/autoarima_vs_prophet.html#example",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Example",
    "text": "Example\n\nLibraries\n\n!pip install statsforecast prophet statsmodels sklearn matplotlib pmdarima\n\n\nimport logging\nimport os\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\nfrom multiprocessing import cpu_count, Pool # for prophet\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima as auto_arima_p\nfrom prophet import Prophet\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, _TS\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom sklearn.model_selection import ParameterGrid\n\nImporting plotly failed. Interactive plots will not work.\n\n\n\nUseful functions\nThe plot_grid function defined below will be useful to plot different time series, and different models’ forecasts.\n\ndef plot_grid(df_train, df_test=None, plot_random=True):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train')\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            for model in df_test.drop(['unique_id', 'ds'], axis=1).columns:\n                if all(np.isnan(test_uid[model])):\n                    continue\n                axes[idx, idy].plot(test_uid['ds'], test_uid[model], label=model)\n\n        axes[idx, idy].set_title(f'M4 Hourly: {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\ndef plot_autocorrelation_grid(df_train):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n\n    unique_ids = random.sample(list(unique_ids), k=8)\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        plot_acf(train_uid['y'].values, ax=axes[idx, idy], \n                 title=f'ACF M4 Hourly {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Autocorrelation')\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\n\n\nData\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\n\n\ntrain = pd.read_csv('M4-Hourly.csv')\ntest = pd.read_csv('M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 16\nuids = train['unique_id'].unique()[:n_series]\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\n\nplot_grid(train, test)\n\n\n\n\nWould an autorregresive model be the right choice for our data? There is no doubt that we observe seasonal periods. The autocorrelation function (acf) can help us to answer the question. Intuitively, we have to observe a decreasing correlation to opt for an AR model.\n\nplot_autocorrelation_grid(train)\n\n\n\n\nThus, we observe a high autocorrelation for previous lags and also for the seasonal lags. Therefore, we will let auto_arima to handle our data.\n\n\nTraining and forecasting\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Hourly data, it would be benefitial to use 24 as seasonality.\n\n?AutoARIMA\n\n\nInit signature:\nAutoARIMA(\n    d: Optional[int] = None,\n    D: Optional[int] = None,\n    max_p: int = 5,\n    max_q: int = 5,\n    max_P: int = 2,\n    max_Q: int = 2,\n    max_order: int = 5,\n    max_d: int = 2,\n    max_D: int = 1,\n    start_p: int = 2,\n    start_q: int = 2,\n    start_P: int = 1,\n    start_Q: int = 1,\n    stationary: bool = False,\n    seasonal: bool = True,\n    ic: str = 'aicc',\n    stepwise: bool = True,\n    nmodels: int = 94,\n    trace: bool = False,\n    approximation: Optional[bool] = False,\n    method: Optional[str] = None,\n    truncate: Optional[bool] = None,\n    test: str = 'kpss',\n    test_kwargs: Optional[str] = None,\n    seasonal_test: str = 'seas',\n    seasonal_test_kwargs: Optional[Dict] = None,\n    allowdrift: bool = False,\n    allowmean: bool = False,\n    blambda: Optional[float] = None,\n    biasadj: bool = False,\n    parallel: bool = False,\n    num_cores: int = 2,\n    season_length: int = 1,\n)\nDocstring:      <no docstring>\nFile:           ~/fede/statsforecast/statsforecast/models.py\nType:           type\nSubclasses:     \n\n\n\n\nAs we see, we can pass season_length to AutoARIMA, so the definition of our models would be,\n\nmodels = [AutoARIMA(season_length=24, approximation=True)]\n\n\nfcst = StatsForecast(df=train, \n                     models=models, \n                     freq='H', \n                     n_jobs=-1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla = end - init\ntime_nixtla\n\n20.36360502243042\n\n\n\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      H1\n      701\n      616.084167\n    \n    \n      H1\n      702\n      544.432129\n    \n    \n      H1\n      703\n      510.414490\n    \n    \n      H1\n      704\n      481.046539\n    \n    \n      H1\n      705\n      460.893066\n    \n  \n\n\n\n\n\nforecasts = forecasts.reset_index()\n\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)"
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html#alternatives",
    "href": "examples/autoarima_vs_prophet.html#alternatives",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Alternatives",
    "text": "Alternatives\n\npmdarima\nYou can use the StatsForecast class to parallelize your own models. In this section we will use it to run the auto_arima model from pmdarima.\n\nclass PMDAutoARIMA(_TS):\n    \n    def __init__(self, season_length: int):\n        self.season_length = season_length\n        \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        mod = auto_arima_p(\n            y, m=self.season_length,\n            with_intercept=False #ensure comparability with Nixtla's implementation\n        ) \n        return {'mean': mod.predict(h)}\n    \n    def __repr__(self):\n        return 'pmdarima'\n\n\nn_series_pmdarima = 2\n\n\nfcst = StatsForecast(\n    df = train.query('unique_id in [\"H1\", \"H10\"]'), \n    models=[PMDAutoARIMA(season_length=24)],\n    freq='H',\n    n_jobs=-1\n)\n\n\ninit = time.time()\nforecast_pmdarima = fcst.forecast(48)\nend = time.time()\n\ntime_pmdarima = end - init\ntime_pmdarima\n\n349.93623208999634\n\n\n\nforecast_pmdarima.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      pmdarima\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      H1\n      701\n      627.479370\n    \n    \n      H1\n      702\n      570.364380\n    \n    \n      H1\n      703\n      541.831482\n    \n    \n      H1\n      704\n      516.475647\n    \n    \n      H1\n      705\n      503.044586\n    \n  \n\n\n\n\n\nforecast_pmdarima = forecast_pmdarima.reset_index()\n\n\ntest = test.merge(forecast_pmdarima, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test, plot_random=False)\n\n\n\n\n\n\nProphet\nProphet is designed to receive a pandas dataframe, so we cannot use StatForecast. Therefore, we need to parallize from scratch.\n\nparams_grid = {'seasonality_mode': ['multiplicative','additive'],\n               'growth': ['linear', 'flat'], \n               'changepoint_prior_scale': [0.1, 0.2, 0.3, 0.4, 0.5], \n               'n_changepoints': [5, 10, 15, 20]} \ngrid = ParameterGrid(params_grid)\n\n\ndef fit_and_predict(index, ts):\n    df = ts.drop(columns='unique_id', axis=1)\n    max_ds = df['ds'].max()\n    df['ds'] = pd.date_range(start='1970-01-01', periods=df.shape[0], freq='H')\n    df_val = df.tail(48) \n    df_train = df.drop(df_val.index) \n    y_val = df_val['y'].values\n    \n    if len(df_train) >= 48:\n        val_results = {'losses': [], 'params': []}\n\n        for params in grid:\n            model = Prophet(seasonality_mode=params['seasonality_mode'],\n                            growth=params['growth'],\n                            weekly_seasonality=True,\n                            daily_seasonality=True,\n                            yearly_seasonality=True,\n                            n_changepoints=params['n_changepoints'],\n                            changepoint_prior_scale=params['changepoint_prior_scale'])\n            model = model.fit(df_train)\n            \n            forecast = model.make_future_dataframe(periods=48, \n                                                   include_history=False, \n                                                   freq='H')\n            forecast = model.predict(forecast)\n            forecast['unique_id'] = index\n            forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n            \n            loss = np.mean(abs(y_val - forecast['yhat'].values))\n            \n            val_results['losses'].append(loss)\n            val_results['params'].append(params)\n\n        idx_params = np.argmin(val_results['losses']) \n        params = val_results['params'][idx_params]\n    else:\n        params = {'seasonality_mode': 'multiplicative',\n                  'growth': 'flat',\n                  'n_changepoints': 150,\n                  'changepoint_prior_scale': 0.5}\n    model = Prophet(seasonality_mode=params['seasonality_mode'],\n                    growth=params['growth'],\n                    weekly_seasonality=True,\n                    daily_seasonality=True,\n                    yearly_seasonality=True,\n                    n_changepoints=params['n_changepoints'],\n                    changepoint_prior_scale=params['changepoint_prior_scale'])\n    model = model.fit(df)\n    \n    forecast = model.make_future_dataframe(periods=48, \n                                           include_history=False, \n                                           freq='H')\n    forecast = model.predict(forecast)\n    forecast.insert(0, 'unique_id', index)\n    forecast['ds'] = np.arange(max_ds + 1, max_ds + 48 + 1)\n    forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n    \n    return forecast\n\n\nlogging.getLogger('prophet').setLevel(logging.WARNING)\n\n\nclass suppress_stdout_stderr(object):\n    '''\n    A context manager for doing a \"deep suppression\" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n       This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n\n    '''\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = [os.dup(1), os.dup(2)]\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        for fd in self.null_fds + self.save_fds:\n            os.close(fd)\n\n\ninit = time.time()\nwith suppress_stdout_stderr():\n    with Pool(cpu_count()) as pool:\n        forecast_prophet = pool.starmap(fit_and_predict, train.groupby('unique_id'))\nend = time.time()\nforecast_prophet = pd.concat(forecast_prophet).rename(columns={'yhat': 'prophet'})\ntime_prophet = end - init\ntime_prophet\n\n2022-08-19 23:07:24 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:25 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:41 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:42 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:08:00 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n\n\n120.9244737625122\n\n\n\nforecast_prophet\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      prophet\n    \n  \n  \n    \n      0\n      H1\n      701\n      631.867439\n    \n    \n      1\n      H1\n      702\n      561.001661\n    \n    \n      2\n      H1\n      703\n      499.299334\n    \n    \n      3\n      H1\n      704\n      456.132082\n    \n    \n      4\n      H1\n      705\n      431.884528\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      H112\n      744\n      5634.503804\n    \n    \n      44\n      H112\n      745\n      5622.643542\n    \n    \n      45\n      H112\n      746\n      5546.302705\n    \n    \n      46\n      H112\n      747\n      5457.777165\n    \n    \n      47\n      H112\n      748\n      5373.944098\n    \n  \n\n768 rows × 3 columns\n\n\n\n\ntest = test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)\n\n\n\n\n\n\nEvaluation\n\n\nTime\nSince AutoARIMA works with numba is useful to calculate the time for just one time series.\n\nfcst = StatsForecast(df=train.query('unique_id == \"H1\"'), \n                     models=models, freq='H', \n                     n_jobs=1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla_1 = end - init\ntime_nixtla_1\n\n11.437001705169678\n\n\n\ntimes = pd.DataFrame({'n_series': np.arange(1, 414 + 1)})\ntimes['pmdarima'] = time_pmdarima * times['n_series'] / n_series_pmdarima\ntimes['prophet'] = time_prophet * times['n_series'] / n_series\ntimes['AutoARIMA_nixtla'] = time_nixtla_1 + times['n_series'] * (time_nixtla - time_nixtla_1) / n_series\ntimes = times.set_index('n_series')\n\n\ntimes.tail(5)\n\n\n\n\n\n  \n    \n      \n      pmdarima\n      prophet\n      AutoARIMA_nixtla\n    \n    \n      n_series\n      \n      \n      \n    \n  \n  \n    \n      410\n      71736.927578\n      3098.689640\n      240.181212\n    \n    \n      411\n      71911.895694\n      3106.247420\n      240.739124\n    \n    \n      412\n      72086.863811\n      3113.805199\n      241.297037\n    \n    \n      413\n      72261.831927\n      3121.362979\n      241.854950\n    \n    \n      414\n      72436.800043\n      3128.920759\n      242.412863\n    \n  \n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize = (24, 7))\n(times/3600).plot(ax=axes[0], linewidth=4)\nnp.log10(times).plot(ax=axes[1], linewidth=4)\naxes[0].set_title('Time across models [Hours]', fontsize=22)\naxes[1].set_title('Time across models [Log10 Scale]', fontsize=22)\naxes[0].set_ylabel('Time [Hours]', fontsize=20)\naxes[1].set_ylabel('Time Seconds [Log10 Scale]', fontsize=20)\nfig.suptitle('Time comparison using M4-Hourly data', fontsize=27)\nfor ax in axes:\n    ax.set_xlabel('Number of Time Series [N]', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid()\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontsize(20)\n\n\n\n\n\nfig.savefig('computational-efficiency.png', dpi=300)\n\n\n\nPerformance\n\npmdarima (only two time series)\n\nname_models = test.drop(['unique_id', 'ds', 'y_test'], 1).columns.tolist()\n\n\ntest_pmdarima = test.query('unique_id in [\"H1\", \"H10\"]')\neval_pmdarima = []\nfor model in name_models:\n    mae = np.mean(abs(test_pmdarima[model] - test_pmdarima['y_test']))\n    eval_pmdarima.append({'model': model, 'mae': mae})\npd.DataFrame(eval_pmdarima).sort_values('mae')\n\n\n\n\n\n  \n    \n      \n      model\n      mae\n    \n  \n  \n    \n      0\n      AutoARIMA\n      20.289669\n    \n    \n      1\n      pmdarima\n      26.461525\n    \n    \n      2\n      prophet\n      43.155861\n    \n  \n\n\n\n\n\n\nProphet\n\neval_prophet = []\nfor model in name_models:\n    if 'pmdarima' in model:\n        continue\n    mae = np.mean(abs(test[model] - test['y_test']))\n    eval_prophet.append({'model': model, 'mae': mae})\npd.DataFrame(eval_prophet).sort_values('mae')\n\n\n\n\n\n  \n    \n      \n      model\n      mae\n    \n  \n  \n    \n      0\n      AutoARIMA\n      680.202970\n    \n    \n      1\n      prophet\n      1066.049049\n    \n  \n\n\n\n\nFor a complete comparison check the complete experiment."
  },
  {
    "objectID": "examples/intermittentdata.html",
    "href": "examples/intermittentdata.html",
    "title": "Intermittent or Sparse Data",
    "section": "",
    "text": "Intermittent or sparse data has very few non-zero observations. This type of data is hard to forecast because the zero values increase the uncertainty about the underlying patterns in the data. Furthermore, once a non-zero observation occurs, there can be considerable variation in its size. Intermittent time series are common in many industries, including finance, retail, transportation, and energy. Given the ubiquity of this type of series, special methods have been developed to forecast them. The first was from Croston (1972), followed by several variants and by different aggregation frameworks.\nStatsForecast has implemented several models to forecast intermittent time series. By the end of this tutorial, you’ll have a good understanding of these models and how to use them.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/intermittentdata.html#install-libraries",
    "href": "examples/intermittentdata.html#install-libraries",
    "title": "Intermittent or Sparse Data",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "examples/intermittentdata.html#load-and-explore-the-data",
    "href": "examples/intermittentdata.html#load-and-explore-the-data",
    "title": "Intermittent or Sparse Data",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use a subset of the M5 Competition dataset. Each time series represents the unit sales of a particular product in a given Walmart store. At this level (product-store), most of the data is intermittent. We first need to import the data, and to do that, we’ll need datasetsforecast.\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m5 import M5\n\nThe function to load the data is M5.load(). It requieres the following argument: - directory: (str) The directory where the data will be downloaded.\nThis function returns multiple outputs, but only the first one with the unit sales is needed.\n\ndf_total, *_ = M5.load('./data')\n\n\ndf_total.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      3.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      4.0\n    \n  \n\n\n\n\nFrom this dataset, we’ll select the first 8 time series. You can select any number you want by changing the value of n_series.\n\nn_series = 8 \nuids = df_total['unique_id'].unique()[:n_series]\ndf = df_total.query('unique_id in @uids')\n\nWe can plot these series using the plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: (bool = True) Plots the time series randomly.\nmax_insample_length: (int) The maximum number of train/insample observations to be plotted.\nengine: (str = plotly). The library used to generate the plots. It can also be matplotlib for static plots.\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(df, plot_random = False, max_insample_length = 100)\n\n\n                                                \n\n\nHere we only plotted the last 100 observations, but we can visualize the complete history by removing max_insample_length. From these plots, we can confirm that the data is indeed intermittent since it has multiple periods with zero sales. In fact, in all cases but one, the median value is zero.\n\ndf.groupby('unique_id')[['y']].median().query('unique_id in @uids')\n\n\n\n\n\n  \n    \n      \n      y\n    \n    \n      unique_id\n      \n    \n  \n  \n    \n      FOODS_1_001_CA_1\n      0.0\n    \n    \n      FOODS_1_001_CA_2\n      1.0\n    \n    \n      FOODS_1_001_CA_3\n      0.0\n    \n    \n      FOODS_1_001_CA_4\n      0.0\n    \n    \n      FOODS_1_001_TX_1\n      0.0\n    \n    \n      FOODS_1_001_TX_2\n      0.0\n    \n    \n      FOODS_1_001_TX_3\n      0.0\n    \n    \n      FOODS_1_001_WI_1\n      0.0"
  },
  {
    "objectID": "examples/intermittentdata.html#train-models-for-intermittent-data",
    "href": "examples/intermittentdata.html#train-models-for-intermittent-data",
    "title": "Intermittent or Sparse Data",
    "section": "Train models for intermittent data",
    "text": "Train models for intermittent data\nBefore training any model, we need to separate the data in a train and a test set. The M5 Competition used the last 28 days as test set, so we’ll do the same.\n\ndates = df['ds'].unique()[-28:] # last 28 days\n\ntrain = df.query('ds not in @dates')\ntest = df.query('ds in @dates')\n\nStatsForecast has efficient implementations of multiple models for intermittent data. The complete list of models available is here. In this notebook, we’ll use:\n\nAgregate-Dissagregate Intermittent Demand Approach (ADIDA)\nCroston Classic\nIntermittent Multiple Aggregation Prediction Algorithm (IMAPA)\nTeunter-Syntetos-Babai (TSB)\n\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them.\n\nfrom statsforecast.models import (\n    ADIDA,\n    CrostonClassic, \n    IMAPA, \n    TSB\n)\n\n# Create a list of models and instantiation parameters \nmodels = [\n    ADIDA(), \n    CrostonClassic(), \n    IMAPA(), \n    TSB(alpha_d = 0.2, alpha_p = 0.2)\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = train, \n    models = models, \n    freq = 'D', \n    n_jobs = -1\n)\n\nNow we’re ready to generate the forecast. To do this, we’ll use the forecast method, which requires the forecasting horizon (in this case, 28 days) as argument.\nThe models for intermittent series that are currently available in StatsForecast can only generate point-forecasts. If prediction intervals are needed, then a probabilisitic model should be used.\n\nhorizon = 28 \nforecasts = sf.forecast(h=horizon)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      ADIDA\n      CrostonClassic\n      IMAPA\n      TSB\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2016-05-23\n      0.791852\n      0.898247\n      0.705835\n      0.434313\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2016-05-24\n      0.791852\n      0.898247\n      0.705835\n      0.434313\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2016-05-25\n      0.791852\n      0.898247\n      0.705835\n      0.434313\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2016-05-26\n      0.791852\n      0.898247\n      0.705835\n      0.434313\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2016-05-27\n      0.791852\n      0.898247\n      0.705835\n      0.434313\n    \n  \n\n\n\n\nFinally, we’ll merge the forecast with the actual values.\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "examples/intermittentdata.html#plot-forecasts-and-compute-accuracy",
    "href": "examples/intermittentdata.html#plot-forecasts-and-compute-accuracy",
    "title": "Intermittent or Sparse Data",
    "section": "Plot forecasts and compute accuracy",
    "text": "Plot forecasts and compute accuracy\nWe can generate plots using the plot described above.\n\nStatsForecast.plot(train, test, plot_random = False, max_insample_length = 100)\n\n\n                                                \n\n\nTo compute the accuracy of the forecasts, we’ll use the Mean Average Error (MAE), which is the sum of the absolute errors divided by the number of forecasts. We’ll create a function to compute the MAE, and for that, we’ll need to import numpy.\n\nimport numpy as np \n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\n\ny_true = test['y'].values\nadida_preds = test['ADIDA'].values\ncroston_preds = test['CrostonClassic'].values\nimapa_preds = test['IMAPA'].values\ntsb_preds = test['TSB'].values\n\nprint('ADIDA MAE: \\t %0.3f' % mae(adida_preds, y_true))\nprint('Croston Classic MAE: \\t %0.3f' % mae(croston_preds, y_true))\nprint('IMAPA MAE: \\t %0.3f' % mae(imapa_preds, y_true))\nprint('TSB   MAE: \\t %0.3f' % mae(tsb_preds, y_true))\n\nADIDA MAE:   0.949\nCroston Classic MAE:     0.944\nIMAPA MAE:   0.957\nTSB   MAE:   1.023\n\n\nHence, on average, the forecasts are one unit off."
  },
  {
    "objectID": "examples/intermittentdata.html#references",
    "href": "examples/intermittentdata.html#references",
    "title": "Intermittent or Sparse Data",
    "section": "References",
    "text": "References\n Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303."
  },
  {
    "objectID": "examples/aws/statsforecast.html",
    "href": "examples/aws/statsforecast.html",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "",
    "text": "We will make use of the M5 competition dataset provided by Walmart. This dataset is interesting for its scale but also the fact that it features many timeseries with infrequent occurances. Such timeseries are common in retail scenarios and are difficult for traditional timeseries forecasting techniques to address.\nThe data are ready for download at the following URLs:\n\nTrain set: https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet\nTemporal exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/temporal.parquet\nStatic exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/static.parquet\n\nA more detailed description of the data can be found here.\n\n\n\n\n\n\nWarning\n\n\n\nThe M5 competition is hierarchical. That is, forecasts are required for different levels of aggregation: national, state, store, etc. In this experiment, we only generate forecasts using the bottom-level data. The evaluation is performed using the bottom-up reconciliation method to obtain the forecasts for the higher hierarchies.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/aws/statsforecast.html#amazon-forecast",
    "href": "examples/aws/statsforecast.html#amazon-forecast",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Amazon Forecast",
    "text": "Amazon Forecast\nAmazon Forecast is a fully automated solution for time series forecasting. The solution can take the time series to forecast and exogenous variables (temporal and static). For this experiment, we used the AutoPredict functionality of Amazon Forecast following the steps of this tutorial. A detailed description of the particular steps for this dataset can be found here.\nAmazon Forecast creates predictors with AutoPredictor, which involves applying the optimal combination of algorithms to each time series in your datasets. The predictor is an Amazon Forecast model that is trained using your target time series, related time series, item metadata, and any additional datasets you include.\nIncluded algorithms range from commonly used statistical algorithms like Autoregressive Integrated Moving Average (ARIMA), to complex neural network algorithms like CNN-QR and DeepAR+.: CNN-QR, DeepAR+, Prophet, NPTS, ARIMA, and ETS.\nTo leverage the probabilistic features of Amazon Forecast and enable confidence intervals for further analysis we forecasted the following quantiles: 0.1 | 0.5 | 0.9.\nThe full pipeline of Amazon Forecast took 4.1 hours and the results can be found here: s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet"
  },
  {
    "objectID": "examples/aws/statsforecast.html#nixtlas-statsforecast",
    "href": "examples/aws/statsforecast.html#nixtlas-statsforecast",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Nixtla’s StatsForecast",
    "text": "Nixtla’s StatsForecast\n\nInstall necessary libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS. (If you don’t want to use a cloud storage provider, you can read your files locally using pandas)\n\n!pip install statsforecast s3fs\n\n\n\nInput format\nWe will use pandas to read the data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nSo we will rename the original columns to make it compatible with StatsForecast.\nDepending on your internet connection, this step should take around 20 seconds.\n\n\n\n\n\n\nWarning\n\n\n\nWe are reading a file from S3, so you need to install the s3fs library. To install it, run ! pip install s3fs\n\n\n\n\nRead data\n\nimport pandas as pd\n\nY_df_m5 = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet') \n\nY_df_m5 = Y_df_m5.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n}) \n\nY_df_m5.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      3.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      4.0\n    \n  \n\n\n\n\n\n\nTrain statistical models\nWe fit the model by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them. For this example, we will use AutoETS and DynamicOptimizedTheta. We set season_length to 7 because we expect seasonal effects every week. (See: Seasonal periods)\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\n\nAutoETS: Exponential Smoothing model. Automatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Ref: AutoETS.\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive.\nDynamicOptimizedTheta: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Ref: DynamicOptimizedTheta.\n\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import (\n    AutoETS,\n    DynamicOptimizedTheta,\n    SeasonalNaive\n)\n\n# Create list of models\nmodels = [\n    AutoETS(season_length=7),\n    DynamicOptimizedTheta(season_length=7),\n]\n\n# Instantiate StatsForecast class\nsf = StatsForecast( \n    models=models,\n    freq='D', \n    n_jobs=-1,\n    fallback_model=SeasonalNaive(season_length=7)\n)\n\n/home/ubuntu/fede/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\nThe forecast method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\n\n\n\n\n\nNote\n\n\n\nThe forecast is inteded to be compatible with distributed clusters, so it does not store any model parameters. If you want to store parameter for everymodel you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nfrom time import time\n\n\ninit = time()\nforecasts_df = sf.forecast(df=Y_df_m5, h=28)\nend = time()\nprint(f'Statsforecast time M5 {(end - init) / 60}')\n\nStatsforecast time M5 14.274124479293823\n\n\nStore the results for further evaluation.\n\nforecasts_df['ThETS'] = forecasts_df[['DynamicOptimizedTheta', 'AutoETS']].clip(0).median(axis=1, numeric_only=True)\nforecasts_df.to_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')"
  },
  {
    "objectID": "examples/aws/statsforecast.html#evaluation",
    "href": "examples/aws/statsforecast.html#evaluation",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Evaluation",
    "text": "Evaluation\nThis section evaluates the performance of StatsForecast and AmazonForecast. To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a large battery of benchmark datasets and evaluation utilities. The library will allow us to calculate the performance of the models using the original evaluation used in the competition.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.m5 import M5, M5Evaluation\n\nThe following function will allow us to evaluate a specific model included in the input dataframe. The function is useful for evaluating different models.\n\nfrom datasetsforecast.m5 import M5, M5Evaluation\nfrom statsforecast import StatsForecast\n\n### Evaluator\ndef evaluate_forecasts(df, model, model_name):\n    Y_hat = df.set_index('ds', append=True)[model].unstack()\n    *_, S_df = M5.load('data')\n    Y_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n    eval_ = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n    eval_ = eval_.rename(columns={'wrmsse': f'{model_name}_{model}_wrmsse'})\n    return eval_\n\nNow let’s read the forecasts generated for each solution.\n\n### Read Forecasts\nstatsforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')\namazonforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet')\n\n### Amazon Forecast wrangling\namazonforecasts_df = amazonforecasts_df.rename(columns={'item_id': 'unique_id', 'date': 'ds'})\n# amazon forecast returns the unique_id column in lower case\n# we need to transform it to upper case to ensure proper merging\namazonforecasts_df['unique_id'] = amazonforecasts_df['unique_id'].str.upper()\namazonforecasts_df = amazonforecasts_df.set_index('unique_id')\n# parse datestamp\namazonforecasts_df['ds'] = pd.to_datetime(amazonforecasts_df['ds']).dt.tz_localize(None)\n\nFinally, let’s use our predefined function to compute the performance of each model.\n\n### Evaluate performances\nm5_eval_df = pd.concat([\n    evaluate_forecasts(statsforecasts_df, 'ThETS', 'StatsForecast'),\n    evaluate_forecasts(statsforecasts_df, 'AutoETS', 'StatsForecast'),\n    evaluate_forecasts(statsforecasts_df, 'DynamicOptimizedTheta', 'StatsForecast'),\n    evaluate_forecasts(amazonforecasts_df, 'p50', 'AmazonForecast'),\n], axis=1)\nm5_eval_df.T\n\n\n\n\n\n  \n    \n      \n      Total\n      Level1\n      Level2\n      Level3\n      Level4\n      Level5\n      Level6\n      Level7\n      Level8\n      Level9\n      Level10\n      Level11\n      Level12\n    \n  \n  \n    \n      StatsForecast_ThETS_wrmsse\n      0.669606\n      0.424331\n      0.515777\n      0.580670\n      0.474098\n      0.552459\n      0.578092\n      0.651079\n      0.642446\n      0.725324\n      1.009390\n      0.967537\n      0.914068\n    \n    \n      StatsForecast_AutoETS_wrmsse\n      0.672404\n      0.430474\n      0.516340\n      0.580736\n      0.482090\n      0.559721\n      0.579939\n      0.655362\n      0.643638\n      0.727967\n      1.010596\n      0.968168\n      0.913820\n    \n    \n      StatsForecast_DynamicOptimizedTheta_wrmsse\n      0.675333\n      0.429670\n      0.521640\n      0.589278\n      0.478730\n      0.557520\n      0.584278\n      0.656283\n      0.650613\n      0.731735\n      1.013910\n      0.971758\n      0.918576\n    \n    \n      AmazonForecast_p50_wrmsse\n      1.617815\n      1.912144\n      1.786991\n      1.736382\n      1.972658\n      2.010498\n      1.805926\n      1.819329\n      1.667225\n      1.619216\n      1.156432\n      1.012942\n      0.914040\n    \n  \n\n\n\n\nThe results (including processing time and costs) can be summarized in the following table."
  },
  {
    "objectID": "examples/aws/amazonforecast.html",
    "href": "examples/aws/amazonforecast.html",
    "title": "statsforecast",
    "section": "",
    "text": "image\n\n\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\nimage\n\n\n\n\n\n\n\n\nimage\n\n\n\n\n\nimage\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/aws/data.html",
    "href": "examples/aws/data.html",
    "title": "Data for the comparison",
    "section": "",
    "text": "This experiment aims to compare the performance of the AmazonForecast automated solutions against classical statistical models using StatsForecast using the M5 and M4 datasets.\nIn this notebook we will explain the data used in the experiment.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/aws/data.html#m5-dataset",
    "href": "examples/aws/data.html#m5-dataset",
    "title": "Data for the comparison",
    "section": "M5 dataset",
    "text": "M5 dataset\n\nTarget data\n\ntrain_df =  pd.read_parquet('s3://m5-benchmarks/data/train/target.parquet')\n\n\ntrain_df.head()\n\n\n\n\n\n  \n    \n      \n      item_id\n      timestamp\n      demand\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      3.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      4.0\n    \n  \n\n\n\n\n\ntrain_df = train_df.rename(columns={'item_id': 'unique_id', \n                                    'timestamp': 'ds',\n                                    'demand': 'y'})\n\n\nStatsForecast.plot(train_df)\n\n\n                                                \n\n\n\n\nStatic variables\n\nstatic_df = pd.read_parquet('s3://m5-benchmarks/data/train/static.parquet')\n\n\nstatic_df.head()\n\n\n\n\n\n  \n    \n      \n      item_id\n      sku_id\n      dept_id\n      cat_id\n      store_id\n      state_id\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      FOODS_1_001\n      FOODS_1\n      FOODS\n      CA_1\n      CA\n    \n    \n      1\n      FOODS_1_001_CA_2\n      FOODS_1_001\n      FOODS_1\n      FOODS\n      CA_2\n      CA\n    \n    \n      2\n      FOODS_1_001_CA_3\n      FOODS_1_001\n      FOODS_1\n      FOODS\n      CA_3\n      CA\n    \n    \n      3\n      FOODS_1_001_CA_4\n      FOODS_1_001\n      FOODS_1\n      FOODS\n      CA_4\n      CA\n    \n    \n      4\n      FOODS_1_001_TX_1\n      FOODS_1_001\n      FOODS_1\n      FOODS\n      TX_1\n      TX\n    \n  \n\n\n\n\n\n\nTemporal variables\n\ntemporal_df = pd.read_parquet('s3://m5-benchmarks/data/train/temporal.parquet')\n\n\ntemporal_df.head()\n\n\n\n\n\n  \n    \n      \n      item_id\n      timestamp\n      snap_CA\n      snap_TX\n      snap_WI\n      sell_price\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      0.0\n      0.0\n      0.0\n      2.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n      0.0\n      0.0\n      2.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n      0.0\n      0.0\n      2.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n      1.0\n      0.0\n      2.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      1.0\n      0.0\n      1.0\n      2.0\n    \n  \n\n\n\n\n\ntemporal_df = temporal_df.rename(columns={'item_id': 'unique_id', \n                                          'timestamp': 'ds'})\n\n\nStatsForecast.plot(train_df, temporal_df)"
  },
  {
    "objectID": "examples/aws/data.html#m4-daily-dataset",
    "href": "examples/aws/data.html#m4-daily-dataset",
    "title": "Data for the comparison",
    "section": "M4 Daily dataset",
    "text": "M4 Daily dataset\n\ntrain_df = pd.read_parquet('s3://m4-benchmarks/data/train/target.parquet')\n\n\ntrain_df.head()\n\n\n\n\n\n  \n    \n      \n      item_id\n      timestamp\n      target_value\n    \n  \n  \n    \n      0\n      D1\n      2019-03-18\n      1017.1\n    \n    \n      1\n      D1\n      2019-03-19\n      1019.3\n    \n    \n      2\n      D1\n      2019-03-20\n      1017.0\n    \n    \n      3\n      D1\n      2019-03-21\n      1019.2\n    \n    \n      4\n      D1\n      2019-03-22\n      1018.7\n    \n  \n\n\n\n\n\ntrain_df = train_df.rename(columns={'item_id': 'unique_id', \n                                    'timestamp': 'ds',\n                                    'target_value': 'y'})\n\n\nStatsForecast.plot(train_df)"
  },
  {
    "objectID": "examples/anomalydetection.html",
    "href": "examples/anomalydetection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/anomalydetection.html#introduction",
    "href": "examples/anomalydetection.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection is a crucial task in time series forecasting. It involves identifying unusual observations that don’t follow the expected dataset patterns. Anomalies, also known as outliers, can be caused by a variety of factors, such as errors in the data collection process, sudden changes in the underlying patterns of the data, or unexpected events. They can pose problems for many forecasting models since they can distort trends, seasonal patterns, or autocorrelation estimates. As a result, anomalies can have a significant impact on the accuracy of the forecasts, and for this reason, it is essential to be able to identify them. Furthermore, anomaly detection has many applications across different industries, such as detecting fraud in financial data, monitoring the performance of online services, or identifying usual patterns in energy usage.\nBy the end of this tutorial, you’ll have a good understanding of how to detect anomalies in time series data using StatsForecast’s probabilistic models.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nRecover insample forecasts and identify anomalies\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce an anomaly has been identified, we must decide what to do with it. For example, we could remove it or replace it with another value. The correct course of action is context-dependent and beyond this notebook’s scope. Removing an anomaly will likely improve the accuracy of the forecast, but it can also underestimate the amount of randomness in the data.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "examples/anomalydetection.html#install-libraries",
    "href": "examples/anomalydetection.html#install-libraries",
    "title": "Anomaly Detection",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "examples/anomalydetection.html#load-and-explore-the-data",
    "href": "examples/anomalydetection.html#load-and-explore-the-data",
    "title": "Anomaly Detection",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use the hourly dataset of the M4 Competition. We’ll first import the data from datasetsforecast, which you can install using pip install datasetsforecast\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m4 import M4\n\nThe function to load the data is M4.load. It requieres the following two arguments:\n\ndirectory: (str) The directory where the data will be downloaded.\ngroup: (str). The group name, which can be Yearly, Quarterly, Monthly, Weekly, Daily or Hourly.\n\nThis function returns multiple outputs, but only the first one with the target series is needed.\n\ndf_total, *_ = M4.load('./data', 'Hourly')\ndf_total.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n    \n    \n      1\n      H1\n      2\n      586.0\n    \n    \n      2\n      H1\n      3\n      586.0\n    \n    \n      3\n      H1\n      4\n      559.0\n    \n    \n      4\n      H1\n      5\n      511.0\n    \n  \n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, df and y.\n\nunique_id: (string, int or category) A unique identifier for the series.\nds: (datestamp or int) A datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS or an integer indexing time.\ny: (numeric) The measurement we wish to forecast.\n\nIn this case, the unique_id and y columns already have the requiered format, but we need to change the data type of the ds column.\n\ndf_total['ds'] = df_total['ds'].astype(int)\n\nFrom this dataset, we’ll select the first 8 time series to reduce the total execution time. You can select any number you want by changing the value of n_series.\n\nn_series = 8 \nuids = df_total['unique_id'].unique()[:n_series]\ndf = df_total.query('unique_id in @uids')\n\nWe can plot these series using the plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nunique_ids: (list[str]) A list with the ids of the time series we want to plot.\nplot_random: (bool = True) Plots the time series randomly.\nplot_anomalies: (bool = False) Plots anomalies for each prediction interval.\nengine: (str = plotly). The library used to generate the plots. It can also be matplotlib for static plots.\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(df, plot_random = False)"
  },
  {
    "objectID": "examples/anomalydetection.html#train-model",
    "href": "examples/anomalydetection.html#train-model",
    "title": "Anomaly Detection",
    "section": "Train model",
    "text": "Train model\nTo generate the forecast, we’ll use the MSTL model, which is well-suited for low-frequency data like the one used here. We first need to import it from statsforecast.models and then we need to instantiate it. Since we’re using hourly data, we have two seasonal periods: one every 24 hours (hourly) and one every 24*7 hours (daily). Hence, we need to set season_length = [24, 24*7].\n\nfrom statsforecast.models import MSTL\n\n# Create a list of models and instantiation parameters \nmodels = [MSTL(season_length = [24, 24*7])]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = df, \n    models = models, \n    freq = 'H', \n    n_jobs = -1\n)\n\nWe’ll now predict the next 48 hours. To do this, we’ll use the forecast method, which requieres the following arguments:\n\nh: (int) The forecasting horizon.\nlevel: (list[float]) The confidence levels of the prediction intervals\nfitted: (bool = False) Returns insample predictions.\n\nIt is important that we select a level and set fitted = True since we’ll need the insample forecasts and their prediction intervals to detect the anomalies.\n\nhorizon = 48\nlevels = [99] \n\nfcst = sf.forecast(h = 48, level = levels, fitted = True)\nfcst = fcst.reset_index()\nfcst.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      MSTL\n      MSTL-lo-99\n      MSTL-hi-99\n    \n  \n  \n    \n      0\n      H1\n      749\n      615.943970\n      597.662170\n      634.225708\n    \n    \n      1\n      H1\n      750\n      559.297791\n      531.316650\n      587.278931\n    \n    \n      2\n      H1\n      751\n      515.693542\n      479.151337\n      552.235718\n    \n    \n      3\n      H1\n      752\n      480.719269\n      436.241547\n      525.197021\n    \n    \n      4\n      H1\n      753\n      467.146484\n      415.199738\n      519.093262\n    \n  \n\n\n\n\nWe can plot the forecasts using the plot method from before.\n\nStatsForecast.plot(df, fcst, plot_random = False)"
  },
  {
    "objectID": "examples/anomalydetection.html#recover-insample-forecasts-and-identify-anomalies",
    "href": "examples/anomalydetection.html#recover-insample-forecasts-and-identify-anomalies",
    "title": "Anomaly Detection",
    "section": "Recover insample forecasts and identify anomalies",
    "text": "Recover insample forecasts and identify anomalies\nIn this example, an anomaly will be any observation outside the prediction interval of the insample forecasts for a given confidence level (here we selected 99%). Hence, we first need to recover the insample forecasts using the forecast_fitted_values method.\n\ninsample_forecasts = sf.forecast_fitted_values().reset_index()\ninsample_forecasts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      MSTL\n      MSTL-lo-99\n      MSTL-hi-99\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n      604.924500\n      588.010376\n      621.838623\n    \n    \n      1\n      H1\n      2\n      586.0\n      585.221802\n      568.307678\n      602.135925\n    \n    \n      2\n      H1\n      3\n      586.0\n      589.740723\n      572.826599\n      606.654846\n    \n    \n      3\n      H1\n      4\n      559.0\n      557.778076\n      540.863953\n      574.692200\n    \n    \n      4\n      H1\n      5\n      511.0\n      506.747009\n      489.832886\n      523.661133\n    \n  \n\n\n\n\nWe can now find all the observations above or below the 99% prediction interval for the insample forecasts.\n\nanomalies = insample_forecasts.loc[(insample_forecasts['y'] >= insample_forecasts['MSTL-hi-99']) | (insample_forecasts['y'] <= insample_forecasts['MSTL-lo-99'])]\nanomalies.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      MSTL\n      MSTL-lo-99\n      MSTL-hi-99\n    \n  \n  \n    \n      168\n      H1\n      169\n      813.0\n      779.849792\n      762.935669\n      796.763916\n    \n    \n      279\n      H1\n      280\n      692.0\n      672.638123\n      655.723999\n      689.552246\n    \n    \n      289\n      H1\n      290\n      770.0\n      792.015442\n      775.101318\n      808.929565\n    \n    \n      308\n      H1\n      309\n      844.0\n      867.809387\n      850.895203\n      884.723511\n    \n    \n      336\n      H1\n      337\n      853.0\n      822.427002\n      805.512878\n      839.341187\n    \n  \n\n\n\n\nWe can plot the anomalies by adding the plot_anomalies = True argument to the plot method.\n\nStatsForecast.plot(insample_forecasts, plot_random = False, plot_anomalies = True)\n\n\n                                                \n\n\nIf we want to take a closer look, we can use the unique_ids argument to select one particular time series, for example, H10.\n\nStatsForecast.plot(insample_forecasts, unique_ids = ['H10'], plot_anomalies = True)\n\n\n                                                \n\n\nHere we identified the anomalies in the data using the MSTL model, but any probabilistic model from StatsForecast can be used. We also selected the 99% prediction interval of the insample forecasts, but other confidence levels can be used as well."
  },
  {
    "objectID": "examples/models_intro.html",
    "href": "examples/models_intro.html",
    "title": "StatsForecast’s Models",
    "section": "",
    "text": "Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n✅\n✅\n✅\n✅\n\n\nAutoETS\n✅\n✅\n✅\n✅\n\n\nAutoCES\n✅\n✅\n✅\n✅\n\n\nAutoTheta\n✅\n✅\n✅\n✅\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/models_intro.html#arima-family",
    "href": "examples/models_intro.html#arima-family",
    "title": "StatsForecast’s Models",
    "section": "ARIMA Family",
    "text": "ARIMA Family\nThese models exploit the existing autocorrelations in the time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nARIMA\n✅\n✅\n✅\n✅\n\n\nAutoRegressive\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "examples/models_intro.html#theta-family",
    "href": "examples/models_intro.html#theta-family",
    "title": "StatsForecast’s Models",
    "section": "Theta Family",
    "text": "Theta Family\nFit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n✅\n✅\n✅\n✅\n\n\nOptimizedTheta\n✅\n✅\n✅\n✅\n\n\nDynamicTheta\n✅\n✅\n✅\n✅\n\n\nDynamicOptimizedTheta\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "examples/models_intro.html#multiple-seasonalities",
    "href": "examples/models_intro.html#multiple-seasonalities",
    "title": "StatsForecast’s Models",
    "section": "Multiple Seasonalities",
    "text": "Multiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "examples/models_intro.html#baseline-models",
    "href": "examples/models_intro.html#baseline-models",
    "title": "StatsForecast’s Models",
    "section": "Baseline Models",
    "text": "Baseline Models\nClassical models for establishing baseline.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n✅\n✅\n✅\n✅\n\n\nNaive\n✅\n✅\n✅\n✅\n\n\nRandomWalkWithDrift\n✅\n✅\n✅\n✅\n\n\nSeasonalNaive\n✅\n✅\n✅\n✅\n\n\nWindowAverage\n✅\n\n\n\n\n\nSeasonalWindowAverage\n✅"
  },
  {
    "objectID": "examples/models_intro.html#exponential-smoothing",
    "href": "examples/models_intro.html#exponential-smoothing",
    "title": "StatsForecast’s Models",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with clear trend and/or seasonality. Use the SimpleExponential family for data with no clear trend or seasonality.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n✅\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n✅\n\n\n\n\n\nSeasonalExponentialSmoothing\n✅\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n✅\n\n\n\n\n\nHolt\n✅\n✅\n✅\n✅\n\n\nHoltWinters\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "examples/models_intro.html#sparse-of-inttermitent",
    "href": "examples/models_intro.html#sparse-of-inttermitent",
    "title": "StatsForecast’s Models",
    "section": "Sparse of Inttermitent",
    "text": "Sparse of Inttermitent\nSuited for series with very few non-zero observations\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n✅\n\n\n\n\n\nCrostonClassic\n✅\n\n\n\n\n\nCrostonOptimized\n✅\n\n\n\n\n\nCrostonSBA\n✅\n\n\n\n\n\nIMAPA\n✅\n\n\n\n\n\nTSB\n✅"
  },
  {
    "objectID": "examples/ets_ray_m5.html",
    "href": "examples/ets_ray_m5.html",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "",
    "text": "In this notebook we show how to use StatsForecast and ray to forecast thounsands of time series in less than 6 minutes (M5 dataset). Also, we show that StatsForecast has better performance in time and accuracy compared to Prophet running on a Spark cluster using DataBricks.\nIn this example, we used a ray cluster (AWS) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM).\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/ets_ray_m5.html#installing-statsforecast-library",
    "href": "examples/ets_ray_m5.html#installing-statsforecast-library",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install \"statsforecast[ray]\" neuralforecast s3fs pyarrow\n\n\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import ETS"
  },
  {
    "objectID": "examples/ets_ray_m5.html#download-data",
    "href": "examples/ets_ray_m5.html#download-data",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Download data",
    "text": "Download data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nY_df = pd.read_parquet('s3://m5-benchmarks/data/train/target.parquet')\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      3.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      4.0\n    \n  \n\n\n\n\nSince the M5 dataset contains intermittent time series, we add a constant to avoid problems during the training phase. Later, we will substract the constant from the forecasts.\n\nconstant = 10\nY_df['y'] += constant"
  },
  {
    "objectID": "examples/ets_ray_m5.html#train-the-model",
    "href": "examples/ets_ray_m5.html#train-the-model",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Train the model",
    "text": "Train the model\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality. Observe that we need to pass the ray address to the ray_address argument.\n\nfcst = StatsForecast(\n    df=Y_df, \n    models=[ETS(season_length=7, model='ZNA')], \n    freq='D', \n    #n_jobs=-1\n    ray_address='ray://ADDRESS:10001'\n)\n\n\ninit = time()\nY_hat = fcst.forecast(28)\nend = time()\nprint(f'Minutes taken by StatsForecast using: {(end - init) / 60}')\n\n/home/ubuntu/miniconda/envs/ray/lib/python3.7/site-packages/ray/util/client/worker.py:618: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.f7ins22n6nyl. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.1afmymq455wu\n  UserWarning,\n\n\nMinutes taken by StatsForecast using: 5.4817593971888225\n\n\nStatsForecast and ray took only 5.48 minutes to train 30,490 time series, compared to 18.23 minutes for Prophet and Spark.\nWe remove the constant.\n\nY_hat['ETS'] -= constant\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = Y_hat.reset_index().set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n\n100%|███████████████████████████████████████████████████████████| 50.2M/50.2M [00:00<00:00, 77.1MiB/s]\n\n\n\nM5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n  \n    \n      \n      wrmsse\n    \n  \n  \n    \n      Total\n      0.677233\n    \n    \n      Level1\n      0.435558\n    \n    \n      Level2\n      0.522863\n    \n    \n      Level3\n      0.582109\n    \n    \n      Level4\n      0.488484\n    \n    \n      Level5\n      0.567825\n    \n    \n      Level6\n      0.587605\n    \n    \n      Level7\n      0.662774\n    \n    \n      Level8\n      0.647712\n    \n    \n      Level9\n      0.732107\n    \n    \n      Level10\n      1.013124\n    \n    \n      Level11\n      0.970465\n    \n    \n      Level12\n      0.916175\n    \n  \n\n\n\n\nAlso, StatsForecast is more accurate than Prophet, since the overall WMRSSE is 0.68, against 0.77 obtained by prophet."
  },
  {
    "objectID": "examples/migrating_R.html",
    "href": "examples/migrating_R.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/prophet_spark_m5.html",
    "href": "examples/prophet_spark_m5.html",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "",
    "text": "The purpose of this notebook is to create a scalability benchmark (time and performance). To that end, Nixtla’s StatsForecast (using the ETS model) is trained on the M5 dataset using spark to distribute the training. As a comparison, Facebook’s Prophet model is used.\nAn AWS cluster (mounted on databricks) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM) with runtime 10.4 LTS was used. This notebook was used as base case.\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/prophet_spark_m5.html#main-results",
    "href": "examples/prophet_spark_m5.html#main-results",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Main results",
    "text": "Main results\n\n\n\nMethod\nTime (mins)\nPerformance (wRMSSE)\n\n\n\n\nStatsForecast\n7.5\n0.68\n\n\nProphet\n18.23\n0.77"
  },
  {
    "objectID": "examples/prophet_spark_m5.html#installing-libraries",
    "href": "examples/prophet_spark_m5.html#installing-libraries",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Installing libraries",
    "text": "Installing libraries\n\npip install prophet \"neuralforecast<1.0.0\" \"statsforecast[fugue]\""
  },
  {
    "objectID": "examples/prophet_spark_m5.html#statsforecast-pipeline",
    "href": "examples/prophet_spark_m5.html#statsforecast-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "StatsForecast pipeline",
    "text": "StatsForecast pipeline\n\nfrom time import time\n\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import ETS, SeasonalNaive\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\n\n\n\n\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\n\n\n\n\n\nForecast\nWith statsforecast you don’t have to download your data. The distributed backend can handle a file with your data.\n\ninit = time()\nets_forecasts = backend.forecast(\n    \"s3://m5-benchmarks/data/train/m5-target.parquet\", \n    [ETS(season_length=7, model='ZAA')], \n    freq=\"D\", \n    h=28, \n).toPandas()\nend = time()\nprint(f'Minutes taken by StatsForecast on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by StatsForecast on a Spark cluster: 7.471468730767568\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = ets_forecasts.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\nwrmsse_ets = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\nwrmsse_ets\n\n\nOut[14]: \n\n\n\n\n\n\n  \n    \n      \n      wrmsse\n    \n  \n  \n    \n      Total\n      0.682358\n    \n    \n      Level1\n      0.449115\n    \n    \n      Level2\n      0.533754\n    \n    \n      Level3\n      0.592317\n    \n    \n      Level4\n      0.497086\n    \n    \n      Level5\n      0.572189\n    \n    \n      Level6\n      0.593880\n    \n    \n      Level7\n      0.665358\n    \n    \n      Level8\n      0.652183\n    \n    \n      Level9\n      0.734492\n    \n    \n      Level10\n      1.012633\n    \n    \n      Level11\n      0.969902\n    \n    \n      Level12\n      0.915380"
  },
  {
    "objectID": "examples/prophet_spark_m5.html#prophet-pipeline",
    "href": "examples/prophet_spark_m5.html#prophet-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Prophet pipeline",
    "text": "Prophet pipeline\n\nimport logging\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom prophet import Prophet\nfrom pyspark.sql.types import *\n\n# disable informational messages from prophet\nlogging.getLogger('py4j').setLevel(logging.ERROR)\n\n\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\n\n\n\n\nDownload data\n\n# structure of the training data set\ntrain_schema = StructType([\n  StructField('unique_id', StringType()),  \n  StructField('ds', DateType()),\n  StructField('y', DoubleType())\n  ])\n \n# read the training file into a dataframe\ntrain = spark.read.parquet(\n  's3://m5-benchmarks/data/train/m5-target.parquet', \n  header=True, \n  schema=train_schema\n )\n \n# make the dataframe queriable as a temporary view\ntrain.createOrReplaceTempView('train')\n\n\n\n\n\n\nsql_statement = '''\n  SELECT\n    unique_id AS unique_id,\n    CAST(ds as date) as ds,\n    y as y\n  FROM train\n  '''\n \nm5_history = (\n  spark\n    .sql( sql_statement )\n    .repartition(sc.defaultParallelism, ['unique_id'])\n  ).cache()\n\n\n\n\n\n\n\nForecast function using Prophet\n\ndef forecast( history_pd: pd.DataFrame ) -> pd.DataFrame:\n  \n  # TRAIN MODEL AS BEFORE\n  # --------------------------------------\n  # remove missing values (more likely at day-store-item level)\n    history_pd = history_pd.dropna()\n\n    # configure the model\n    model = Prophet(\n        growth='linear',\n        daily_seasonality=False,\n        weekly_seasonality=True,\n        yearly_seasonality=True,\n        seasonality_mode='multiplicative'\n    )\n\n    # train the model\n    model.fit( history_pd )\n    # --------------------------------------\n\n    # BUILD FORECAST AS BEFORE\n    # --------------------------------------\n    # make predictions\n    future_pd = model.make_future_dataframe(\n        periods=28, \n        freq='d', \n        include_history=False\n    )\n    forecast_pd = model.predict( future_pd )  \n    # --------------------------------------\n\n    # ASSEMBLE EXPECTED RESULT SET\n    # --------------------------------------\n    # get relevant fields from forecast\n    forecast_pd['unique_id'] = history_pd['unique_id'].unique()[0]\n    f_pd = forecast_pd[['unique_id', 'ds','yhat']]\n    # --------------------------------------\n\n    # return expected dataset\n    return f_pd\n\n\n\n\n\n\nresult_schema = StructType([\n  StructField('unique_id', StringType()), \n  StructField('ds',DateType()),\n  StructField('yhat',FloatType()),\n])\n\n\n\n\n\n\nTraining Prophet on the M5 dataset\n\ninit = time()\nresults = (\n  m5_history\n    .groupBy('unique_id')\n      .applyInPandas(forecast, schema=result_schema)\n    ).toPandas()\nend = time()\nprint(f'Minutes taken by Prophet on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by Prophet on a Spark cluster: 18.23116923570633\n\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = results.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\n\n\n\n\nwrmsse = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n\nwrmsse\n\n\nOut[10]: \n\n\n\n\n\n\n  \n    \n      \n      wrmsse\n    \n  \n  \n    \n      Total\n      0.771800\n    \n    \n      Level1\n      0.507905\n    \n    \n      Level2\n      0.586328\n    \n    \n      Level3\n      0.666686\n    \n    \n      Level4\n      0.549358\n    \n    \n      Level5\n      0.655003\n    \n    \n      Level6\n      0.647176\n    \n    \n      Level7\n      0.747047\n    \n    \n      Level8\n      0.743422\n    \n    \n      Level9\n      0.824667\n    \n    \n      Level10\n      1.207069\n    \n    \n      Level11\n      1.108780\n    \n    \n      Level12\n      1.018163"
  },
  {
    "objectID": "examples/uncertaintyintervals.html",
    "href": "examples/uncertaintyintervals.html",
    "title": "Probabilistic forecasting",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#introduction",
    "href": "examples/uncertaintyintervals.html#introduction",
    "title": "Probabilistic forecasting",
    "section": "Introduction",
    "text": "Introduction\nWhen we generate a forecast, we usually produce a single value known as the point forecast. This value, however, doesn’t tell us anything about the uncertainty associated with the forecast. To have a measure of this uncertainty, we need prediction intervals.\nA prediction interval is a range of values that the forecast can take with a given probability. Hence, a 95% prediction interval should contain a range of values that include the actual future value with probability 95%. Probabilistic forecasting aims to generate the full forecast distribution. Point forecasting, on the other hand, usually returns the mean or the median or said distribution. However, in real-world scenarios, it is better to forecast not only the most probable future outcome, but many alternative outcomes as well.\nStatsForecast has many models that can generate point forecasts. It also has probabilistic models than generate the same point forecasts and their prediction intervals. These models are stochastic data generating processes that can produce entire forecast distributions. By the end of this tutorial, you’ll have a good understanding of the probabilistic models available in StatsForecast and will be able to use them to generate point forecasts and prediction intervals. Furthermore, you’ll also learn how to generate plots with the historical data, the point forecasts, and the prediction intervals.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the terms are often confused, prediction intervals are not the same as confidence intervals.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn practice, most prediction intervals are too narrow since models do not account for all sources of uncertainty. A discussion about this can be found here.\n\n\nOutline:\n\nInstall libraries\nLoad and explore the data\nTrain models\nPlot prediction intervals\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#install-libraries",
    "href": "examples/uncertaintyintervals.html#install-libraries",
    "title": "Probabilistic forecasting",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#load-and-explore-the-data",
    "href": "examples/uncertaintyintervals.html#load-and-explore-the-data",
    "title": "Probabilistic forecasting",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use the hourly dataset from the M4 Competition. We first need to download the data from a URL and then load it as a pandas dataframe. Notice that we’ll load the train and the test data separately. We’ll also rename the y column of the test data as y_test.\n\nimport pandas as pd \n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n    \n    \n      1\n      H1\n      2\n      586.0\n    \n    \n      2\n      H1\n      3\n      586.0\n    \n    \n      3\n      H1\n      4\n      559.0\n    \n    \n      4\n      H1\n      5\n      511.0\n    \n  \n\n\n\n\n\ntest.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y_test\n    \n  \n  \n    \n      0\n      H1\n      701\n      619.0\n    \n    \n      1\n      H1\n      702\n      565.0\n    \n    \n      2\n      H1\n      703\n      532.0\n    \n    \n      3\n      H1\n      704\n      495.0\n    \n    \n      4\n      H1\n      705\n      481.0\n    \n  \n\n\n\n\nSince the goal of this notebook is to generate prediction intervals, we’ll only use the first 8 series of the dataset to reduce the total computational time.\n\nn_series = 8 \nuids = train['unique_id'].unique()[:n_series] # select first n_series of the dataset\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\nWe can plot these series using the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: bool = True. Plots the time series randomly.\nmodels: List[str]. A list with the models we want to plot.\nlevel: List[float]. A list with the prediction intervals we want to plot.\nengine: str = plotly. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(train, test, plot_random = False)\n\n/Users/fedex/projects/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#train-models",
    "href": "examples/uncertaintyintervals.html#train-models",
    "title": "Probabilistic forecasting",
    "section": "Train models",
    "text": "Train models\nStatsForecast can train multiple models on different time series efficiently. Most of these models can generate a probabilistic forecast, which means that they can produce both point forecasts and prediction intervals.\nFor this example, we’ll use AutoETS and the following baseline models:\n\nHistoricAverage\nNaive\nRandomWalkWithDrift\nSeasonalNaive\n\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them. Given that we’re working with hourly data, we need to set seasonal_length=24 in the models that requiere this parameter.\n\nfrom statsforecast.models import (\n    AutoETS, \n    HistoricAverage, \n    Naive, \n    RandomWalkWithDrift, \n    SeasonalNaive\n)\n\n# Create a list of models and instantiation parameters \nmodels = [\n    AutoETS(season_length=24),\n    HistoricAverage(), \n    Naive(), \n    RandomWalkWithDrift(), \n    SeasonalNaive(season_length=24)\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df=train, \n    models=models, \n    freq='H', \n    n_jobs=-1\n)\n\nNow we’re ready to generate the point forecasts and the prediction intervals. To do this, we’ll use the forecast method, which takes two arguments:\n\nh: An integer that represent the forecasting horizon. In this case, we’ll forecast the next 48 hours.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nlevels = [80, 90, 95, 99] # confidence levels of the prediction intervals \n\nforecasts = sf.forecast(h=48, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoETS\n      AutoETS-lo-99\n      AutoETS-lo-95\n      AutoETS-lo-90\n      AutoETS-lo-80\n      AutoETS-hi-80\n      AutoETS-hi-90\n      AutoETS-hi-95\n      ...\n      RWD-hi-99\n      SeasonalNaive\n      SeasonalNaive-lo-80\n      SeasonalNaive-lo-90\n      SeasonalNaive-lo-95\n      SeasonalNaive-lo-99\n      SeasonalNaive-hi-80\n      SeasonalNaive-hi-90\n      SeasonalNaive-hi-95\n      SeasonalNaive-hi-99\n    \n  \n  \n    \n      0\n      H1\n      701\n      631.889587\n      533.371826\n      556.926819\n      568.978882\n      582.874084\n      680.905090\n      694.800354\n      706.852356\n      ...\n      789.416626\n      691.0\n      582.823792\n      552.157349\n      525.558777\n      473.573395\n      799.176208\n      829.842651\n      856.441223\n      908.426575\n    \n    \n      1\n      H1\n      702\n      559.750854\n      460.738586\n      484.411835\n      496.524353\n      510.489288\n      609.012329\n      622.977295\n      635.089844\n      ...\n      833.254150\n      618.0\n      509.823822\n      479.157379\n      452.558807\n      400.573395\n      726.176208\n      756.842651\n      783.441223\n      835.426575\n    \n    \n      2\n      H1\n      703\n      519.235474\n      419.731232\n      443.522095\n      455.694794\n      469.729156\n      568.741821\n      582.776123\n      594.948853\n      ...\n      866.990601\n      563.0\n      454.823822\n      424.157379\n      397.558807\n      345.573395\n      671.176208\n      701.842651\n      728.441223\n      780.426575\n    \n    \n      3\n      H1\n      704\n      486.973358\n      386.979523\n      410.887451\n      423.120056\n      437.223480\n      536.723267\n      550.826660\n      563.059265\n      ...\n      895.510132\n      529.0\n      420.823822\n      390.157379\n      363.558807\n      311.573395\n      637.176208\n      667.842651\n      694.441223\n      746.426575\n    \n    \n      4\n      H1\n      705\n      464.697357\n      364.216339\n      388.240753\n      400.532959\n      414.705078\n      514.689636\n      528.861755\n      541.153992\n      ...\n      920.702881\n      504.0\n      395.823822\n      365.157379\n      338.558807\n      286.573395\n      612.176208\n      642.842651\n      669.441223\n      721.426575\n    \n  \n\n5 rows × 47 columns\n\n\n\nWe’ll now merge the forecasts and their prediction intervals with the test set. This will allow us generate the plots of each probabilistic model.\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#plot-prediction-intervals",
    "href": "examples/uncertaintyintervals.html#plot-prediction-intervals",
    "title": "Probabilistic forecasting",
    "section": "Plot prediction intervals",
    "text": "Plot prediction intervals\nTo plot the point and the prediction intervals, we’ll use the statsforecast.plot method again. Notice that now we also need to specify the model and the levels that we want to plot.\n\nAutoETS\n\nsf.plot(train, test, plot_random = False, models=['AutoETS'], level=levels)\n\n\n                                                \n\n\n\n\nHistoric Average\n\nsf.plot(train, test, plot_random = False, models=['HistoricAverage'], level=levels)\n\n\n                                                \n\n\n\n\nNaive\n\nsf.plot(train, test, plot_random = False, models=['Naive'], level=levels)\n\n\n                                                \n\n\n\n\nRandom Walk with Drift\n\nsf.plot(train, test, plot_random = False, models=['RWD'], level=levels)\n\n\n                                                \n\n\n\n\nSeasonal Naive\n\nsf.plot(train, test, plot_random = False, models=['SeasonalNaive'], level=levels)\n\n\n                                                \n\n\nFrom these plots, we can conclude that the uncertainty around each forecast varies according to the model that is being used. For the same time series, one model can predict a wider range of possible future values than others."
  },
  {
    "objectID": "examples/uncertaintyintervals.html#references",
    "href": "examples/uncertaintyintervals.html#references",
    "title": "Probabilistic forecasting",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, The Statistical Forecasting Perspective”."
  },
  {
    "objectID": "examples/installation.html",
    "href": "examples/installation.html",
    "title": "Install",
    "section": "",
    "text": "You can install the released version of StatsForecast from the Python package index with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\n\n\n\n\n\n\nWarning\n\n\n\nWe are constantly updating StatsForecast, so we suggest fixing the version to avoid issues. pip install statsforecast==\"1.0.0\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe recommend installing your libraries inside a python virtual or conda environment.\n\n\n\nUser our env (optional)\nIf you don’t have a Conda environment and need tools like Numba, Pandas, NumPy, Jupyter, StatsModels, and Nbdev you can use ours by following these steps:\n\nClone the StatsForecast repo:\n\n$ git clone https://github.com/Nixtla/statsforecast.git && cd statsforecast\n\nCreate the environment using the environment.yml file:\n\n$ conda env create -f environment.yml\n\nActivate the environment:\n\n$ conda activate statsforecast\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/forecastingatscale.html",
    "href": "examples/forecastingatscale.html",
    "title": "Forecasting at Scale",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html",
    "href": "examples/getting_started_with_auto_arima_and_ets.html",
    "title": "Forecast with ARIMA and ETS",
    "section": "",
    "text": "Tip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#introduction",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#introduction",
    "title": "Forecast with ARIMA and ETS",
    "section": "Introduction",
    "text": "Introduction\nAutomatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\nThe R’s programming language offers two great packages by Rob Hyndmann for Automatic forecasting:\n\nAutoARIMA and\nETS\n\nBoth models are highly accurate and reliable. Time series practitioners use them as reference or baseline models in a variety of forecasting tasks.\nBefore StatsForecast, similar alternatives -in terms of accuracy and computational efficiency- did not exist for the Python ecosystem. Seeking to bridge that Gap, we developed a new and highly efficient pure-Python implementation of these classic algorithms. In this notebook, we will show you how to use them."
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#install-the-statsforecast-library",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#install-the-statsforecast-library",
    "title": "Forecast with ARIMA and ETS",
    "section": "Install the StatsForecast Library",
    "text": "Install the StatsForecast Library\n\n!pip install statsforecast"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#load-the-data",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#load-the-data",
    "title": "Forecast with ARIMA and ETS",
    "section": "Load the Data",
    "text": "Load the Data\nFor this notebook, we will use the classical AirPassenger Data set. For simplicity’s sake, you can import it from StatsForecast.\n\nfrom statsforecast.utils import AirPassengersDF\n\nY_df = AirPassengersDF\n\nY_df.head()\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0\n    \n  \n\n\n\n\nSplit the dataset in train and test to evaluate your model at a later stage.\n\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 monthly observations for train\nY_test_df = Y_df[Y_df.ds>'1959-12-31'] # 12 monthly observations for test\n\n\n\n\n\n\n\nTip\n\n\n\nCross Validation or Backtesting is the recommended method to evaluate performance of time series models. Cross-validation consists in evaluating the performance of a certain model across different past windows. You can use the StatsForecast.cross_validation method from the StatsForecast class. See also: Cross Validation for Time Series.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, ETS\nfrom statsforecast.utils import AirPassengersDF"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#forecast-with-different-models",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#forecast-with-different-models",
    "title": "Forecast with ARIMA and ETS",
    "section": "Forecast with different models",
    "text": "Forecast with different models\nStatsForecast includes a wide range of models. For this example we will use two classical univarate models:\nETS: The exponential smoothing (ETS) algorithm is especially suited for data with seasonality and trend. ETS computes a weighted average over all observations in the input time series dataset as its prediction. In contrast to moving average methods with constant weights, ETS weights exponentially decrease over time, capturing long term dependencies while prioritizing new observations.\nAutoARIMA: The autoregressive integrated moving average (ARIMA), combines differencing steps, lag regression and moving averages into a single method capable of modeling non-stationary time series. This method complements on ETS and it is based on the description of data’s autocorrelations.\nIt is always a good idea to include benchmark models to have an estimate of how much accuracy we are gaining. For this exercise, we will use a Naive model.\n\nfrom statsforecast import StatsForecast #Imports the core StatsForecast class\nfrom statsforecast.models import AutoARIMA, ETS, Naive #Imports the models you will use\n\nDefine the parameters that you want to use in your models.\n\nseason_length = 12 # Monthly data \nhorizon = len(Y_test_df) # Predict the lenght of the test df\n\n# Include the models you imported\nmodels = [\n    AutoARIMA(season_length=season_length),\n    ETS(season_length=season_length),\n    Naive()\n]\n\n# Instansiate the StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_train_df,\n    models=models,\n    freq='M', \n    n_jobs=-1\n)\n\n# Forecast for the defined horizon\nY_hat_df = sf.forecast(horizon)\n\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      ETS\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1960-01-31\n      424.160156\n      406.651276\n      405.0\n    \n    \n      1.0\n      1960-02-29\n      407.081696\n      401.732910\n      405.0\n    \n    \n      1.0\n      1960-03-31\n      470.860535\n      456.289642\n      405.0\n    \n    \n      1.0\n      1960-04-30\n      460.913605\n      440.870514\n      405.0\n    \n    \n      1.0\n      1960-05-31\n      484.900879\n      440.333923\n      405.0\n    \n  \n\n\n\n\nFor efficiency’s sake, the forecast method converts the unique_id column to an index. You can revert to the default index of the data frame using the pd.reset_index method from Pandas.\n\nY_hat_df.reset_index()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n      ETS\n      Naive\n    \n  \n  \n    \n      0\n      1.0\n      1960-01-31\n      424.160156\n      406.651276\n      405.0\n    \n    \n      1\n      1.0\n      1960-02-29\n      407.081696\n      401.732910\n      405.0\n    \n    \n      2\n      1.0\n      1960-03-31\n      470.860535\n      456.289642\n      405.0\n    \n    \n      3\n      1.0\n      1960-04-30\n      460.913605\n      440.870514\n      405.0\n    \n    \n      4\n      1.0\n      1960-05-31\n      484.900879\n      440.333923\n      405.0\n    \n    \n      5\n      1.0\n      1960-06-30\n      536.903931\n      496.866058\n      405.0\n    \n    \n      6\n      1.0\n      1960-07-31\n      612.903198\n      545.839111\n      405.0\n    \n    \n      7\n      1.0\n      1960-08-31\n      623.903381\n      544.672485\n      405.0\n    \n    \n      8\n      1.0\n      1960-09-30\n      527.903320\n      477.034485\n      405.0\n    \n    \n      9\n      1.0\n      1960-10-31\n      471.903320\n      412.423096\n      405.0\n    \n    \n      10\n      1.0\n      1960-11-30\n      426.903320\n      357.949158\n      405.0\n    \n    \n      11\n      1.0\n      1960-12-31\n      469.903320\n      402.032745\n      405.0"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#plot-the-predictions",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#plot-the-predictions",
    "title": "Forecast with ARIMA and ETS",
    "section": "Plot the predictions",
    "text": "Plot the predictions\nPlot the forecasts (also known as Y hat) against the real values of test using Matplot lib.\n\nimport matplotlib.pyplot as plt\n\n\n# Merge the forecasts with the true values\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\nplot_df[['y', 'AutoARIMA', 'ETS']].plot(ax=ax, linewidth=2)\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#evaluate-the-predictions",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#evaluate-the-predictions",
    "title": "Forecast with ARIMA and ETS",
    "section": "Evaluate the predictions",
    "text": "Evaluate the predictions\nFinally, we evaluate the predictions accuracy using the Mean Absolute Error:\n\\[\n\\qquad MAE = \\frac{1}{Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}|\\qquad\n\\]\n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\ny_true = Y_test_df['y'].values\nets_preds = Y_hat_df['ETS'].values\narima_preds = Y_hat_df['AutoARIMA'].values\nnaive_preds = Y_hat_df['Naive'].values\n\nprint('ETS   MAE: %0.3f' % mae(ets_preds, y_true))\nprint('ARIMA MAE: %0.3f' % mae(arima_preds, y_true))\nprint('Naive MAE: %0.3f' % mae(naive_preds, y_true))\n\nETS   MAE: 35.612\nARIMA MAE: 18.551\nNaive MAE: 76.000\n\n\nThe best-performing model for this dataset is the ARIMA model. Notice that in both cases, our models are fare better estimates of the future than our baseline model.\n\n\n\n\n\n\nTip\n\n\n\nFor a complete list of available automatic forecasting models -as well as benchmark models- visit the model’s section of the documentation."
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#references",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#references",
    "title": "Forecast with ARIMA and ETS",
    "section": "References",
    "text": "References\nHyndman, RJ and Khandakar, Y (2008) “Automatic time series forecasting: The forecast package for R”, Journal of Statistical Software, 26(3)."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html",
    "href": "examples/electricitypeakforecasting.html",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#libraries",
    "href": "examples/electricitypeakforecasting.html#libraries",
    "title": "Detect Demand Peaks",
    "section": "Libraries",
    "text": "Libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#load-data",
    "href": "examples/electricitypeakforecasting.html#load-data",
    "title": "Detect Demand Peaks",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, download and read the 2022 historic total demand of the ERCOT market, available here. The data processing includes adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest. This step should take around 2s.\n\nimport numpy as np\nimport pandas as pd\n\n\n# Load data\nhistoric = pd.read_csv('./Native_Load_2022.csv')\n# Add missing hour due to daylight saving time\nhistoric = pd.concat([historic, pd.DataFrame({'Hour Ending':['03/13/2022 03:00'], 'ERCOT':['43980.57']})])\nhistoric = historic.sort_values('Hour Ending').reset_index(drop=True)\n# Convert to datetime\nhistoric['ERCOT'] = historic['ERCOT'].str.replace(',','').astype(float)\nhistoric = historic[~pd.isna(historic['ERCOT'])]\nhistoric['ds'] = pd.to_datetime(historic['Hour Ending'].str[:10]) + pd.to_timedelta(np.tile(range(24), len(historic)//24),'h')\nhistoric['unique_id'] = 'ERCOT'\nhistoric['y'] = historic['ERCOT']\n# Select relevant columns and dates\nY_df = historic[['unique_id', 'ds', 'y']]\nY_df = Y_df[Y_df['ds']<='2022-10-01']\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/Users/cchallu/NIXTLA/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nWe observe that the time series exhibits seasonal patterns. Moreover, the time series contains 6,552 observations, so it is necessary to use computationally efficient methods to deploy them in production."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "href": "examples/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "title": "Detect Demand Peaks",
    "section": "Fit and Forecast MSTL model",
    "text": "Fit and Forecast MSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\n\n\n\n\n\n\nTip\n\n\n\nCheck our detailed explanation and tutorial on MSTL here\n\n\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, instantiate the model and define the parameters. The electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities. See this link for a detailed explanation on how to set seasonal lengths. In this example we use the AutoARIMA model for the trend component, however, any StatsForecast model can be used. The complete list of models is available here.\n\nmodels = [MSTL(\n            season_length=[24, 24 * 7], # seasonalities of the time series \n            trend_forecaster=AutoARIMA(nmodels=10) # model used to forecast trend\n            )\n          ]\n\n\n\n\n\n\n\nTip\n\n\n\nThe parameter nmodels of the AutoARIMA controls the number of models considered in stepwise search. The default is 94, reduce it to decrease training times!\n\n\nWe fit the model by instantiating a StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nThe cross_validation method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. This method re-trains the model and forecast each window. See this tutorial for an animation of how the windows are defined.\nUse the cross_validation method to produce all the daily forecasts for September. To produce daily forecasts set the forecasting horizon h as 24. In this example we are simulating deploying the pipeline during September, so set the number of windows as 30 (one for each day). Finally, set the step size between windows as 24, to only produce one forecast per day.\n\ncrossvalidation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=30\n  )\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      MSTL\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      ERCOT\n      2022-09-01 00:00:00\n      2022-08-31 23:00:00\n      45482.468750\n      47126.179688\n    \n    \n      ERCOT\n      2022-09-01 01:00:00\n      2022-08-31 23:00:00\n      43602.660156\n      45088.542969\n    \n    \n      ERCOT\n      2022-09-01 02:00:00\n      2022-08-31 23:00:00\n      42284.820312\n      43897.175781\n    \n    \n      ERCOT\n      2022-09-01 03:00:00\n      2022-08-31 23:00:00\n      41663.160156\n      43187.812500\n    \n    \n      ERCOT\n      2022-09-01 04:00:00\n      2022-08-31 23:00:00\n      41710.621094\n      43369.859375\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using cross_validation make sure the forecasts are produced at the desired timestamps. Check the cutoff column which specifices the last timestamp before the forecasting window."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#peak-detection",
    "href": "examples/electricitypeakforecasting.html#peak-detection",
    "title": "Detect Demand Peaks",
    "section": "Peak Detection",
    "text": "Peak Detection\nFinally, we use the forecasts in crossvaldation_df to detect the daily hourly demand peaks. For each day, we set the detected peaks as the highest forecasts. In this case, we want to predict one peak (npeaks); depending on your setting and goals, this parameter might change. For example, the number of peaks can correspond to how many hours a battery can be discharged to reduce demand.\n\nnpeaks = 1 # Number of peaks\n\nFor the ERCOT 4CP detection task we are interested in correctly predicting the highest monthly load. Next, we filter the day in September with the highest hourly demand and predict the peak.\n\ncrossvalidation_df = crossvalidation_df.reset_index()[['ds','y','MSTL']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['MSTL'].argsort().iloc[-npeaks:].values # Predicted peaks\n\nIn the following plot we see how the MSTL model is able to correctly detect the coincident peak for September 2022.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(10, 5))\nplt.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nplt.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['MSTL'], color='green', label=f'Predicted Top-{npeaks}')\nplt.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nplt.plot(cv_df_day['ds'], cv_df_day['MSTL'], label='Forecast', color='red')\nplt.xlabel('Time')\nplt.ylabel('Load (MW)')\nplt.grid()\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this example we only include September. However, MSTL can correctly predict the peaks for the 4 months of 2022. You can try this by increasing the nwindows parameter of cross_validation or filtering the Y_df dataset. The complete run for all months take only 10 minutes."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#next-steps",
    "href": "examples/electricitypeakforecasting.html#next-steps",
    "title": "Detect Demand Peaks",
    "section": "Next steps",
    "text": "Next steps\nStatsForecast and MSTL in particular are good benchmarking models for peak detection. However, it might be useful to explore further and newer forecasting algorithms. We have seen particularly good results with the N-HiTS, a deep-learning model from Nixtla’s NeuralForecast library.\nLearn how to predict ERCOT demand peaks with our deep-learning N-HiTS model and the NeuralForecast library in this tutorial."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#references",
    "href": "examples/electricitypeakforecasting.html#references",
    "title": "Detect Demand Peaks",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/nondailydata.html",
    "href": "examples/nondailydata.html",
    "title": "Non Daily Data",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "examples/experiments.html",
    "href": "examples/experiments.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this set of self-contained experiments and benchmarks with other popular libraries in Python and R.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html",
    "href": "examples/statisticalneuralmethods.html",
    "title": "Statistical and Neural Forecasting methods",
    "section": "",
    "text": "In time series forecasting and analysis, it can be challenging to determine which model is best suited for a particular group of series. This decision often relies on intuition, which may not always align with reality.\nIn this tutorial, we will demonstrate a robust approach for selecting models for groups of series using the M5 benchmark dataset. We will train the following models:\nBy using the Nixtla libraries, we will be able to make more informed decisions on which models to use for specific groups of series in our dataset.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#installing-libraries",
    "href": "examples/statisticalneuralmethods.html#installing-libraries",
    "title": "Statistical and Neural Forecasting methods",
    "section": "Installing Libraries",
    "text": "Installing Libraries\n\n!pip install statsforecast datasetforecast s3fs pyarrow"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#download-and-prepare-data",
    "href": "examples/statisticalneuralmethods.html#download-and-prepare-data",
    "title": "Statistical and Neural Forecasting methods",
    "section": "Download and prepare data",
    "text": "Download and prepare data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nimport pandas as pd\n\n\nY_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet')\n\n\n#Feature: The Nixtlaverse takes as input a df with id's, timestamps and values.\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\n\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\nFor simplicity sake we will keep just one category\n\nY_df = Y_df.query('unique_id.str.startswith(\"FOODS_3\")')\n\nY_df['unique_id'] = Y_df['unique_id'].astype(str)"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#statsforecast",
    "href": "examples/statisticalneuralmethods.html#statsforecast",
    "title": "Statistical and Neural Forecasting methods",
    "section": "StatsForecast",
    "text": "StatsForecast\nStatsForecast offers a collection of popular univariate time series forecasting models optimized for high performance and scalability.\nFeatures:\n\nCollection of local models.\nSimple: training, forecasting and backtesting of many models in a few lines\nOptimized for speed\nScales horizontally with Spark, Dask, Ray\n\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality.\n\n# Import models\nfrom statsforecast.models import (\n    SeasonalNaive,\n    Naive,\n    HistoricAverage,\n    CrostonOptimized,\n    ADIDA,\n    IMAPA,\n    AutoETS\n)\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails. Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\nhorizon = 28\nmodels = [\n    SeasonalNaive(season_length=7),\n    Naive(),\n    HistoricAverage(),\n    CrostonOptimized(),\n    ADIDA(),\n    IMAPA(),\n    AutoETS(season_length=7)\n]\n\n\n# Instantiate statsforecast class\nsf = StatsForecast(\n    models=models, \n    freq='D', \n    n_jobs=-1,\n)\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\nfrom time import time\n\ninit = time()\nfcst_df = sf.forecast(df=Y_df, h=28, level=[90])\nend = time()\nprint(f'Forecast Minutes: {(end - init) / 60}')\n\nForecast Minutes: 2.4677709062894184\n\n\n\nfcst_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      SeasonalNaive\n      SeasonalNaive-lo-90\n      SeasonalNaive-hi-90\n      Naive\n      Naive-lo-90\n      Naive-hi-90\n      HistoricAverage\n      HistoricAverage-lo-90\n      HistoricAverage-hi-90\n      CrostonOptimized\n      ADIDA\n      IMAPA\n      AutoETS\n      AutoETS-lo-90\n      AutoETS-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      FOODS_3_001_CA_1\n      2016-05-23\n      1.0\n      -2.847174\n      4.847174\n      2.0\n      0.098363\n      3.901637\n      0.448738\n      -1.009579\n      1.907055\n      0.345192\n      0.345477\n      0.347249\n      0.381414\n      -1.028122\n      1.790950\n    \n    \n      FOODS_3_001_CA_1\n      2016-05-24\n      0.0\n      -3.847174\n      3.847174\n      2.0\n      -0.689321\n      4.689321\n      0.448738\n      -1.009579\n      1.907055\n      0.345192\n      0.345477\n      0.347249\n      0.286933\n      -1.124136\n      1.698003\n    \n    \n      FOODS_3_001_CA_1\n      2016-05-25\n      0.0\n      -3.847174\n      3.847174\n      2.0\n      -1.293732\n      5.293732\n      0.448738\n      -1.009579\n      1.907055\n      0.345192\n      0.345477\n      0.347249\n      0.334987\n      -1.077614\n      1.747588\n    \n    \n      FOODS_3_001_CA_1\n      2016-05-26\n      1.0\n      -2.847174\n      4.847174\n      2.0\n      -1.803274\n      5.803274\n      0.448738\n      -1.009579\n      1.907055\n      0.345192\n      0.345477\n      0.347249\n      0.186851\n      -1.227280\n      1.600982\n    \n    \n      FOODS_3_001_CA_1\n      2016-05-27\n      0.0\n      -3.847174\n      3.847174\n      2.0\n      -2.252190\n      6.252190\n      0.448738\n      -1.009579\n      1.907055\n      0.345192\n      0.345477\n      0.347249\n      0.308112\n      -1.107548\n      1.723771\n    \n  \n\n\n\n\n\nFeature Support for distibuted engines\n\n# You can use a ray, spark or dask cluster to\n# scale your forecasting task\n#from statsforecast.distributed.fugue import FugueBackend\n\n#backend = FugueBackend(spark, {'fugue.spark.use_pandas_udf':True})\n\n#sf_cluster = StatsForecast(\n#    models=models, \n#    freq='D', \n#    n_jobs=-1,\n#    backend=backend\n#)\n#"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#neuralforecast",
    "href": "examples/statisticalneuralmethods.html#neuralforecast",
    "title": "Statistical and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nNeuralForecast offers a large collection of neural forecasting models focused on their usability, and robustness. The models range from classic networks like MLP, RNNs to novel proven contributions like NBEATS, NHITS, TFT and other architectures.\nFeatures:\n\nCollection of global models.\nSimple: training, forecasting and backtesing of many models in a few lines.\nGPU Support\n\nThis machine doesn’t have GPU, but Google Colabs offers some for free.\nUsing Colab’s GPU to train NeuralForecast.\n\n# Read the results from Colab\nfcst_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/forecast-nf.parquet')\n\n\nfcst_nf_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoNHITS\n      AutoNHITS-lo-90\n      AutoNHITS-hi-90\n      AutoTFT\n      AutoTFT-lo-90\n      AutoTFT-hi-90\n    \n  \n  \n    \n      0\n      FOODS_3_001_CA_1\n      2016-05-23\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n    \n      1\n      FOODS_3_001_CA_1\n      2016-05-24\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n    \n      2\n      FOODS_3_001_CA_1\n      2016-05-25\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      1.0\n    \n    \n      3\n      FOODS_3_001_CA_1\n      2016-05-26\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n    \n      4\n      FOODS_3_001_CA_1\n      2016-05-27\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n  \n\n\n\n\n\n# Merge the two dataframes. \nfcst_df = fcst_df.merge(fcst_nf_df, how='left', on=['unique_id', 'ds'])\n\n\nfcst_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      SeasonalNaive\n      SeasonalNaive-lo-90\n      SeasonalNaive-hi-90\n      Naive\n      Naive-lo-90\n      Naive-hi-90\n      HistoricAverage\n      HistoricAverage-lo-90\n      ...\n      IMAPA\n      AutoETS\n      AutoETS-lo-90\n      AutoETS-hi-90\n      AutoNHITS\n      AutoNHITS-lo-90\n      AutoNHITS-hi-90\n      AutoTFT\n      AutoTFT-lo-90\n      AutoTFT-hi-90\n    \n  \n  \n    \n      0\n      FOODS_3_001_CA_1\n      2016-05-23\n      1.0\n      -2.847174\n      4.847174\n      2.0\n      0.098363\n      3.901637\n      0.448738\n      -1.009579\n      ...\n      0.347249\n      0.381414\n      -1.028122\n      1.790950\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n    \n      1\n      FOODS_3_001_CA_1\n      2016-05-24\n      0.0\n      -3.847174\n      3.847174\n      2.0\n      -0.689321\n      4.689321\n      0.448738\n      -1.009579\n      ...\n      0.347249\n      0.286933\n      -1.124136\n      1.698003\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n    \n      2\n      FOODS_3_001_CA_1\n      2016-05-25\n      0.0\n      -3.847174\n      3.847174\n      2.0\n      -1.293732\n      5.293732\n      0.448738\n      -1.009579\n      ...\n      0.347249\n      0.334987\n      -1.077614\n      1.747588\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      1.0\n    \n    \n      3\n      FOODS_3_001_CA_1\n      2016-05-26\n      1.0\n      -2.847174\n      4.847174\n      2.0\n      -1.803274\n      5.803274\n      0.448738\n      -1.009579\n      ...\n      0.347249\n      0.186851\n      -1.227280\n      1.600982\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n    \n      4\n      FOODS_3_001_CA_1\n      2016-05-27\n      0.0\n      -3.847174\n      3.847174\n      2.0\n      -2.252190\n      6.252190\n      0.448738\n      -1.009579\n      ...\n      0.347249\n      0.308112\n      -1.107548\n      1.723771\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      2.0\n    \n  \n\n5 rows × 23 columns"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#forecast-plots",
    "href": "examples/statisticalneuralmethods.html#forecast-plots",
    "title": "Statistical and Neural Forecasting methods",
    "section": "Forecast plots",
    "text": "Forecast plots\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3)\n\n\n                                                \n\n\nUse the plot function to explore models and ID’s\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3, \n        models=['CrostonOptimized', 'AutoNHITS', 'SeasonalNaive'])"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#statsforecast-1",
    "href": "examples/statisticalneuralmethods.html#statsforecast-1",
    "title": "Statistical and Neural Forecasting methods",
    "section": "StatsForecast",
    "text": "StatsForecast\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows (int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ninit = time()\ncv_df = sf.cross_validation(df=Y_df, h=horizon, n_windows=3, step_size=horizon, level=[90])\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\n/home/ubuntu/fede/statsforecast/statsforecast/ets.py:1039: RuntimeWarning:\n\ndivide by zero encountered in divide\n\n\n\nCV Minutes: 6.015557058652242\n\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      SeasonalNaive\n      SeasonalNaive-lo-90\n      SeasonalNaive-hi-90\n      Naive\n      Naive-lo-90\n      Naive-hi-90\n      HistoricAverage\n      HistoricAverage-lo-90\n      HistoricAverage-hi-90\n      CrostonOptimized\n      ADIDA\n      IMAPA\n      AutoETS\n      AutoETS-lo-90\n      AutoETS-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      FOODS_3_001_CA_1\n      2016-02-29\n      2016-02-28\n      0.0\n      2.0\n      -1.878885\n      5.878885\n      0.0\n      -1.917011\n      1.917011\n      0.449111\n      -1.021813\n      1.920036\n      0.618472\n      0.618375\n      0.617998\n      0.655286\n      -0.765731\n      2.076302\n    \n    \n      FOODS_3_001_CA_1\n      2016-03-01\n      2016-02-28\n      1.0\n      0.0\n      -3.878885\n      3.878885\n      0.0\n      -2.711064\n      2.711064\n      0.449111\n      -1.021813\n      1.920036\n      0.618472\n      0.618375\n      0.617998\n      0.568595\n      -0.853966\n      1.991155\n    \n    \n      FOODS_3_001_CA_1\n      2016-03-02\n      2016-02-28\n      1.0\n      0.0\n      -3.878885\n      3.878885\n      0.0\n      -3.320361\n      3.320361\n      0.449111\n      -1.021813\n      1.920036\n      0.618472\n      0.618375\n      0.617998\n      0.618805\n      -0.805298\n      2.042908\n    \n    \n      FOODS_3_001_CA_1\n      2016-03-03\n      2016-02-28\n      0.0\n      1.0\n      -2.878885\n      4.878885\n      0.0\n      -3.834023\n      3.834023\n      0.449111\n      -1.021813\n      1.920036\n      0.618472\n      0.618375\n      0.617998\n      0.455891\n      -0.969753\n      1.881534\n    \n    \n      FOODS_3_001_CA_1\n      2016-03-04\n      2016-02-28\n      0.0\n      1.0\n      -2.878885\n      4.878885\n      0.0\n      -4.286568\n      4.286568\n      0.449111\n      -1.021813\n      1.920036\n      0.618472\n      0.618375\n      0.617998\n      0.591197\n      -0.835987\n      2.018380"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#neuralforecast-1",
    "href": "examples/statisticalneuralmethods.html#neuralforecast-1",
    "title": "Statistical and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nThis machine doesn’t have GPU, but Google Colabs offers some for free.\nUsing Colab’s GPU to train NeuralForecast.\n\ncv_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/cross-validation-nf.parquet')\n\n\ncv_nf_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      cutoff\n      AutoNHITS\n      AutoNHITS-lo-90\n      AutoNHITS-hi-90\n      AutoTFT\n      AutoTFT-lo-90\n      AutoTFT-hi-90\n      y\n    \n  \n  \n    \n      0\n      FOODS_3_001_CA_1\n      2016-02-29\n      2016-02-28\n      0.0\n      0.0\n      2.0\n      1.0\n      0.0\n      2.0\n      0.0\n    \n    \n      1\n      FOODS_3_001_CA_1\n      2016-03-01\n      2016-02-28\n      0.0\n      0.0\n      2.0\n      1.0\n      0.0\n      2.0\n      1.0\n    \n    \n      2\n      FOODS_3_001_CA_1\n      2016-03-02\n      2016-02-28\n      0.0\n      0.0\n      2.0\n      1.0\n      0.0\n      2.0\n      1.0\n    \n    \n      3\n      FOODS_3_001_CA_1\n      2016-03-03\n      2016-02-28\n      0.0\n      0.0\n      2.0\n      1.0\n      0.0\n      2.0\n      0.0\n    \n    \n      4\n      FOODS_3_001_CA_1\n      2016-03-04\n      2016-02-28\n      0.0\n      0.0\n      2.0\n      1.0\n      0.0\n      2.0\n      0.0"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#merge-cross-validation-forecasts",
    "href": "examples/statisticalneuralmethods.html#merge-cross-validation-forecasts",
    "title": "Statistical and Neural Forecasting methods",
    "section": "Merge cross validation forecasts",
    "text": "Merge cross validation forecasts\n\ncv_df = cv_df.merge(cv_nf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#plots-cv",
    "href": "examples/statisticalneuralmethods.html#plots-cv",
    "title": "Statistical and Neural Forecasting methods",
    "section": "Plots CV",
    "text": "Plots CV\n\ncutoffs = cv_df['cutoff'].unique()\n\n\nfor cutoff in cutoffs:\n    sf.plot(Y_df, \n            cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n            max_insample_length=28 * 5, \n            unique_ids=['FOODS_3_001_CA_1'])\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\nAggregate Demand\n\nagg_cv_df = cv_df.loc[:,~cv_df.columns.str.contains('hi|lo')].groupby(['ds', 'cutoff']).sum(numeric_only=True).reset_index()\nagg_cv_df.insert(0, 'unique_id', 'agg_demand')\n\n\nagg_Y_df = Y_df.groupby(['ds']).sum(numeric_only=True).reset_index()\nagg_Y_df.insert(0, 'unique_id', 'agg_demand')\n\n\nfor cutoff in cutoffs:\n    sf.plot(agg_Y_df, \n            agg_cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']),\n            max_insample_length=28 * 5)"
  },
  {
    "objectID": "examples/statisticalneuralmethods.html#evaluation-per-series-and-cv-window",
    "href": "examples/statisticalneuralmethods.html#evaluation-per-series-and-cv-window",
    "title": "Statistical and Neural Forecasting methods",
    "section": "Evaluation per series and CV window",
    "text": "Evaluation per series and CV window\nIn this section we will evaluate the performance of each model for each time series and each cross validation window. Since we have many combinations, we will use dask to parallelize the evaluation. The parallelization will be done using fugue.\n\nfrom typing import List, Callable\n\nfrom distributed import Client\nfrom fugue import transform\nfrom fugue_dask import DaskExecutionEngine\nfrom datasetsforecast.losses import mse, mae, smape\n\nThe evaluate function receives a unique combination of a time series and a window, and calculates different metrics for each model in df.\n\ndef evaluate(df: pd.DataFrame, metrics: List[Callable]) -> pd.DataFrame:\n    eval_ = {}\n    models = df.loc[:, ~df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\n    for model in models:\n        eval_[model] = {}\n        for metric in metrics:\n            eval_[model][metric.__name__] = metric(df['y'], df[model])\n    eval_df = pd.DataFrame(eval_).rename_axis('metric').reset_index()\n    eval_df.insert(0, 'cutoff', df['cutoff'].iloc[0])\n    eval_df.insert(0, 'unique_id', df['unique_id'].iloc[0])\n    return eval_df\n\n\nstr_models = cv_df.loc[:, ~cv_df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\nstr_models = ','.join([f\"{model}:float\" for model in str_models])\ncv_df['cutoff'] = cv_df['cutoff'].astype(str)\ncv_df['unique_id'] = cv_df['unique_id'].astype(str)\n\nLet’s cleate a dask client.\n\nclient = Client() # without this, dask is not in distributed mode\n# fugue.dask.dataframe.default.partitions determines the default partitions for a new DaskDataFrame\nengine = DaskExecutionEngine({\"fugue.dask.dataframe.default.partitions\": 96})\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/distributed/dashboard/core.py:20: UserWarning:\n\n\nDask needs bokeh >= 2.4.2, < 3 for the dashboard.\nYou have bokeh==3.0.2.\nContinuing without the dashboard.\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/distributed/node.py:183: UserWarning:\n\nPort 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 37415 instead\n\n\n\nThe transform function takes the evaluate functions and applies it to each combination of time series (unique_id) and cross validation window (cutoff) using the dask client we created before.\n\nevaluation_df = transform(\n    cv_df.loc[:, ~cv_df.columns.str.contains('lo|hi')], \n    evaluate, \n    engine=engine, #\"dask\",\n    params={'metrics': [mse, mae, smape]}, \n    schema=f\"unique_id:str,cutoff:str,metric:str, {str_models}\", \n    as_local=True,\n    partition={'by': ['unique_id', 'cutoff']}\n)\n\n\nevaluation_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      cutoff\n      metric\n      SeasonalNaive\n      Naive\n      HistoricAverage\n      CrostonOptimized\n      ADIDA\n      IMAPA\n      AutoETS\n      AutoNHITS\n      AutoTFT\n    \n  \n  \n    \n      0\n      FOODS_3_018_CA_3\n      2016-03-27\n      mse\n      2.750000\n      2.035714\n      2.257121\n      1.965976\n      1.959041\n      1.967155\n      1.960783\n      2.035714\n      2.035714\n    \n    \n      1\n      FOODS_3_018_CA_3\n      2016-03-27\n      mae\n      1.250000\n      1.035714\n      1.127709\n      1.035714\n      1.035714\n      1.035714\n      1.045668\n      1.035714\n      1.035714\n    \n    \n      2\n      FOODS_3_018_CA_3\n      2016-03-27\n      smape\n      99.523811\n      84.761902\n      96.612183\n      84.034660\n      83.957138\n      84.047737\n      87.137726\n      84.761902\n      84.761902\n    \n    \n      3\n      FOODS_3_021_TX_1\n      2016-03-27\n      mse\n      1.000000\n      4.857143\n      4.788925\n      0.884868\n      0.903405\n      0.903405\n      0.721010\n      0.857143\n      0.857143\n    \n    \n      4\n      FOODS_3_021_TX_1\n      2016-03-27\n      mae\n      0.714286\n      2.000000\n      1.985319\n      0.785646\n      0.806466\n      0.806466\n      0.745452\n      0.714286\n      0.714286\n    \n  \n\n\n\n\n\n# Calculate the mean metric for each cross validation window\nevaluation_df.groupby(['cutoff', 'metric']).mean(numeric_only=True)\n\n\n\n\n\n  \n    \n      \n      \n      SeasonalNaive\n      Naive\n      HistoricAverage\n      CrostonOptimized\n      ADIDA\n      IMAPA\n      AutoETS\n      AutoNHITS\n      AutoTFT\n    \n    \n      cutoff\n      metric\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2016-02-28\n      mae\n      1.744289\n      2.040496\n      1.730704\n      1.633017\n      1.527965\n      1.528772\n      1.497553\n      1.434938\n      1.485419\n    \n    \n      mse\n      14.510710\n      19.080585\n      12.858994\n      11.785032\n      11.114498\n      11.100909\n      10.347871\n      10.010982\n      10.964664\n    \n    \n      smape\n      85.202042\n      87.719086\n      125.418488\n      124.749908\n      127.591858\n      127.704102\n      127.786316\n      79.132614\n      80.983368\n    \n    \n      2016-03-27\n      mae\n      1.795973\n      2.106449\n      1.754029\n      1.662087\n      1.570701\n      1.572741\n      1.535450\n      1.432412\n      1.502393\n    \n    \n      mse\n      14.810259\n      26.044472\n      12.804104\n      12.020620\n      12.083861\n      12.120033\n      11.315217\n      9.445867\n      10.762877\n    \n    \n      smape\n      87.407471\n      89.453247\n      123.587196\n      123.460030\n      123.428459\n      123.538521\n      123.610352\n      79.926781\n      82.013168\n    \n    \n      2016-04-24\n      mae\n      1.785983\n      1.990774\n      1.762506\n      1.609268\n      1.527627\n      1.529721\n      1.501773\n      1.447401\n      1.505127\n    \n    \n      mse\n      13.476350\n      16.234917\n      13.151311\n      10.647048\n      10.072225\n      10.062395\n      9.393267\n      9.363891\n      10.436214\n    \n    \n      smape\n      89.238815\n      90.685867\n      121.124947\n      119.721245\n      120.325401\n      120.345284\n      120.648216\n      81.402748\n      83.614029\n    \n  \n\n\n\n\nResults showed in previous experiments.\n\n\n\nmodel\nMSE\n\n\n\n\nMQCNN\n10.09\n\n\nDeepAR-student_t\n10.11\n\n\nDeepAR-lognormal\n30.20\n\n\nDeepAR\n9.13\n\n\nNPTS\n11.53\n\n\n\nTop 3 models: DeepAR, AutoNHITS, AutoETS.\n\nDistribution of errors\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nevaluation_df_melted = pd.melt(evaluation_df, id_vars=['unique_id', 'cutoff', 'metric'], var_name='model', value_name='error')\n\n\nSMAPE\n\nsns.violinplot(evaluation_df_melted.query('metric==\"smape\"'), x='error', y='model')\n\n<AxesSubplot: xlabel='error', ylabel='model'>\n\n\n\n\n\n\n\n\nChoose models for groups of series\nFeature:\n\nA unified dataframe with forecasts for all different models\nEasy Ensamble\nE.g. Average predictions\nOr MinMax (Choosing is ensembling)\n\n\n# Choose the best model for each time series, metric, and cross validation window\nevaluation_df['best_model'] = evaluation_df.idxmin(axis=1, numeric_only=True)\n# count how many times a model wins per metric and cross validation window\ncount_best_model = evaluation_df.groupby(['cutoff', 'metric', 'best_model']).size().rename('n').to_frame().reset_index()\n# plot results\nsns.barplot(count_best_model, x='n', y='best_model', hue='metric')\n\n<AxesSubplot: xlabel='n', ylabel='best_model'>\n\n\n\n\n\n\n\nEt pluribus unum: an inclusive forecasting Pie.\n\n# For the mse, calculate how many times a model wins\neval_series_df = evaluation_df.query('metric == \"mse\"').groupby(['unique_id']).mean(numeric_only=True)\neval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\ncounts_series = eval_series_df.value_counts('best_model')\nplt.pie(counts_series, labels=counts_series.index, autopct='%.0f%%')\nplt.show()\n\n\n\n\n\nsf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n        max_insample_length=28 * 6, \n        models=['AutoNHITS'],\n        unique_ids=eval_series_df.query('best_model == \"AutoNHITS\"').index[:8])"
  },
  {
    "objectID": "examples/contributing.html",
    "href": "examples/contributing.html",
    "title": "How to Contribute",
    "section": "",
    "text": "Ensure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/contributing.html#do-you-have-a-feature-request",
    "href": "examples/contributing.html#do-you-have-a-feature-request",
    "title": "How to Contribute",
    "section": "Do you have a feature request?",
    "text": "Do you have a feature request?\n\nEnsure that it hasn’t been yet implemented in the main branch of the repository and that there’s not an Issue requesting it yet.\nOpen a new issue and make sure to describe it clearly, mention how it improves the project and why its useful."
  },
  {
    "objectID": "examples/contributing.html#do-you-want-to-fix-a-bug-or-implement-a-feature",
    "href": "examples/contributing.html#do-you-want-to-fix-a-bug-or-implement-a-feature",
    "title": "How to Contribute",
    "section": "Do you want to fix a bug or implement a feature?",
    "text": "Do you want to fix a bug or implement a feature?\nBug fixes and features are added through pull requests (PRs)."
  },
  {
    "objectID": "examples/contributing.html#pr-submission-guidelines",
    "href": "examples/contributing.html#pr-submission-guidelines",
    "title": "How to Contribute",
    "section": "PR submission guidelines",
    "text": "PR submission guidelines\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nEnsure that your PR includes a test that fails without your patch, and passes with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\nLocal setup for working on a PR\n\n1. Clone the repository\n\nHTTPS: git clone https://github.com/Nixtla/statsforecast.git\nSSH: git clone git@github.com:Nixtla/statsforecast.git\nGitHub CLI: gh repo clone Nixtla/statsforecast\n\n\n\n2. Set up a conda environment\nThe repo comes with an environment.yml file which contains the libraries needed to run all the tests. In order to set up the environment you must have conda installed, we recommend miniconda.\nOnce you have conda go to the top level directory of the repository and run:\nconda env create -f environment.yml\n\n\n3. Install the library\nOnce you have your environment setup, activate it using conda activate statsforecast and then install the library in editable mode using pip install -e \".[dev]\"\n\n\n4. Install git hooks\nBefore doing any changes to the code, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts).\nnbdev_install_hooks\n\n\n\nBuilding the library\nThe library is built using the notebooks contained in the nbs folder. If you want to make any changes to the library you have to find the relevant notebook, make your changes and then call nbdev_export.\n\n\nRunning tests\nIf you’re working on the local interface you can just use nbdev_test.\n\n\nLinters\nThis project uses a couple of linters to validate different aspects of the code. Before opening a PR, please make sure that it passes all the linting tasks by following the next steps.\n\nRun the linting tasks\n\nmypy statsforecast/\nflake8 --select=F statsforecast/\n\n\n\n\nCleaning notebooks\nSince the notebooks output cells can vary from run to run (even if they produce the same outputs) the notebooks are cleaned before committing them. Please make sure to run nbdev_clean before committing your changes."
  },
  {
    "objectID": "examples/contributing.html#do-you-want-to-contribute-to-the-documentation",
    "href": "examples/contributing.html#do-you-want-to-contribute-to-the-documentation",
    "title": "How to Contribute",
    "section": "Do you want to contribute to the documentation?",
    "text": "Do you want to contribute to the documentation?\n\nDocs are automatically created from the notebooks in the nbs folder.\nIn order to modify the documentation:\n\nFind the relevant notebook.\nMake your changes.\nRun all cells.\nIf you are modifying library notebooks (not in nbs/examples), clean all outputs using Edit > Clear All Outputs.\nRun nbdev_preview."
  },
  {
    "objectID": "examples/outliers.html",
    "href": "examples/outliers.html",
    "title": "Outliers",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "examples/scale/spark.html",
    "href": "examples/scale/spark.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this content.\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/scale/dask.html",
    "href": "examples/scale/dask.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this content.\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/scale/ray.html",
    "href": "examples/scale/ray.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this content.\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/crossvalidation.html",
    "href": "examples/crossvalidation.html",
    "title": "Cross validation",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/crossvalidation.html#introduction",
    "href": "examples/crossvalidation.html#introduction",
    "title": "Cross validation",
    "section": "Introduction",
    "text": "Introduction\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it.\n\nStatsforecast has an implementation of time series cross-validation that is fast and easy to use. This implementation makes cross-validation a distributed operation, which makes it less time-consuming. In this notebook, we’ll use it on a subset of the M4 Competition hourly dataset.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nPerform time series cross-validation\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "examples/crossvalidation.html#install-libraries",
    "href": "examples/crossvalidation.html#install-libraries",
    "title": "Cross validation",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages with pip install statsforecast\n\npip install statsforecast\n\n\nfrom statsforecast import StatsForecast # required to instantiate StastForecast object and use cross-validation method"
  },
  {
    "objectID": "examples/crossvalidation.html#load-and-explore-the-data",
    "href": "examples/crossvalidation.html#load-and-explore-the-data",
    "title": "Cross validation",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nAs stated in the introduction, we’ll use the M4 Competition hourly dataset. We’ll first import the data from an URL using pandas.\n\nimport pandas as pd \n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet') # load the data \nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n    \n    \n      1\n      H1\n      2\n      586.0\n    \n    \n      2\n      H1\n      3\n      586.0\n    \n    \n      3\n      H1\n      4\n      559.0\n    \n    \n      4\n      H1\n      5\n      511.0\n    \n  \n\n\n\n\nThe input to StatsForecast is a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int, or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS.\nThe y (numeric) represents the measurement we wish to forecast.\n\nThe data in this example already has this format, so no changes are needed.\nTo keep the time required to execute this notebook to a minimum, we’ll only use one time series from the data, namely the one with unique_id == 'H1'. However, you can use as many as you want, with no additional changes to the code needed.\n\ndf = Y_df[Y_df['unique_id'] == 'H1'] # select time series\n\nWe can plot the time series we’ll work with using StatsForecast.plot method.\n\nStatsForecast.plot(df)"
  },
  {
    "objectID": "examples/crossvalidation.html#train-model",
    "href": "examples/crossvalidation.html#train-model",
    "title": "Cross validation",
    "section": "Train model",
    "text": "Train model\nFor this example, we’ll use StastForecast AutoETS. We first need to import it from statsforecast.models and then we need to instantiate a new StatsForecast object.\nThe StatsForecast object has the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. See panda’s available frequencies.\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame df.\n\nfrom statsforecast.models import AutoETS\n\nmodels = [AutoETS(season_length = 24)]\n\nsf = StatsForecast(\n    df = df, \n    models = models, \n    freq = 'H', \n    n_jobs = -1\n)"
  },
  {
    "objectID": "examples/crossvalidation.html#perform-time-series-cross-validation",
    "href": "examples/crossvalidation.html#perform-time-series-cross-validation",
    "title": "Cross validation",
    "section": "Perform time series cross-validation",
    "text": "Perform time series cross-validation\nOnce the StatsForecastobject has been instantiated, we can use the cross_validation method, which takes the following arguments:\n\ndf: training data frame with StatsForecast format\nh (int): represents the h steps into the future that will be forecasted\nstep_size (int): step size between each window, meaning how often do you want to run the forecasting process.\nn_windows (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\n\nFor this particular example, we’ll use 3 windows of 24 hours.\n\ncrossvalidation_df = sf.cross_validation(\n    df = df,\n    h = 24,\n    step_size = 24,\n    n_windows = 3\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      AutoETS\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      677\n      676\n      691.0\n      677.761047\n    \n    \n      H1\n      678\n      676\n      618.0\n      607.817871\n    \n    \n      H1\n      679\n      676\n      563.0\n      569.437744\n    \n    \n      H1\n      680\n      676\n      529.0\n      537.340027\n    \n    \n      H1\n      681\n      676\n      504.0\n      515.571106\n    \n  \n\n\n\n\nWe’ll now plot the forecast for each cutoff period. To make the plots clearer, we’ll rename the actual values in each period.\n\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \n\ncutoff = crossvalidation_df['cutoff'].unique()\n\nfor k in range(len(cutoff)): \n    cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n    StatsForecast.plot(df, cv.loc[:, cv.columns != 'cutoff'])\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nNotice that in each cutoff period, we generated a forecast for the next 24 hours using only the data y before said period."
  },
  {
    "objectID": "examples/crossvalidation.html#evaluate-results",
    "href": "examples/crossvalidation.html#evaluate-results",
    "title": "Cross validation",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we’ll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\npip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\n\nThe forecasts, in this case, AutoETS.\n\n\nrmse = rmse(crossvalidation_df['actual'], crossvalidation_df['AutoETS'])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  33.90342\n\n\nThis measure should better reflect the predictive abilities of our model, since it used different time periods to test its accuracy.\n\n\n\n\n\n\nTip\n\n\n\nCross validation is especially useful when comparing multiple models. Here’s an example with multiple models and time series."
  },
  {
    "objectID": "examples/crossvalidation.html#references",
    "href": "examples/crossvalidation.html#references",
    "title": "Cross validation",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Time series cross-validation”."
  },
  {
    "objectID": "examples/exogenous.html",
    "href": "examples/exogenous.html",
    "title": "Exogenous Regressors",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/exogenous.html#introduction",
    "href": "examples/exogenous.html#introduction",
    "title": "Exogenous Regressors",
    "section": "Introduction",
    "text": "Introduction\nExogenous regressors are variables that can affect the values of a time series. They may not be directly related to the variable that is beging forecasted, but they can still have an impact on it. Examples of exogenous regressors are weather data, economic indicators, or promotional sales. They are typically collected from external sources and by incorporating them into a forecasting model, they can improve the accuracy of our predictions.\nBy the end of this tutorial, you’ll have a good understanding of how to incorporate exogenous regressors into StatsForecast’s models. Furthermore, you’ll see how to evaluate their performance and decide whether or not they can help enhance the forecast.\nOutline\n\nInstall libraries\nLoad and explore the data\nSplit train/test set\nAdd exogenous regressors\nCreate future exogenous regressors\nTrain model\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "examples/exogenous.html#install-libraries",
    "href": "examples/exogenous.html#install-libraries",
    "title": "Exogenous Regressors",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "examples/exogenous.html#load-and-explore-the-data",
    "href": "examples/exogenous.html#load-and-explore-the-data",
    "title": "Exogenous Regressors",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nIn this example, we’ll use a single time series from the M5 Competition dataset. This series represents the daily sales of a product in a Walmart store. We’ll first import the complete dataset from datasetsforecast, which you can install using pip install datasetsforecast.\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m5 import M5\n\nThe function to load the data is M5.load, which requieres the following argument.\n\ndirectory: (str) The directory where the data will be downloaded.\n\nThis function returns multiple outputs. We need the first two.\n\nY_df: (pandas DataFrame) The target time series with columns [unique_id, ds, y].\nX_df: (pandas DataFrame) Exogenous time series with columns [unique_id, ds, exogenous regressors].\n\n\nY_df, X_df, *_ = M5.load('./data')\n\nINFO:datasetsforecast.utils:Successfully downloaded m5.zip, 50219189, bytes.\nINFO:datasetsforecast.utils:Decompressing zip file...\nINFO:datasetsforecast.utils:Successfully decompressed data/m5/datasets/m5.zip\n\n\nWe now need to filter the dataset. The product-store combination that we’ll use in this notebook has unique_id = FOODS_3_586_CA_3. This time series was chosen because it is not intermittent and has exogenous regressors that will be useful for forecasting.\n\n# Filter data \nY_ts = Y_df[Y_df['unique_id'] == 'FOODS_3_586_CA_3'].reset_index(drop = True)\nX_ts = X_df[X_df['unique_id'] == 'FOODS_3_586_CA_3'].reset_index(drop = True)\n\nY_ts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      FOODS_3_586_CA_3\n      2011-01-29\n      56.0\n    \n    \n      1\n      FOODS_3_586_CA_3\n      2011-01-30\n      55.0\n    \n    \n      2\n      FOODS_3_586_CA_3\n      2011-01-31\n      45.0\n    \n    \n      3\n      FOODS_3_586_CA_3\n      2011-02-01\n      57.0\n    \n    \n      4\n      FOODS_3_586_CA_3\n      2011-02-02\n      54.0\n    \n  \n\n\n\n\nWe can plot the sales of this product-store combination with the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the requiered ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nengine: str = plotly. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\n/Users/fedex/miniconda3/envs/statsforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\nStatsForecast.plot(Y_ts)\n\n\n                                                \n\n\nThe M5 Competition included several exogenous regressors. Here we’ll use the following two.\n\nsell_price: The price of the product for the given store. The price is provided per week.\nsnap_CA: A binary variable indicating whether the store allows SNAP purchases (1 if yes, 0 otherwise). SNAP stands for Supplement Nutrition Assitance Program, and it gives individuals and families money to help them purchase food products.\n\n\nX_ts = X_ts[['unique_id', 'ds', 'sell_price', 'snap_CA']]\nX_ts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      sell_price\n      snap_CA\n    \n  \n  \n    \n      0\n      FOODS_3_586_CA_3\n      2011-01-29\n      1.48\n      0\n    \n    \n      1\n      FOODS_3_586_CA_3\n      2011-01-30\n      1.48\n      0\n    \n    \n      2\n      FOODS_3_586_CA_3\n      2011-01-31\n      1.48\n      0\n    \n    \n      3\n      FOODS_3_586_CA_3\n      2011-02-01\n      1.48\n      1\n    \n    \n      4\n      FOODS_3_586_CA_3\n      2011-02-02\n      1.48\n      1\n    \n  \n\n\n\n\nHere the unique_id is a category, but for the exogenous regressors it needs to be a string.\n\nX_ts['unique_id'] = X_ts.unique_id.astype(str)\n\nWe can plot the exogenous regressors using plotly. We could use statsforecast.plot, but then one of the regressors must be renamed y, and the name must be changed back to the original before generating the forecast.\n\nStatsForecast.plot(X_ts)\n\n\n                                                \n\n\nFrom this plot, we can conclude that price has increased twice and that SNAP occurs at regular intervals."
  },
  {
    "objectID": "examples/exogenous.html#split-traintest-set",
    "href": "examples/exogenous.html#split-traintest-set",
    "title": "Exogenous Regressors",
    "section": "Split train/test set",
    "text": "Split train/test set\nIn the M5 Competition, participants had to forecast sales for the last 28 days in the dataset. We’ll use the same forecast horizon and create the train and test sets accordingly.\n\n# Extract dates for train and test set \ndates = Y_df['ds'].unique()\ndtrain = dates[:-28]\ndtest = dates[-28:]\n\nY_train = Y_ts.query('ds in @dtrain')\nY_test = Y_ts.query('ds in @dtest') \n\nX_train = X_ts.query('ds in @dtrain') \nX_test = X_ts.query('ds in @dtest')"
  },
  {
    "objectID": "examples/exogenous.html#add-exogenous-regressors",
    "href": "examples/exogenous.html#add-exogenous-regressors",
    "title": "Exogenous Regressors",
    "section": "Add exogenous regressors",
    "text": "Add exogenous regressors\nThe exogenous regressors need to be place after the target variable y.\n\ntrain = Y_train.merge(X_ts, how = 'left', on = ['unique_id', 'ds']) \ntrain.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      sell_price\n      snap_CA\n    \n  \n  \n    \n      0\n      FOODS_3_586_CA_3\n      2011-01-29\n      56.0\n      1.48\n      0\n    \n    \n      1\n      FOODS_3_586_CA_3\n      2011-01-30\n      55.0\n      1.48\n      0\n    \n    \n      2\n      FOODS_3_586_CA_3\n      2011-01-31\n      45.0\n      1.48\n      0\n    \n    \n      3\n      FOODS_3_586_CA_3\n      2011-02-01\n      57.0\n      1.48\n      1\n    \n    \n      4\n      FOODS_3_586_CA_3\n      2011-02-02\n      54.0\n      1.48\n      1"
  },
  {
    "objectID": "examples/exogenous.html#create-future-exogenous-regressors",
    "href": "examples/exogenous.html#create-future-exogenous-regressors",
    "title": "Exogenous Regressors",
    "section": "Create future exogenous regressors",
    "text": "Create future exogenous regressors\nWe need to include the future values of the exogenous regressors so that we can produce the forecasts. Notice that we already have this information in X_test.\n\nX_test.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      sell_price\n      snap_CA\n    \n  \n  \n    \n      1941\n      FOODS_3_586_CA_3\n      2016-05-23\n      1.68\n      0\n    \n    \n      1942\n      FOODS_3_586_CA_3\n      2016-05-24\n      1.68\n      0\n    \n    \n      1943\n      FOODS_3_586_CA_3\n      2016-05-25\n      1.68\n      0\n    \n    \n      1944\n      FOODS_3_586_CA_3\n      2016-05-26\n      1.68\n      0\n    \n    \n      1945\n      FOODS_3_586_CA_3\n      2016-05-27\n      1.68\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the future values of the exogenous regressors are not available, then they must be forecasted or the regressors need to be eliminated from the model. Without them, it is not possible to generate the forecast."
  },
  {
    "objectID": "examples/exogenous.html#train-model",
    "href": "examples/exogenous.html#train-model",
    "title": "Exogenous Regressors",
    "section": "Train model",
    "text": "Train model\nTo generate the forecast, we’ll use AutoARIMA, which is one of the models available in StatsForecast that allows exogenous regressors. To use this model, we first need to import it from statsforecast.models and then we need to instatiate it. Given that we’re working with daily data, we need to set season_length = 7.\n\nfrom statsforecast.models import AutoARIMA\n\n# Create a list with the model and its instantiation parameters \nmodels = [AutoARIMA(season_length = 7)]\n\nNext, we need to instantiate a new StatsForecast object, which has the following parameters.\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    models=models, \n    freq='D', \n    n_jobs=-1\n)\n\nNow we’re ready to generate the forecast. To do this, we’ll use the forecast method, which takes the following arguments.\n\nh: An integer that represents the forecast horizon. In this case, we’ll forecast the next 28 days.\nX_df: A pandas dataframe with the future values of the exogenous regressors.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nhorizon = 28\nlevel = [95]\n\nfcst = sf.forecast(df=train, h=horizon, X_df=X_test, level=level)\nfcst = fcst.reset_index()\nfcst.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n      AutoARIMA-lo-95\n      AutoARIMA-hi-95\n    \n  \n  \n    \n      0\n      FOODS_3_586_CA_3\n      2016-05-23\n      72.956276\n      44.109074\n      101.803490\n    \n    \n      1\n      FOODS_3_586_CA_3\n      2016-05-24\n      71.138611\n      40.761467\n      101.515747\n    \n    \n      2\n      FOODS_3_586_CA_3\n      2016-05-25\n      68.140945\n      37.550083\n      98.731804\n    \n    \n      3\n      FOODS_3_586_CA_3\n      2016-05-26\n      65.485588\n      34.841637\n      96.129539\n    \n    \n      4\n      FOODS_3_586_CA_3\n      2016-05-27\n      64.961441\n      34.291969\n      95.630905\n    \n  \n\n\n\n\nWe can plot the forecasts with the statsforecast.plot method described above.\n\nStatsForecast.plot(Y_ts, fcst, max_insample_length=28*2)"
  },
  {
    "objectID": "examples/exogenous.html#evaluate-results",
    "href": "examples/exogenous.html#evaluate-results",
    "title": "Exogenous Regressors",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe’ll merge the test set and the forecast to evaluate the accuracy using the mean absolute error (MAE).\n\nres = Y_test.merge(fcst, how='left', on=['unique_id', 'ds'])\nres.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      AutoARIMA\n      AutoARIMA-lo-95\n      AutoARIMA-hi-95\n    \n  \n  \n    \n      0\n      FOODS_3_586_CA_3\n      2016-05-23\n      66.0\n      72.956276\n      44.109074\n      101.803490\n    \n    \n      1\n      FOODS_3_586_CA_3\n      2016-05-24\n      62.0\n      71.138611\n      40.761467\n      101.515747\n    \n    \n      2\n      FOODS_3_586_CA_3\n      2016-05-25\n      40.0\n      68.140945\n      37.550083\n      98.731804\n    \n    \n      3\n      FOODS_3_586_CA_3\n      2016-05-26\n      72.0\n      65.485588\n      34.841637\n      96.129539\n    \n    \n      4\n      FOODS_3_586_CA_3\n      2016-05-27\n      69.0\n      64.961441\n      34.291969\n      95.630905\n    \n  \n\n\n\n\n\nmae = abs(res['y']-res['AutoARIMA']).mean()\nprint('The MAE with exogenous regressors is '+str(round(mae,2)))\n\nThe MAE with exogenous regressors is 11.42\n\n\nTo check whether the exogenous regressors were useful or not, we need to generate the forecast again, now without them. To do this, we simple pass the dataframe wihtout exogenous variables to the forecast method. Notice that the data only includes unique_id, ds, and y. The forecast method no longer requieres the future values of the exogenous regressors X_df.\n\n# univariate model \nfcst_u = sf.forecast(df=train[['unique_id', 'ds', 'y']], h=28)\n\nres_u = Y_test.merge(fcst_u, how='left', on=['unique_id', 'ds'])\nmae_u = abs(res_u['y']-res_u['AutoARIMA']).mean()\n\n\nprint('The MAE without exogenous regressors is '+str(round(mae_u,2)))\n\nThe MAE without exogenous regressors is 12.18\n\n\nHence, we can conclude that using sell_price and snap_CA as external regressors helped improve the forecast."
  },
  {
    "objectID": "examples/getting_started_short.html",
    "href": "examples/getting_started_short.html",
    "title": "Quick Start",
    "section": "",
    "text": "StatsForecast follows the sklearn model API. For this minimal example, you will create an instance of the StatsForecast class and then call its fit and predict methods. We recommend this option if speed is not paramount and you want to explore the fitted values and parameters.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to forecast many series, we recommend using the forecast method. Check this Getting Started with multiple time series guide.\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\nAs an example, let’s look at the US Air Passengers dataset. This time series consists of monthly totals of a US airline passengers from 1949 to 1960. The CSV is available here.\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nFirst, we’ll import the data:\n\n! pip install StatsForecast\n\nUsageError: unrecognized arguments: hide output\n\n\n\nimport pandas as pd\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/air-passengers.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      AirPassengers\n      1949-01-01\n      112\n    \n    \n      1\n      AirPassengers\n      1949-02-01\n      118\n    \n    \n      2\n      AirPassengers\n      1949-03-01\n      132\n    \n    \n      3\n      AirPassengers\n      1949-04-01\n      129\n    \n    \n      4\n      AirPassengers\n      1949-05-01\n      121\n    \n  \n\n\n\n\nWe fit the model by instantiating a new StatsForecast object with its two required parameters:\n\nmodels: a list of models. Select the models you want from models and import them. For this example, we will use a AutoARIMA model. We set season_length to 12 because we expect seasonal effects every 12 months. (See: Seasonal periods)\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\n\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\nforecast_df = sf.predict(h=12, level=[90]) \n\nforecast_df.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      AirPassengers\n      1961-07-31\n      633.230774\n      589.562378\n      676.899170\n    \n    \n      AirPassengers\n      1961-08-31\n      535.230774\n      489.082153\n      581.379456\n    \n    \n      AirPassengers\n      1961-09-30\n      488.230804\n      439.728699\n      536.732910\n    \n    \n      AirPassengers\n      1961-10-31\n      417.230804\n      366.484253\n      467.977356\n    \n    \n      AirPassengers\n      1961-11-30\n      459.230804\n      406.334930\n      512.126648\n    \n  \n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\ndf[\"ds\"]=pd.to_datetime(df[\"ds\"])\nsf.plot(df, forecast_df, level=[90])\n\n\n                                                \n\n\n\n\n\n\n\n\nNext Steps\n\n\n\n\nBuild and end to end forecasting pipeline following best practices in End to End Walkthrough\nForecast millions of series in a scalable cluster in the cloud using Spark and Nixtla\nDetect anomalies in your past observations\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/multipleseasonalities.html",
    "href": "examples/multipleseasonalities.html",
    "title": "Multiple seasonalities",
    "section": "",
    "text": "Tip\n\n\n\nFor this task, StatsForecast’s MSTL is 68% more accurate and 600% faster than Prophet and NeuralProphet. (Reproduce experiments here)\nMultiple seasonal data refers to time series that have more than one clear seasonality. Multiple seasonality is traditionally present in data that is sampled at a low frequency. For example, hourly electricity data exhibits daily and weekly seasonality. That means that there are clear patterns of electricity consumption for specific hours of the day like 6:00pm vs 3:00am or for specific days like Sunday vs Friday.\nTraditional statistical models are not able to model more than one seasonal length. In this example, we will show how to model the two seasonalities efficiently using Multiple Seasonal-Trend decompositions with LOESS (MSTL).\nFor this example, we will use hourly electricity load data from Pennsylvania, New Jersey, and Maryland (PJM). The original data can be found here. (Click here for info on PJM)\nFirst, we will load the data, then we will use the StatsForecast.fit and StatsForecast.predict methods to predict the next 24 hours. We will then decompose the different elements of the time series into trends and its multiple seasonalities. At the end, you will use the StatsForecast.forecast for production-ready forecasting.\nOutline\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/multipleseasonalities.html#install-libraries",
    "href": "examples/multipleseasonalities.html#install-libraries",
    "title": "Multiple seasonalities",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast ``"
  },
  {
    "objectID": "examples/multipleseasonalities.html#load-data",
    "href": "examples/multipleseasonalities.html#load-data",
    "title": "Multiple seasonalities",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nYou will read the data with pandas and change the necessary names. This step should take around 2s.\n\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/jnagura/Energy-consumption-prediction-analysis/master/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf.tail()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      32891\n      PJM_Load_hourly\n      2001-01-01 20:00:00\n      35209.0\n    \n    \n      32892\n      PJM_Load_hourly\n      2001-01-01 21:00:00\n      34791.0\n    \n    \n      32893\n      PJM_Load_hourly\n      2001-01-01 22:00:00\n      33669.0\n    \n    \n      32894\n      PJM_Load_hourly\n      2001-01-01 23:00:00\n      31809.0\n    \n    \n      32895\n      PJM_Load_hourly\n      2001-01-02 00:00:00\n      29506.0\n    \n  \n\n\n\n\nStatsForecast can handle unsorted data, however, for plotting purposes, it is convenient to sort the data frame.\n\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA. In this case, it will print just one series given that we have just one unique_id.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nThe time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods."
  },
  {
    "objectID": "examples/multipleseasonalities.html#fit-an-mstl-model",
    "href": "examples/multipleseasonalities.html#fit-an-mstl-model",
    "title": "Multiple seasonalities",
    "section": "Fit an MSTL model",
    "text": "Fit an MSTL model\nThe MSTL (Multiple Seasonal-Trend decompositions using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a non-seasonal model and each seasonality using a SeasonalNaive model. You can choose the non-seasonal model you want to use to forecast the trend component of the MSTL model. In this example, we will use an AutoARIMA.\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] for season length. The trend component will be forecasted with an AutoARIMA model. (You can also try with: AutoTheta, AutoCES, and AutoETS)\n\n# Create a list of models and instantiation parameters\n\nmodels = [MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)]\n\nWe fit the models by instantiating a new StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(\n    models=models, # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nUse the fit method to fit each model to each time series. In this case, we are just fitting one model to one series. Check this guide to learn how to fit many models to many series.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nsf = sf.fit(df=df)"
  },
  {
    "objectID": "examples/multipleseasonalities.html#decompose-the-series",
    "href": "examples/multipleseasonalities.html#decompose-the-series",
    "title": "Multiple seasonalities",
    "section": "Decompose the series",
    "text": "Decompose the series\nOnce the model is fitted, access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case, we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n\n\n  \n    \n      \n      data\n      trend\n      seasonal24\n      seasonal168\n      remainder\n    \n  \n  \n    \n      0\n      22259.0\n      26183.898892\n      -5215.124554\n      609.000432\n      681.225229\n    \n    \n      1\n      21244.0\n      26181.599305\n      -6255.673234\n      603.823918\n      714.250011\n    \n    \n      2\n      20651.0\n      26179.294886\n      -6905.329895\n      636.820423\n      740.214587\n    \n    \n      3\n      20421.0\n      26176.985472\n      -7073.420118\n      615.825999\n      701.608647\n    \n    \n      4\n      20713.0\n      26174.670877\n      -7062.395760\n      991.521912\n      609.202971\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32891\n      36392.0\n      33123.552727\n      4387.149171\n      -488.177882\n      -630.524015\n    \n    \n      32892\n      35082.0\n      33148.242575\n      3479.852929\n      -682.928737\n      -863.166767\n    \n    \n      32893\n      33890.0\n      33172.926165\n      2307.808829\n      -650.566775\n      -940.168219\n    \n    \n      32894\n      32590.0\n      33197.603322\n      748.587723\n      -555.177849\n      -801.013195\n    \n    \n      32895\n      31569.0\n      33222.273902\n      -967.124123\n      -265.895357\n      -420.254422\n    \n  \n\n32896 rows × 5 columns\n\n\n\nWe will use matplotlib, to visualize the different components of the series.\n\nimport matplotlib.pyplot as plt\n\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe a clear upward trend (orange line) and seasonality repeating every day (24H) and every week (168H)."
  },
  {
    "objectID": "examples/multipleseasonalities.html#predict-the-next-24-hours",
    "href": "examples/multipleseasonalities.html#predict-the-next-24-hours",
    "title": "Multiple seasonalities",
    "section": "Predict the next 24 hours",
    "text": "Predict the next 24 hours\n\nProbabilistic forecasting with levels\n\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2002-01-01 01:00:00\n      29956.744141\n      29585.187500\n      30328.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 02:00:00\n      29057.691406\n      28407.498047\n      29707.884766\n    \n    \n      PJM_Load_hourly\n      2002-01-01 03:00:00\n      28654.699219\n      27767.101562\n      29542.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 04:00:00\n      28499.009766\n      27407.640625\n      29590.378906\n    \n    \n      PJM_Load_hourly\n      2002-01-01 05:00:00\n      28821.716797\n      27552.236328\n      30091.197266\n    \n  \n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\nsf.plot(df, forecasts, max_insample_length=24 * 7)"
  },
  {
    "objectID": "examples/multipleseasonalities.html#forecast-in-production",
    "href": "examples/multipleseasonalities.html#forecast-in-production",
    "title": "Multiple seasonalities",
    "section": "Forecast in production",
    "text": "Forecast in production\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nforecasts_df = sf.forecast(h=24, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2002-01-01 01:00:00\n      29956.744141\n      29585.187500\n      30328.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 02:00:00\n      29057.691406\n      28407.498047\n      29707.884766\n    \n    \n      PJM_Load_hourly\n      2002-01-01 03:00:00\n      28654.699219\n      27767.101562\n      29542.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 04:00:00\n      28499.009766\n      27407.640625\n      29590.378906\n    \n    \n      PJM_Load_hourly\n      2002-01-01 05:00:00\n      28821.716797\n      27552.236328\n      30091.197266"
  },
  {
    "objectID": "examples/multipleseasonalities.html#references",
    "href": "examples/multipleseasonalities.html#references",
    "title": "Multiple seasonalities",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”."
  },
  {
    "objectID": "examples/multipleseasonalities.html#next-steps",
    "href": "examples/multipleseasonalities.html#next-steps",
    "title": "Multiple seasonalities",
    "section": "Next Steps",
    "text": "Next Steps\n\nLearn how to use cross-validation to assess the robustness of your model."
  },
  {
    "objectID": "examples/electricityloadforecasting.html",
    "href": "examples/electricityloadforecasting.html",
    "title": "Electricity load forecast",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "examples/electricityloadforecasting.html#introduction",
    "href": "examples/electricityloadforecasting.html#introduction",
    "title": "Electricity load forecast",
    "section": "Introduction",
    "text": "Introduction\nSome time series are generated from very low frequency data. These data generally exhibit multiple seasonalities. For example, hourly data may exhibit repeated patterns every hour (every 24 observations) or every day (every 24 * 7, hours per day, observations). This is the case for electricity load. Electricity load may vary hourly, e.g., during the evenings electricity consumption may be expected to increase. But also, the electricity load varies by week. Perhaps on weekends there is an increase in electrical activity.\nIn this example we will show how to model the two seasonalities of the time series to generate accurate forecasts in a short time. We will use hourly PJM electricity load data. The original data can be found here."
  },
  {
    "objectID": "examples/electricityloadforecasting.html#libraries",
    "href": "examples/electricityloadforecasting.html#libraries",
    "title": "Electricity load forecast",
    "section": "Libraries",
    "text": "Libraries\nIn this example we will use the following libraries: - [StatsForecast](https://Nixtla.github.io/statsforecast/core.html#statsforecast). Lightning ⚡️ fast forecasting with statistical and econometric models. Includes the MSTL model for multiple seasonalities. - DatasetsForecast. Used to evaluate the performance of the forecasts. - Prophet. Benchmark model developed by Facebook. - NeuralProphet. Deep Learning version of Prophet. Used as benchark.\n\n!pip install statsforecast\n!pip install datasetsforecast\n!pip install prophet\n!pip install \"neuralprophet[live]\""
  },
  {
    "objectID": "examples/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "href": "examples/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "title": "Electricity load forecast",
    "section": "Forecast using Multiple Seasonalities",
    "text": "Forecast using Multiple Seasonalities\n\nElectricity Load Data\nAccording to the dataset’s page,\n\nPJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia. The hourly power consumption data comes from PJM’s website and are in megawatts (MW).\n\nLet’s take a look to the data.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\npd.plotting.register_matplotlib_converters()\nplt.rc(\"figure\", figsize=(10, 8))\nplt.rc(\"font\", size=10)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/jnagura/Energy-consumption-prediction-analysis/master/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\ndf.tail()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      32891\n      PJM_Load_hourly\n      2001-12-31 20:00:00\n      36392.0\n    \n    \n      32892\n      PJM_Load_hourly\n      2001-12-31 21:00:00\n      35082.0\n    \n    \n      32893\n      PJM_Load_hourly\n      2001-12-31 22:00:00\n      33890.0\n    \n    \n      32894\n      PJM_Load_hourly\n      2001-12-31 23:00:00\n      32590.0\n    \n    \n      32895\n      PJM_Load_hourly\n      2002-01-01 00:00:00\n      31569.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.plot(x='ds', y='y')\n\n<matplotlib.axes._subplots.AxesSubplot>\n\n\n\n\n\nWe clearly observe that the time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods to display them in production.\n\n\nMSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\nStatsForecast contains a fast implementation of the MSTL model. Also, the decomposition of the time series can be calculated.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA, SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\n/usr/local/lib/python3.7/dist-packages/statsforecast/core.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\nFirst we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities that the MSTL model receives. We must also specify the manner in which the trend will be forecasted. In this case we will use the AutoARIMA model.\n\nmstl = MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)\n\nOnce the model is instantiated, we have to instantiate the StatsForecast class to create forecasts.\n\nsf = StatsForecast(\n    models=[mstl], # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\nFit the model\nAfer that, we just have to use the fit method to fit each model to each time series.\n\nsf = sf.fit(df=df)\n\n\n\nDecompose the time series in multiple seasonalities\nOnce the model is fitted, we can access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      data\n      trend\n      seasonal24\n      seasonal168\n      remainder\n    \n  \n  \n    \n      0\n      22259.0\n      26183.898892\n      -5215.124554\n      609.000432\n      681.225229\n    \n    \n      1\n      21244.0\n      26181.599305\n      -6255.673234\n      603.823918\n      714.250011\n    \n    \n      2\n      20651.0\n      26179.294886\n      -6905.329895\n      636.820423\n      740.214587\n    \n    \n      3\n      20421.0\n      26176.985472\n      -7073.420118\n      615.825999\n      701.608647\n    \n    \n      4\n      20713.0\n      26174.670877\n      -7062.395760\n      991.521912\n      609.202971\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32891\n      36392.0\n      33123.552727\n      4387.149171\n      -488.177882\n      -630.524015\n    \n    \n      32892\n      35082.0\n      33148.242575\n      3479.852929\n      -682.928737\n      -863.166767\n    \n    \n      32893\n      33890.0\n      33172.926165\n      2307.808829\n      -650.566775\n      -940.168219\n    \n    \n      32894\n      32590.0\n      33197.603322\n      748.587723\n      -555.177849\n      -801.013195\n    \n    \n      32895\n      31569.0\n      33222.273902\n      -967.124123\n      -265.895357\n      -420.254422\n    \n  \n\n32896 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet’s look graphically at the different components of the time series.\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe that there is a clear trend towards the high (orange line). This component would be predicted with the AutoARIMA model. We can also observe that every 24 hours and every 24 * 7 hours there is a very well defined pattern. These two components will be forecast separately using a SeasonalNaive model.\n\n\nProduce forecasts\nTo generate forecasts we only have to use the predict method specifying the forecast horizon (h). In addition, to calculate prediction intervals associated to the forecasts, we can include the parameter level that receives a list of levels of the prediction intervals we want to build. In this case we will only calculate the 90% forecast interval (level=[90]).\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2002-01-01 01:00:00\n      29956.744141\n      29585.187500\n      30328.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 02:00:00\n      29057.691406\n      28407.498047\n      29707.884766\n    \n    \n      PJM_Load_hourly\n      2002-01-01 03:00:00\n      28654.699219\n      27767.101562\n      29542.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 04:00:00\n      28499.009766\n      27407.640625\n      29590.378906\n    \n    \n      PJM_Load_hourly\n      2002-01-01 05:00:00\n      28821.716797\n      27552.236328\n      30091.197266\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet’s look at our forecasts graphically.\n\n_, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = pd.concat([df, forecasts]).set_index('ds').tail(24 * 7)\ndf_plot[['y', 'MSTL']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['MSTL-lo-90'], \n                df_plot['MSTL-hi-90'],\n                alpha=.35,\n                color='orange',\n                label='MSTL-level-90')\nax.set_title('PJM Load Hourly', fontsize=22)\nax.set_ylabel('Electricity Load', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\nIn the next section we will plot different models so it is convenient to reuse the previous code with the following function.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(24 * 7)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2)\n    colors = ['orange', 'green', 'red']\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-90'], \n                        df_plot[f'{model}-hi-90'],\n                        alpha=.35,\n                        color=color,\n                        label=f'{model}-level-90')\n    ax.set_title('PJM Load Hourly', fontsize=22)\n    ax.set_ylabel('Electricity Load', fontsize=20)\n    ax.set_xlabel('Timestamp [t]', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid()\n\n\n\n\nPerformance of the MSTL model\n\nSplit Train/Test sets\nTo validate the accuracy of the MSTL model, we will show its performance on unseen data. We will use a classical time series technique that consists of dividing the data into a training set and a test set. We will leave the last 24 observations (the last day) as the test set. So the model will train on 32,872 observations.\n\ndf_test = df.tail(24)\ndf_train = df.drop(df_test.index)\n\n\n\nMSTL model\nIn addition to the MSTL model, we will include the SeasonalNaive model as a benchmark to validate the added value of the MSTL model. Including StatsForecast models is as simple as adding them to the list of models to be fitted.\n\nsf = StatsForecast(\n    models=[mstl, SeasonalNaive(season_length=24)], # add SeasonalNaive model to the list\n    freq='H'\n)\n\nTo measure the fitting time we will use the time module.\n\nfrom time import time\n\nTo retrieve the forecasts of the test set we only have to do fit and predict as before.\n\ninit = time()\nsf = sf.fit(df=df_train)\nforecasts_test = sf.predict(h=len(df_test), level=[90])\nend = time()\nforecasts_test.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n      SeasonalNaive\n      SeasonalNaive-lo-90\n      SeasonalNaive-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2001-12-31 01:00:00\n      28345.212891\n      27973.570312\n      28716.853516\n      28326.0\n      23468.693359\n      33183.308594\n    \n    \n      PJM_Load_hourly\n      2001-12-31 02:00:00\n      27567.455078\n      26917.085938\n      28217.824219\n      27362.0\n      22504.693359\n      32219.306641\n    \n    \n      PJM_Load_hourly\n      2001-12-31 03:00:00\n      27260.001953\n      26372.138672\n      28147.865234\n      27108.0\n      22250.693359\n      31965.306641\n    \n    \n      PJM_Load_hourly\n      2001-12-31 04:00:00\n      27328.125000\n      26236.410156\n      28419.839844\n      26865.0\n      22007.693359\n      31722.306641\n    \n    \n      PJM_Load_hourly\n      2001-12-31 05:00:00\n      27640.673828\n      26370.773438\n      28910.572266\n      26808.0\n      21950.693359\n      31665.306641\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntime_mstl = (end - init) / 60\nprint(f'MSTL Time: {time_mstl:.2f} minutes')\n\nMSTL Time: 0.43 minutes\n\n\nThen we were able to generate forecasts for the next 24 hours. Now let’s look at the graphical comparison of the forecasts with the actual values.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\nLet’s look at those produced only by MSTL.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL'])\n\n\n\n\nWe note that MSTL produces very accurate forecasts that follow the behavior of the time series. Now let us calculate numerically the accuracy of the model. We will use the following metrics: MAE, MAPE, MASE, RMSE, SMAPE.\n\nfrom datasetsforecast.losses import (\n    mae, mape, mase, rmse, smape\n)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, models):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    for model in models:\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                 y_true[model].values, \n                                                 y_hist['y'].values, seasonality=24)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mase\n      mae\n      mape\n      rmse\n      smape\n    \n  \n  \n    \n      MSTL\n      0.341926\n      709.932048\n      2.182804\n      892.888012\n      2.162832\n    \n    \n      SeasonalNaive\n      0.894653\n      1857.541667\n      5.648190\n      2201.384101\n      5.868604\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe observe that MSTL has an improvement of about 60% over the SeasonalNaive method in the test set measured in MASE.\n\n\nComparison with Prophet\nOne of the most widely used models for time series forecasting is Prophet. This model is known for its ability to model different seasonalities (weekly, daily yearly). We will use this model as a benchmark to see if the MSTL adds value for this time series.\n\nfrom prophet import Prophet\n\n# create prophet model\nprophet = Prophet(interval_width=0.9)\ninit = time()\nprophet.fit(df_train)\n# produce forecasts\nfuture = prophet.make_future_dataframe(periods=len(df_test), freq='H', include_history=False)\nforecast_prophet = prophet.predict(future)\nend = time()\n# data wrangling\nforecast_prophet = forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\nforecast_prophet.columns = ['ds', 'Prophet', 'Prophet-lo-90', 'Prophet-hi-90']\nforecast_prophet.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_prophet.head()\n\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpc6f7_v9l/08tirxvt.json\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpc6f7_v9l/gp60b_b1.json\nDEBUG:cmdstanpy:idx 0\nDEBUG:cmdstanpy:running CmdStan, num_threads: None\nDEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.7/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=27632', 'data', 'file=/tmp/tmpc6f7_v9l/08tirxvt.json', 'init=/tmp/tmpc6f7_v9l/gp60b_b1.json', 'output', 'file=/tmp/tmpc6f7_v9l/prophet_modelkou9navr/prophet_model-20221111002506.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n00:25:06 - cmdstanpy - INFO - Chain [1] start processing\nINFO:cmdstanpy:Chain [1] start processing\n00:25:58 - cmdstanpy - INFO - Chain [1] done processing\nINFO:cmdstanpy:Chain [1] done processing\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      unique_id\n      ds\n      Prophet\n      Prophet-lo-90\n      Prophet-hi-90\n    \n  \n  \n    \n      0\n      PJM_Load_hourly\n      2001-12-31 01:00:00\n      25255.037078\n      20589.007557\n      30472.856691\n    \n    \n      1\n      PJM_Load_hourly\n      2001-12-31 02:00:00\n      23961.427979\n      19078.763798\n      28752.007690\n    \n    \n      2\n      PJM_Load_hourly\n      2001-12-31 03:00:00\n      23285.418846\n      18491.391041\n      28213.042892\n    \n    \n      3\n      PJM_Load_hourly\n      2001-12-31 04:00:00\n      23293.143264\n      18620.414846\n      28364.115196\n    \n    \n      4\n      PJM_Load_hourly\n      2001-12-31 05:00:00\n      24067.737572\n      19062.318724\n      29162.041606\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntime_prophet = (end - init) / 60\nprint(f'Prophet Time: {time_prophet:.2f} minutes')\n\nProphet Time: 0.93 minutes\n\n\n\ntimes = pd.DataFrame({'model': ['MSTL', 'Prophet'], 'time (mins)': [time_mstl, time_prophet]})\ntimes\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      model\n      time (mins)\n    \n  \n  \n    \n      0\n      MSTL\n      0.425080\n    \n    \n      1\n      Prophet\n      0.928628\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe observe that the time required for Prophet to perform the fit and predict pipeline is greater than MSTL. Let’s look at the forecasts produced by Prophet.\n\nforecasts_test = forecasts_test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive', 'Prophet'])\n\n\n\n\nWe note that Prophet is able to capture the overall behavior of the time series. However, in some cases it produces forecasts well below the actual value. It also does not correctly adjust the valleys.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'Prophet', 'SeasonalNaive'])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mase\n      mae\n      mape\n      rmse\n      smape\n    \n  \n  \n    \n      MSTL\n      0.341926\n      709.932048\n      2.182804\n      892.888012\n      2.162832\n    \n    \n      Prophet\n      1.107472\n      2299.413375\n      7.427523\n      2742.792022\n      7.789355\n    \n    \n      SeasonalNaive\n      0.894653\n      1857.541667\n      5.648190\n      2201.384101\n      5.868604\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIn terms of accuracy, Prophet is not able to produce better forecasts than the SeasonalNaive model, however, the MSTL model improves Prophet’s forecasts by 69% (MASE).\n\n\nComparison with NeuralProphet\nNeuralProphet is the version of Prophet using deep learning. This model is also capable of handling different seasonalities so we will also use it as a benchmark.\n\nfrom neuralprophet import NeuralProphet\n\nneuralprophet = NeuralProphet(quantiles=[0.05, 0.95])\ninit = time()\nneuralprophet.fit(df_train.drop(columns='unique_id'))\nfuture = neuralprophet.make_future_dataframe(df=df_train.drop(columns='unique_id'), periods=len(df_test))\nforecast_np = neuralprophet.predict(future)\nend = time()\nforecast_np = forecast_np[['ds', 'yhat1', 'yhat1 5.0%', 'yhat1 95.0%']]\nforecast_np.columns = ['ds', 'NeuralProphet', 'NeuralProphet-lo-90', 'NeuralProphet-hi-90']\nforecast_np.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_np.head()\n\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as H\nINFO:NP.df_utils:Dataframe freq automatically defined as H\nINFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\nINFO:NP.config:Setting normalization to global as only one dataframe provided for training.\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\nINFO:NP.config:Auto-set batch_size to 64\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 76\nINFO:NP.config:Auto-set epochs to 76\n\n\n\n\n\nWARNING - (py.warnings._showwarnmsg) - /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nWARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nWARNING - (py.warnings._showwarnmsg) - /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([324, 1, 1])) that is different to the input size (torch.Size([324, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nWARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([324, 1, 1])) that is different to the input size (torch.Size([324, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nINFO - (NP.utils_torch.lr_range_test) - lr-range-test results: steep: 3.89E-02, min: 2.68E-01\nINFO:NP.utils_torch:lr-range-test results: steep: 3.89E-02, min: 2.68E-01\n\n\n\n\n\nINFO - (NP.utils_torch.lr_range_test) - lr-range-test results: steep: 5.03E-02, min: 1.82E-01\nINFO:NP.utils_torch:lr-range-test results: steep: 5.03E-02, min: 1.82E-01\nINFO - (NP.forecaster._init_train_loader) - lr-range-test selected learning rate: 5.81E-02\nINFO:NP.forecaster:lr-range-test selected learning rate: 5.81E-02\nEpoch[76/76]: 100%|██████████| 76/76 [02:17<00:00,  1.81s/it, SmoothL1Loss=0.00872, MAE=2.27e+3, RMSE=3.03e+3, Loss=0.00862, RegLoss=0]\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO:NP.df_utils:Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\nINFO:NP.df_utils:Returning df with no ID column\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO:NP.df_utils:Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO:NP.df_utils:Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\nINFO:NP.df_utils:Returning df with no ID column\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      unique_id\n      ds\n      NeuralProphet\n      NeuralProphet-lo-90\n      NeuralProphet-hi-90\n    \n  \n  \n    \n      0\n      PJM_Load_hourly\n      2001-12-31 01:00:00\n      25015.931641\n      22304.785156\n      27442.351562\n    \n    \n      1\n      PJM_Load_hourly\n      2001-12-31 02:00:00\n      24125.677734\n      21462.275391\n      26571.785156\n    \n    \n      2\n      PJM_Load_hourly\n      2001-12-31 03:00:00\n      23730.714844\n      20973.000000\n      26321.234375\n    \n    \n      3\n      PJM_Load_hourly\n      2001-12-31 04:00:00\n      23469.591797\n      20735.953125\n      26097.132812\n    \n    \n      4\n      PJM_Load_hourly\n      2001-12-31 05:00:00\n      23887.208984\n      21221.152344\n      26459.939453\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntime_np = (end - init) / 60\nprint(f'Prophet Time: {time_np:.2f} minutes')\n\nProphet Time: 2.39 minutes\n\n\n\ntimes = times.append({'model': 'NeuralProphet', 'time (mins)': time_np}, ignore_index=True)\ntimes\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      model\n      time (mins)\n    \n  \n  \n    \n      0\n      MSTL\n      0.425080\n    \n    \n      1\n      Prophet\n      0.928628\n    \n    \n      2\n      NeuralProphet\n      2.393841\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe observe that NeuralProphet requires a longer processing time than Prophet and MSTL.\n\nforecasts_test = forecasts_test.merge(forecast_np, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet'])\n\n\n\n\nThe forecasts graph shows that NeuralProphet generates very similar results to Prophet, as expected.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet', 'SeasonalNaive'])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mase\n      mae\n      mape\n      rmse\n      smape\n    \n  \n  \n    \n      MSTL\n      0.341926\n      709.932048\n      2.182804\n      892.888012\n      2.162832\n    \n    \n      NeuralProphet\n      1.089964\n      2263.060303\n      7.310210\n      2681.742759\n      7.651353\n    \n    \n      Prophet\n      1.107472\n      2299.413375\n      7.427523\n      2742.792022\n      7.789355\n    \n    \n      SeasonalNaive\n      0.894653\n      1857.541667\n      5.648190\n      2201.384101\n      5.868604\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWith respect to numerical evaluation, NeuralProphet improves the results of Prohet, as expected, however, MSTL improves over NeuralProphet’s foreacasts by 68% (MASE).\n\n\n\n\n\n\nImportant\n\n\n\nThe performance of NeuralProphet can be improved using hyperparameter optimization, which can increase the fitting time significantly. In this example we show its performance with the default version."
  },
  {
    "objectID": "examples/electricityloadforecasting.html#conclusion",
    "href": "examples/electricityloadforecasting.html#conclusion",
    "title": "Electricity load forecast",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we introduced MSTL, a model originally developed by Kasun Bandara, Rob Hyndman and Christoph Bergmeir capable of handling time series with multiple seasonalities. We also showed that for the PJM electricity load time series offers better performance in time and accuracy than the Prophet and NeuralProphet models."
  },
  {
    "objectID": "examples/electricityloadforecasting.html#references",
    "href": "examples/electricityloadforecasting.html#references",
    "title": "Electricity load forecast",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”."
  },
  {
    "objectID": "examples/getting_started_complete.html",
    "href": "examples/getting_started_complete.html",
    "title": "End to End Walkthrough",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "examples/getting_started_complete.html#install-libraries",
    "href": "examples/getting_started_complete.html#install-libraries",
    "title": "End to End Walkthrough",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS and datasetsforecast for common error metrics like MAE or MASE.\nInstall the necessary packages using pip install statsforecast s3fs datasetsforecast ``"
  },
  {
    "objectID": "examples/getting_started_complete.html#read-the-data",
    "href": "examples/getting_started_complete.html#read-the-data",
    "title": "End to End Walkthrough",
    "section": "Read the data",
    "text": "Read the data\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nThis data set already satisfies the requirement.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport pandas as pd\n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n    \n    \n      1\n      H1\n      2\n      586.0\n    \n    \n      2\n      H1\n      3\n      586.0\n    \n    \n      3\n      H1\n      4\n      559.0\n    \n    \n      4\n      H1\n      5\n      511.0\n    \n  \n\n\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility’s sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\n\nY_df = Y_df.query('unique_id in @uids') \n\nY_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster"
  },
  {
    "objectID": "examples/getting_started_complete.html#explore-data-with-the-plot-method",
    "href": "examples/getting_started_complete.html#explore-data-with-the-plot-method",
    "title": "End to End Walkthrough",
    "section": "Explore Data with the plot method",
    "text": "Explore Data with the plot method\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "examples/getting_started_complete.html#train-multiple-models-for-many-series",
    "href": "examples/getting_started_complete.html#train-multiple-models-for-many-series",
    "title": "End to End Walkthrough",
    "section": "Train multiple models for many series",
    "text": "Train multiple models for many series\nStatsForecast can train many models on many time series efficiently.\nStart by importing and instantiating the desired models. StatsForecast offers a wide variety of models grouped in the following categories:\n\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt’s Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nHere you can check the complete list of models.\nFor this example we will use:\n\nAutoARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Ref: AutoARIMA.\nHoltWinters: triple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality. Ref: HoltWinters\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive\nHistoricAverage: arthimetic mean. Ref: HistoricAverage.\nDynamicOptimizedTheta: The theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series. Ref: DynamicOptimizedTheta.\n\nImport and instantiate the models. Setting the season_length argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful.\n\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive\n)\n\n\n# Create a list of models and instantiation parameters\nmodels = [\n    AutoARIMA(season_length=24),\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=24),\n    HistoricAverage(),\n    DOT(season_length=24)\n]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n    n_jobs=-1,\n    fallback_model = SeasonalNaive(season_length=7)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nThe forecast method is compatible with distributed clusters, so it does not store any model parameters. If you want to store parameters for every model you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nforecasts_df = sf.forecast(h=48, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-hi-90\n      HoltWinters\n      CrostonClassic\n      SeasonalNaive\n      SeasonalNaive-lo-90\n      SeasonalNaive-hi-90\n      HistoricAverage\n      HistoricAverage-lo-90\n      HistoricAverage-hi-90\n      DynamicOptimizedTheta\n      DynamicOptimizedTheta-lo-90\n      DynamicOptimizedTheta-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      749\n      592.461792\n      572.325623\n      612.597961\n      829.0\n      708.21405\n      635.0\n      537.471191\n      732.528809\n      660.982117\n      398.03772\n      923.926514\n      592.701843\n      577.677307\n      611.652649\n    \n    \n      H1\n      750\n      527.174316\n      495.321777\n      559.026855\n      807.0\n      708.21405\n      572.0\n      474.471222\n      669.528809\n      660.982117\n      398.03772\n      923.926514\n      525.589111\n      505.449738\n      546.621826\n    \n    \n      H1\n      751\n      488.418549\n      445.535583\n      531.301514\n      785.0\n      708.21405\n      532.0\n      434.471222\n      629.528809\n      660.982117\n      398.03772\n      923.926514\n      489.251801\n      462.072876\n      512.424133\n    \n    \n      H1\n      752\n      452.284454\n      400.677155\n      503.891785\n      756.0\n      708.21405\n      493.0\n      395.471222\n      590.528809\n      660.982117\n      398.03772\n      923.926514\n      456.195038\n      430.554291\n      478.260956\n    \n    \n      H1\n      753\n      433.127563\n      374.070984\n      492.184143\n      719.0\n      708.21405\n      477.0\n      379.471222\n      574.528809\n      660.982117\n      398.03772\n      923.926514\n      436.290527\n      411.051239\n      461.815948\n    \n  \n\n\n\n\nPlot the results of 8 randon series using the StatsForecast.plot method.\n\nsf.plot(Y_df,forecasts_df)\n\n\n                                                \n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nsf.plot(Y_df, forecasts_df, models=[\"HoltWinters\",\"DynamicOptimizedTheta\"], unique_ids=[\"H10\", \"H105\"], level=[90])\n\n\n                                                \n\n\n\n# Explore other models \nsf.plot(Y_df, forecasts_df, models=[\"AutoARIMA\"], unique_ids=[\"H10\", \"H105\"], level=[90])"
  },
  {
    "objectID": "examples/getting_started_complete.html#evaluate-the-models-performance",
    "href": "examples/getting_started_complete.html#evaluate-the-models-performance",
    "title": "End to End Walkthrough",
    "section": "Evaluate the model’s performance",
    "text": "Evaluate the model’s performance\nIn previous steps, we’ve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 2 days (n_windows=2), forecasting every second day (step_size=48). Depending on your computer, this step should take around 1 min.\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvaldation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=2\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvaldation_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      AutoARIMA\n      HoltWinters\n      CrostonClassic\n      SeasonalNaive\n      HistoricAverage\n      DynamicOptimizedTheta\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      701\n      700\n      619.0\n      603.925415\n      847.0\n      742.668762\n      691.0\n      661.674988\n      612.767517\n    \n    \n      H1\n      702\n      700\n      565.0\n      507.591736\n      820.0\n      742.668762\n      618.0\n      661.674988\n      536.846252\n    \n    \n      H1\n      703\n      700\n      532.0\n      481.281677\n      790.0\n      742.668762\n      563.0\n      661.674988\n      497.824280\n    \n    \n      H1\n      704\n      700\n      495.0\n      444.410248\n      784.0\n      742.668762\n      529.0\n      661.674988\n      464.723236\n    \n    \n      H1\n      705\n      700\n      481.0\n      421.168762\n      752.0\n      742.668762\n      504.0\n      661.674988\n      440.972351\n    \n  \n\n\n\n\nNext, we will evaluate the performance of every model for every series using common error metrics like Mean Absolute Error (MAE) or Mean Square Error (MSE) Define a utility function to evaluate different error metrics for the cross validation data frame.\nFirst import the desired error metrics from datasetsforecast.losses. Then define a utility function that takes a cross-validation data frame as a metric and returns an evaluation data frame with the average of the error metric for every unique id and fitted model and all cutoffs.\n\nfrom datasetsforecast.losses import mse, mae, rmse\n\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    for model in models:\n        eval_ = df.groupby(['unique_id', 'cutoff']).apply(lambda x: metric(x['y'].values, x[model].values)).to_frame() # Calculate loss for every unique_id, model and cutoff.\n        eval_.columns = [model]\n        evals.append(eval_)\n    evals = pd.concat(evals, axis=1)\n    evals = evals.groupby(['unique_id']).mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = evaluate_cross_validation(crossvaldation_df, mse)\n\nevaluation_df.head()\n\n\n\n\n\n  \n    \n      \n      AutoARIMA\n      HoltWinters\n      CrostonClassic\n      SeasonalNaive\n      HistoricAverage\n      DynamicOptimizedTheta\n      best_model\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      1979.302246\n      44888.019531\n      28038.736328\n      1422.666748\n      20927.664062\n      1296.333984\n      DynamicOptimizedTheta\n    \n    \n      H10\n      458.892700\n      2812.916504\n      1483.484131\n      96.895828\n      1980.367432\n      379.621124\n      SeasonalNaive\n    \n    \n      H100\n      8629.948242\n      121625.375000\n      91945.140625\n      12019.000000\n      78491.187500\n      21699.648438\n      AutoARIMA\n    \n    \n      H101\n      6818.348633\n      28453.394531\n      16183.634766\n      10944.458008\n      18208.404297\n      63698.074219\n      AutoARIMA\n    \n    \n      H102\n      65489.968750\n      232924.843750\n      132655.296875\n      12699.896484\n      309110.468750\n      31393.521484\n      SeasonalNaive\n    \n  \n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best. In this case, the Arima and Seasonal Naive are the best models for 10 series and the Theta model should be used for two.\n\nsummary_df = evaluation_df.groupby('best_model').size().sort_values().to_frame()\n\nsummary_df.reset_index().columns = [\"Model\", \"Nr. of unique_ids\"]\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nseasonal_ids = evaluation_df.query('best_model == \"SeasonalNaive\"').index\n\nsf.plot(Y_df,forecasts_df, unique_ids=seasonal_ids, models=[\"SeasonalNaive\",\"DynamicOptimizedTheta\"])"
  },
  {
    "objectID": "examples/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "href": "examples/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "title": "End to End Walkthrough",
    "section": "Select the best model for every unique series",
    "text": "Select the best model for every unique series\nDefine a utility function that takes your forecast’s data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df):\n    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n    df.columns = ['model', 'best_model_forecast'] \n    df = df.join(evaluation_df[['best_model']])\n    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n    df.columns = df.columns.droplevel()\n    df = df.reset_index(level=1)\n    return df\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(forecasts_df, evaluation_df)\n\nprod_forecasts_df.head()\n\n\n\n\n\n  \n    \n      model\n      ds\n      best_model\n      best_model-hi-90\n      best_model-lo-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      749\n      592.701843\n      611.652649\n      577.677307\n    \n    \n      H1\n      750\n      525.589111\n      546.621826\n      505.449738\n    \n    \n      H1\n      751\n      489.251801\n      512.424133\n      462.072876\n    \n    \n      H1\n      752\n      456.195038\n      478.260956\n      430.554291\n    \n    \n      H1\n      753\n      436.290527\n      461.815948\n      411.051239\n    \n  \n\n\n\n\nPlot the results.\n\nsf.plot(Y_df, prod_forecasts_df, level=[90])"
  },
  {
    "objectID": "arima.html",
    "href": "arima.html",
    "title": "ARIMA",
    "section": "",
    "text": "source\n\npredict_arima\n\n predict_arima (model, n_ahead, newxreg=None, se_fit=True)\n\n\nmyarima(ap, order=(2, 1, 1), seasonal={'order': (0, 1, 0), 'period': 12}, \n        constant=False, ic='aicc', method='CSS-ML')['aic']\n\n\nsource\n\n\narima_string\n\n arima_string (model, padding=False)\n\n\nsource\n\n\nforecast_arima\n\n forecast_arima (model, h=None, level=None, fan=False, xreg=None,\n                 blambda=None, bootstrap=False, npaths=5000, biasadj=None)\n\n\nsource\n\n\nfitted_arima\n\n fitted_arima (model, h=1)\n\nReturns h-step forecasts for the data used in fitting the model.\n\nsource\n\n\nauto_arima_f\n\n auto_arima_f (x, d=None, D=None, max_p=5, max_q=5, max_P=2, max_Q=2,\n               max_order=5, max_d=2, max_D=1, start_p=2, start_q=2,\n               start_P=1, start_Q=1, stationary=False, seasonal=True,\n               ic='aicc', stepwise=True, nmodels=94, trace=False,\n               approximation=None, method=None, truncate=None, xreg=None,\n               test='kpss', test_kwargs=None, seasonal_test='seas',\n               seasonal_test_kwargs=None, allowdrift=True, allowmean=True,\n               blambda=None, biasadj=False, parallel=False, num_cores=2,\n               period=1)\n\n\nsource\n\n\nprint_statsforecast_ARIMA\n\n print_statsforecast_ARIMA (model, digits=3, se=True)\n\n\nsource\n\n\nARIMASummary\n\n ARIMASummary (model)\n\nARIMA Summary.\n\nsource\n\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=None, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=True, allowmean:bool=True,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            parallel:bool=False, num_cores:int=2, period:int=1)\n\nAn AutoARIMA estimator.\nReturns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.core.html",
    "href": "distributed.core.html",
    "title": "Core",
    "section": "",
    "text": "source\n\nParallelBackend\n\n ParallelBackend ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "distributed.utils.html",
    "href": "distributed.utils.html",
    "title": "Distributed utils",
    "section": "",
    "text": "source\n\nforecast\n\n forecast (df, models, freq, h, fallback_model=None, X_df=None,\n           level=None,\n           parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\nsource\n\n\ncross_validation\n\n cross_validation (df, models, freq, h, n_windows=1, step_size=1,\n                   test_size=None, input_size=None,\n                   parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "ces.html",
    "href": "ces.html",
    "title": "CES Model",
    "section": "",
    "text": "source\n\n\n\n ces_target_fn (optimal_param, init_alpha_0, init_alpha_1, init_beta_0,\n                init_beta_1, opt_alpha_0, opt_alpha_1, opt_beta_0,\n                opt_beta_1, y, m, init_states, n_components, seasontype,\n                nmse)\nGive us a ⭐ on Github"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "StatsForecast Blog",
    "section": "",
    "text": "Scalable Time Series Modeling with open-source projects\n\n\n\n\n\nHow to Forecast 1M Time Series in 15 Minutes with Spark, Fugue and Nixtla’s Statsforecast.\n\n\n\n\n\n\nOct 5, 2022\n\n\nFugue, Nixtla\n\n\n\n\n\n\nNo matching items\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "",
    "text": "By Fugue and Nixtla. Originally posted on TDS.\nTime-series modeling, analysis, and prediction of trends and seasonalities for data collected over time is a rapidly growing category of software applications.\nBusinesses, from electricity and economics to healthcare analytics, collect time-series data daily to predict patterns and build better data-driven product experiences. For example, temperature and humidity prediction is used in manufacturing to prevent defects, streaming metrics predictions help identify music’s popular artists, and sales forecasting for thousands of SKUs across different locations in the supply chain is used to optimize inventory costs. As data generation increases, the forecasting necessities have evolved from modeling a few time series to predicting millions.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Motivation",
    "text": "Motivation\nNixtla is an open-source project focused on state-of-the-art time series forecasting. They have a couple of libraries such as StatsForecast for statistical models, NeuralForecast for deep learning, and HierarchicalForecast for forecast aggregations across different levels of hierarchies. These are production-ready time series libraries focused on different modeling techniques.\nThis article looks at StatsForecast, a lightning-fast forecasting library with statistical and econometrics models. The AutoARIMA model of Nixtla is 20x faster than pmdarima, and the ETS (error, trend, seasonal) models performed 4x faster than statsmodels and are more robust. The benchmarks and code to reproduce can be found here. A huge part of the performance increase is due to using a JIT compiler called numba to achieve high speeds.\nThe faster iteration time means that data scientists can run more experiments and converge to more accurate models faster. It also means that running benchmarks at scale becomes easier.\nIn this article, we are interested in the scalability of the StatsForecast library in fitting models over Spark or Dask using the Fugue library. This combination will allow us to train a huge number of models distributedly over a temporary cluster quickly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Experiment Setup",
    "text": "Experiment Setup\nWhen dealing with large time series data, users normally have to deal with thousands of logically independent time series (think of telemetry of different users or different product sales). In this case, we can train one big model over all of the series, or we can create one model for each series. Both are valid approaches since the bigger model will pick up trends across the population, while training thousands of models may fit individual series data better.\n\n\n\n\n\n\nNote\n\n\n\nNote: to pick up both the micro and macro trends of the time series population in one model, check the Nixtla HierarchicalForecast library, but this is also more computationally expensive and trickier to scale.\n\n\nThis article will deal with the scenario where we train a couple of models (AutoARIMA or ETS) per univariate time series. For this setup, we group the full data by time series, and then train each model for each group. The image below illustrates this. The distributed DataFrame can either be a Spark or Dask DataFrame.\n\n\n\nAutoARIMA per partition\n\n\nNixtla previously released benchmarks with Anyscale on distributing this model training on Ray. The setup and results can be found in this blog. The results are also shown below. It took 2000 cpus to run one million AutoARIMA models in 35 minutes. We’ll compare this against running on Spark.\n\n\n\nStatsForecast on Ray results"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "StatsForecast code",
    "text": "StatsForecast code\nFirst, we’ll look at the StatsForecast code used to run the AutoARIMA distributedly on Ray. This is a simplified version to run the scenario with a one million time series. It is also updated for the recent StatsForecast v1.0.0 release, so it may look a bit different from the code in the previous benchmarks.\nfrom time import time\n\nimport pandas as pd\nfrom statsforecast.utils import generate_series\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nseries = generate_series(n_series=1000000, seed=1)\n\nmodel = StatsForecast(df=series,\n                      models=[AutoARIMA()], \n                      freq='D', \n                      n_jobs=-1,\n              ray_address=ray_address)\n\ninit = time()\nforecasts = model.forecast(7)\nprint(f'n_series: 1000000 total time: {(time() - init) / 60}')\nThe interface of StatsForecast is very minimal. It is already designed to perform the AutoARIMA on each group of data. Just supplying the ray_address will make this code snippet run distributedly. Without it, n_jobswill indicate the number of parallel processes for forecasting. model.forecast() will do the fit and predict in one step, and the input to this method in the time horizon to forecast."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Using Fugue to run on Spark and Dask",
    "text": "Using Fugue to run on Spark and Dask\nFugue is an abstraction layer that ports Python, Pandas, and SQL code to Spark and Dask. The most minimal interface is the transform() function. This function takes in a function and DataFrame, and brings it to Spark or Dask. We can use the transform() function to bring StatsForecast execution to Spark.\nThere are two parts to the code below. First, we have the forecast logic defined in the forecast_series function. Some parameters are hardcoded for simplicity. The most important one is that n_jobs=1. This is because Spark or Dask will already serve as the parallelization layer, and having two stages of parallelism can cause resource deadlocks.\nfrom fugue import transform\n\ndef forecast_series(df: pd.DataFrame, models) -> pd.DataFrame:\n    tdf = df.set_index(\"unique_id\")\n    model = StatsForecast(df=tdf, models=models, freq='D', n_jobs=1)\n    return model.forecast(7).reset_index()\n\ntransform(series.reset_index(),\n          forecast_series,\n          params=dict(models=[AutoARIMA()]),\n          schema=\"unique_id:int, ds:date, AutoARIMA:float\",\n          partition={\"by\": \"unique_id\"},\n          engine=\"spark\"\n          ).show()\nSecond, the transform() function is used to apply the forecast_series() function on Spark. The first two arguments are the DataFrame and function to be applied. Output schema is a requirement for Spark, so we need to pass it in, and the partition argument will take care of splitting the time series modelling by unique_id.\nThis code already works and returns a Spark DataFrame output."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Nixtla’s FugueBackend",
    "text": "Nixtla’s FugueBackend\nThe transform() above is a general look at what Fugue can do. In practice, the Fugue and Nixtla teams collaborated to add a more native FugueBackend to the StatsForecast library. Along with it is a utility forecast() function to simplify the forecasting interface. Below is an end-to-end example of running StatsForecast on one million time series.\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\nforecast(spark.read.parquet(\"/tmp/1m.parquet\"), \n         [AutoARIMA()], \n         freq=\"D\", \n         h=7, \n         parallel=backend).toPandas()\nWe just need to create the FugueBackend, which takes in a SparkSession and passes it to forecast(). This function can take either a DataFrame or file path to the data. If a file path is provided, it will be loaded with the parallel backend. In this example above, we replaced the file each time we ran the experiment to generate benchmarks.\n\n\n\n\n\n\nDanger\n\n\n\nIt’s also important to note that we can test locally before running the forecast() on full data. All we have to do is not supply anything for the parallel argument; everything will run on Pandas sequentially."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Benchmark Results",
    "text": "Benchmark Results\nThe benchmark results can be seen below. As of the time of this writing, Dask and Ray made recent releases, so only the Spark metrics are up to date. We will make a follow-up article after running these experiments with the updates.\n\n\n\nSpark and Dask benchmarks for StatsForecast at scale\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The attempt was to use 2000 cpus but we were limited by available compute instances on AWS.\n\n\nThe important part here is that AutoARIMA trained one million time series models in less than 15 minutes. The cluster configuration is attached in the appendix. With very few lines of code, we were able to orchestrate the training of these time series models distributedly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Conclusion",
    "text": "Conclusion\nTraining thousands of time series models distributedly normally takes a lot of coding with Spark and Dask, but we were able to run these experiments with very few lines of code. Nixtla’s StatsForecast offers the ability to quickly utilize all of the compute resources available to find the best model for each time series. All users need to do is supply a relevant parallel backend (Ray or Fugue) to run on a cluster.\nOn the scale of one million timeseries, our total training time took 12 minutes for AutoARIMA. This is the equivalent of close to 400 cpu-hours that we ran immediately, allowing data scientists to quickly iterate at scale without having to write the explicit code for parallelization. Because we used an ephemeral cluster, the cost is effectively the same as running this sequentially on an EC2 instance (parallelized over all cores)."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Resources",
    "text": "Resources\n\nNixtla StatsForecast repo\nStatsForecast docs\nFugue repo\nFugue tutorials\n\nTo chat with us:\n\nFugue Slack\nNixtla Slack"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Appendix",
    "text": "Appendix\nFor anyone. interested in the cluster configuration, it can be seen below. This will spin up a Databricks cluster. The important thing is the node_type_id that has the machines used.\n{\n    \"num_workers\": 20,\n    \"cluster_name\": \"fugue-nixtla-2\",\n    \"spark_version\": \"10.4.x-scala2.12\",\n    \"spark_conf\": {\n        \"spark.speculation\": \"true\",\n        \"spark.sql.shuffle.partitions\": \"8000\",\n        \"spark.sql.adaptive.enabled\": \"false\",\n        \"spark.task.cpus\": \"1\"\n    },\n    \"aws_attributes\": {\n        \"first_on_demand\": 1,\n        \"availability\": \"SPOT_WITH_FALLBACK\",\n        \"zone_id\": \"us-west-2c\",\n        \"spot_bid_price_percent\": 100,\n        \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n        \"ebs_volume_count\": 1,\n        \"ebs_volume_size\": 32\n    },\n    \"node_type_id\": \"m5.24xlarge\",\n    \"driver_node_type_id\": \"m5.2xlarge\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"MKL_NUM_THREADS\": \"1\",\n        \"OPENBLAS_NUM_THREADS\": \"1\",\n        \"VECLIB_MAXIMUM_THREADS\": \"1\",\n        \"OMP_NUM_THREADS\": \"1\",\n        \"NUMEXPR_NUM_THREADS\": \"1\"\n    },\n    \"autotermination_minutes\": 20,\n    \"enable_elastic_disk\": false,\n    \"cluster_source\": \"UI\",\n    \"init_scripts\": [],\n    \"runtime_engine\": \"STANDARD\",\n    \"cluster_id\": \"0728-004950-oefym0ss\"\n}"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core Methods",
    "section": "",
    "text": "The core methods of StatsForecast are:\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "core.html#statsforecast",
    "href": "core.html#statsforecast",
    "title": "Core Methods",
    "section": "StatsForecast",
    "text": "StatsForecast\n\n StatsForecast (models:List[Any], freq:str, n_jobs:int=1,\n                ray_address:Optional[str]=None,\n                df:Optional[pandas.core.frame.DataFrame]=None,\n                sort_df:bool=True, fallback_model:Optional[Any]=None,\n                verbose:bool=False, backend:Optional[Any]=None)\n\nTrain statistical models.\nThe StatsForecast class allows you to efficiently fit multiple StatsForecast models for large sets of time series. It operates with pandas DataFrame df that identifies series and datestamps with the unique_id and ds columns. The y column denotes the target time series variable.\nThe class has memory-efficient StatsForecast.forecast method that avoids storing partial model outputs. While the StatsForecast.fit and StatsForecast.predict methods with Scikit-learn interface store the fitted models.\nThe StatsForecast class offers parallelization utilities with Dask, Spark and Ray back-ends. See distributed computing example here.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodels\ntyping.List[typing.Any]\n\nList of instantiated objects models.StatsForecast.\n\n\nfreq\nstr\n\nFrequency of the data.See panda’s available frequencies.\n\n\nn_jobs\nint\n1\nNumber of jobs used in the parallel processing, use -1 for all cores.\n\n\nray_address\ntyping.Optional[str]\nNone\nRay address for distributed processing.\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nfallback_model\ntyping.Optional[typing.Any]\nNone\nModel to be used if a model fails. Only works with the forecast and cross_validation methods.\n\n\nverbose\nbool\nFalse\nPrints TQDM progress bar when n_jobs=1.\n\n\n\n\n# StatsForecast's class usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import generate_series\nfrom statsforecast.models import ( \n    ADIDA,\n    AutoARIMA,\n    CrostonClassic,\n    CrostonOptimized,\n    CrostonSBA,\n    ETS,\n    HistoricAverage,\n    IMAPA,\n    Naive,\n    RandomWalkWithDrift,\n    SeasonalExponentialSmoothing,\n    SeasonalNaive,\n    SeasonalWindowAverage,\n    SimpleExponentialSmoothing,\n    TSB,\n    WindowAverage,\n    DynamicOptimizedTheta,\n    AutoETS,\n    AutoCES\n)\n\n# Generate synthetic panel DataFrame for example\npanel_df = generate_series(n_series=9, equal_ends=False)\npanel_df.groupby('unique_id').tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      y\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      0\n      2000-08-06\n      1.212726\n    \n    \n      0\n      2000-08-07\n      2.442669\n    \n    \n      0\n      2000-08-08\n      3.339940\n    \n    \n      0\n      2000-08-09\n      4.228065\n    \n    \n      1\n      2000-04-03\n      0.048275\n    \n    \n      1\n      2000-04-04\n      1.128070\n    \n    \n      1\n      2000-04-05\n      2.295968\n    \n    \n      1\n      2000-04-06\n      3.238239\n    \n    \n      2\n      2000-06-12\n      6.480128\n    \n    \n      2\n      2000-06-13\n      0.036217\n    \n    \n      2\n      2000-06-14\n      1.009650\n    \n    \n      2\n      2000-06-15\n      2.489787\n    \n    \n      3\n      2000-08-26\n      3.289840\n    \n    \n      3\n      2000-08-27\n      4.227949\n    \n    \n      3\n      2000-08-28\n      5.321176\n    \n    \n      3\n      2000-08-29\n      6.127013\n    \n    \n      4\n      2001-01-04\n      5.403709\n    \n    \n      4\n      2001-01-05\n      6.081779\n    \n    \n      4\n      2001-01-06\n      0.438420\n    \n    \n      4\n      2001-01-07\n      1.386855\n    \n    \n      5\n      2000-10-24\n      5.011166\n    \n    \n      5\n      2000-10-25\n      6.397153\n    \n    \n      5\n      2000-10-26\n      0.462146\n    \n    \n      5\n      2000-10-27\n      1.253125\n    \n    \n      6\n      2000-08-29\n      5.407805\n    \n    \n      6\n      2000-08-30\n      6.340789\n    \n    \n      6\n      2000-08-31\n      0.202894\n    \n    \n      6\n      2000-09-01\n      1.491204\n    \n    \n      7\n      2001-02-09\n      1.068102\n    \n    \n      7\n      2001-02-10\n      2.233974\n    \n    \n      7\n      2001-02-11\n      3.484143\n    \n    \n      7\n      2001-02-12\n      4.176505\n    \n    \n      8\n      2000-02-25\n      4.110373\n    \n    \n      8\n      2000-02-26\n      5.483879\n    \n    \n      8\n      2000-02-27\n      6.068916\n    \n    \n      8\n      2000-02-28\n      0.040499\n    \n  \n\n\n\n\n\n# Declare list of instantiated StatsForecast estimators to be fitted\n# You can try other estimator's hyperparameters\n# You can try other methods from the `models.StatsForecast` collection\n# Check them here: https://nixtla.github.io/statsforecast/models.html\nmodels=[AutoARIMA(), Naive(), \n        ETS(), AutoARIMA(allowmean=True, alias='MeanAutoARIMA')] \n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=models,\n                     freq='D', \n                     n_jobs=1, \n                     verbose=True)\n\n# Efficiently predict\nfcsts_df = fcst.forecast(h=4, fitted=True)\nfcsts_df.groupby('unique_id').tail(4)\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.fit",
    "href": "core.html#statsforecast.fit",
    "title": "Core Methods",
    "section": "StatsForecast.fit",
    "text": "StatsForecast.fit\n\n StatsForecast.fit (df:Optional[pandas.core.frame.DataFrame]=None,\n                    sort_df:bool=True)\n\nFit statistical models.\nFit models to a large set of time series from DataFrame df and store fitted models for later inspection.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nReturns\nStatsForecast\n\nReturns with stored StatsForecast fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#satstforecast.predict",
    "href": "core.html#satstforecast.predict",
    "title": "Core Methods",
    "section": "SatstForecast.predict",
    "text": "SatstForecast.predict\n\n SatstForecast.predict (h:int,\n                        X_df:Optional[pandas.core.frame.DataFrame]=None,\n                        level:Optional[List[int]]=None)\n\nPredict statistical models.\nUse stored fitted models to predict large set of time series from DataFrame df.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with [unique_id, ds] columns and df’s future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nReturns\npandas.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.fit_predict",
    "href": "core.html#statsforecast.fit_predict",
    "title": "Core Methods",
    "section": "StatsForecast.fit_predict",
    "text": "StatsForecast.fit_predict\n\n StatsForecast.fit_predict (h:int,\n                            df:Optional[pandas.core.frame.DataFrame]=None,\n                            X_df:Optional[pandas.core.frame.DataFrame]=Non\n                            e, level:Optional[List[int]]=None,\n                            sort_df:bool=True)\n\nFit and Predict with statistical models.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\nIn contrast to StatsForecast.forecast this method stores partial models outputs.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous variables.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nX_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with [unique_id, ds] columns and df’s future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nReturns\npandas.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.forecast",
    "href": "core.html#statsforecast.forecast",
    "title": "Core Methods",
    "section": "StatsForecast.forecast",
    "text": "StatsForecast.forecast\n\n StatsForecast.forecast (h:int,\n                         df:Optional[pandas.core.frame.DataFrame]=None,\n                         X_df:Optional[pandas.core.frame.DataFrame]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False, sort_df:bool=True)\n\nMemory Efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nX_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with [unique_id, ds] columns and df’s future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nfitted\nbool\nFalse\nWether or not return insample predictions.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nReturns\npandas.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\n# StatsForecast.forecast method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import AutoARIMA, Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA(), Naive()],\n                     freq='D', n_jobs=1)\n\n# Efficiently predict without storing memory\nfcsts_df = fcst.forecast(h=4, fitted=True)\nfcsts_df.groupby('unique_id').tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1961-01-01\n      476.006500\n      432.0\n    \n    \n      1.0\n      1961-01-02\n      482.846222\n      432.0\n    \n    \n      1.0\n      1961-01-03\n      512.423523\n      432.0\n    \n    \n      1.0\n      1961-01-04\n      502.038269\n      432.0\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.forecast_fitted_values",
    "href": "core.html#statsforecast.forecast_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.forecast_fitted_values",
    "text": "StatsForecast.forecast_fitted_values\n\n StatsForecast.forecast_fitted_values ()\n\nAccess insample predictions.\nAfter executing StatsForecast.forecast, you can access the insample prediction values for each model. To get them, you need to pass fitted=True to the StatsForecast.forecast method and then use the StatsForecast.forecast_fitted_values method.\n\n# StatsForecast.forecast_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nfcsts_df = fcst.forecast(h=12, fitted=True, level=(90, 10))\ninsample_fcsts_df = fcst.forecast_fitted_values()\ninsample_fcsts_df.tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      y\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-lo-10\n      AutoARIMA-hi-10\n      AutoARIMA-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1960-09-30\n      508.0\n      572.654175\n      525.092163\n      569.020630\n      576.287781\n      620.216187\n    \n    \n      1.0\n      1960-10-31\n      461.0\n      451.528259\n      403.966248\n      447.894684\n      455.161835\n      499.090271\n    \n    \n      1.0\n      1960-11-30\n      390.0\n      437.915375\n      390.353363\n      434.281799\n      441.548981\n      485.477386\n    \n    \n      1.0\n      1960-12-31\n      432.0\n      369.718781\n      322.156769\n      366.085205\n      373.352356\n      417.280792\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.cross_validation",
    "href": "core.html#statsforecast.cross_validation",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation",
    "text": "StatsForecast.cross_validation\n\n StatsForecast.cross_validation (h:int,\n                                 df:Optional[pandas.core.frame.DataFrame]=\n                                 None, n_windows:int=1, step_size:int=1,\n                                 test_size:Optional[int]=None,\n                                 input_size:Optional[int]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False, refit:bool=True,\n                                 sort_df:bool=True)\n\nTemporal Cross-Validation.\nEfficiently fits a list of StatsForecast models through multiple training windows, in either chained or rolled manner.\nStatsForecast.models’ speed allows to overcome this evaluation technique high computational costs. Temporal cross-validation provides better model’s generalization measurements by increasing the test’s length and diversity.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nn_windows\nint\n1\nNumber of windows used for cross validation.\n\n\nstep_size\nint\n1\nStep size between each window.\n\n\ntest_size\ntyping.Optional[int]\nNone\nLength of test size. If passed, set n_windows=None.\n\n\ninput_size\ntyping.Optional[int]\nNone\nInput size for each window, if not none rolled windows.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nfitted\nbool\nFalse\nWether or not returns insample predictions.\n\n\nrefit\nbool\nTrue\nWether or not refit the model for each window.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by unique_id and ds.\n\n\nReturns\npandas.DataFrame\n\nDataFrame with insample models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\n# StatsForecast.crossvalidation method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1, verbose=True)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(14, n_windows=2)\nrolled_fcsts_df.head(4)\n\nCross Validation Time Series 1:   0%|          | 0/2 [00:00<?, ?it/s]Cross Validation Time Series 1: 100%|##########| 2/2 [00:00<00:00, 7958.83it/s]\n\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1960-12-17\n      1960-12-16\n      407.0\n      463.0\n    \n    \n      1.0\n      1960-12-18\n      1960-12-16\n      362.0\n      463.0\n    \n    \n      1.0\n      1960-12-19\n      1960-12-16\n      405.0\n      463.0\n    \n    \n      1.0\n      1960-12-20\n      1960-12-16\n      417.0\n      463.0\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.cross_validation_fitted_values",
    "href": "core.html#statsforecast.cross_validation_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation_fitted_values",
    "text": "StatsForecast.cross_validation_fitted_values\n\n StatsForecast.cross_validation_fitted_values ()\n\nAccess insample cross validated predictions.\nAfter executing StatsForecast.cross_validation, you can access the insample prediction values for each model and window. To get them, you need to pass fitted=True to the StatsForecast.cross_validation method and then use the StatsForecast.cross_validation_fitted_values method.\n\n# StatsForecast.cross_validation_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(h=12, n_windows=2, fitted=True)\ninsample_rolled_fcsts_df = fcst.cross_validation_fitted_values()\ninsample_rolled_fcsts_df.tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1959-09-30\n      1959-12-31\n      463.0\n      559.0\n    \n    \n      1.0\n      1959-10-31\n      1959-12-31\n      407.0\n      463.0\n    \n    \n      1.0\n      1959-11-30\n      1959-12-31\n      362.0\n      407.0\n    \n    \n      1.0\n      1959-12-31\n      1959-12-31\n      405.0\n      362.0\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.plot",
    "href": "core.html#statsforecast.plot",
    "title": "Core Methods",
    "section": "StatsForecast.plot",
    "text": "StatsForecast.plot\n\n StatsForecast.plot (df:pandas.core.frame.DataFrame,\n                     forecasts_df:Optional[pandas.core.frame.DataFrame]=No\n                     ne, unique_ids:Union[List[str],NoneType,numpy.ndarray\n                     ]=None, plot_random:bool=True,\n                     models:Optional[List[str]]=None,\n                     level:Optional[List[float]]=None,\n                     max_insample_length:Optional[int]=None,\n                     plot_anomalies:bool=False, engine:str='plotly',\n                     resampler_kwargs:Optional[Dict]=None)\n\nPlot forecasts and insample values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataFrame with columns [unique_id, ds, y].\n\n\nforecasts_df\ntyping.Optional[pandas.core.frame.DataFrame]\nNone\nDataFrame with columns [unique_id, ds] and models.\n\n\nunique_ids\ntyping.Union[typing.List[str], NoneType, numpy.ndarray]\nNone\nTime Series to plot.If None, time series are selected randomly.\n\n\nplot_random\nbool\nTrue\nSelect time series to plot randomly.\n\n\nmodels\ntyping.Optional[typing.List[str]]\nNone\nList of models to plot.\n\n\nlevel\ntyping.Optional[typing.List[float]]\nNone\nList of prediction intervals to plot if paseed.\n\n\nmax_insample_length\ntyping.Optional[int]\nNone\nMax number of train/insample observations to be plotted.\n\n\nplot_anomalies\nbool\nFalse\nPlot anomalies for each prediction interval.\n\n\nengine\nstr\nplotly\nLibrary used to plot. ‘plotly’, ‘plotly-resampler’ or ‘matplotlib’.\n\n\nresampler_kwargs\ntyping.Optional[typing.Dict]\nNone\nKwargs to be passed to plotly-resampler constructor. kwargs for plotly-resampler .show_dash method can be passed as sub-dictionary under the “show_dash” key."
  },
  {
    "objectID": "core.html#integer-datestamp",
    "href": "core.html#integer-datestamp",
    "title": "Core Methods",
    "section": "Integer datestamp",
    "text": "Integer datestamp\nThe StatsForecast class can also receive integers as datestamp, the following example shows how to do it.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengers as ap\nfrom statsforecast.models import HistoricAverage\n\n\nint_ds_df = pd.DataFrame({'ds': np.arange(1, len(ap) + 1), 'y': ap})\nint_ds_df.insert(0, 'unique_id', 'AirPassengers')\nint_ds_df.set_index('unique_id', inplace=True)\nint_ds_df.head()\n\n\nint_ds_df.tail()\n\n\nfcst = StatsForecast(df=int_ds_df, models=[HistoricAverage()], freq='D')\nhorizon = 7\nforecast = fcst.forecast(horizon)\nforecast.head()\n\n\nlast_date = int_ds_df['ds'].max()\ntest_eq(forecast['ds'].values, np.arange(last_date + 1, last_date + 1 + horizon))\n\n\nint_ds_cv = fcst.cross_validation(h=7, test_size=8, n_windows=None)\nint_ds_cv"
  },
  {
    "objectID": "core.html#external-regressors",
    "href": "core.html#external-regressors",
    "title": "Core Methods",
    "section": "External regressors",
    "text": "External regressors\nEvery column after y is considered an external regressor and will be passed to the models that allow them. If you use them you must supply the future values to the StatsForecast.forecast method.\n\nclass LinearRegression:\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, y, X):\n        self.coefs_, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return self\n    \n    def predict(self, h, X):\n        mean = X @ coefs\n        return mean\n    \n    def __repr__(self):\n        return 'LinearRegression()'\n    \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        coefs, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return {'mean': X_future @ coefs}\n    \n    def new(self):\n        b = type(self).__new__(type(self))\n        b.__dict__.update(self.__dict__)\n        return b\n\n\nseries_xreg = series = generate_series(10_000, equal_ends=True)\nseries_xreg['intercept'] = 1\nseries_xreg['dayofweek'] = series_xreg['ds'].dt.dayofweek\nseries_xreg = pd.get_dummies(series_xreg, columns=['dayofweek'], drop_first=True)\nseries_xreg\n\n\ndates = sorted(series_xreg['ds'].unique())\nvalid_start = dates[-14]\ntrain_mask = series_xreg['ds'] < valid_start\nseries_train = series_xreg[train_mask]\nseries_valid = series_xreg[~train_mask]\nX_valid = series_valid.drop(columns=['y'])\nfcst = StatsForecast(\n    df=series_train,\n    models=[LinearRegression()],\n    freq='D',\n)\nxreg_res = fcst.forecast(14, X_df=X_valid)\nxreg_res['y'] = series_valid['y'].values\n\n\nxreg_res.groupby('ds').mean().plot()\n\n\nxreg_res_cv = fcst.cross_validation(h=3, test_size=5, n_windows=None)"
  },
  {
    "objectID": "core.html#confidence-intervals",
    "href": "core.html#confidence-intervals",
    "title": "Core Methods",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nYou can pass the argument level to the StatsForecast.forecast method to calculate confidence intervals. Not all models can calculate them at the moment, so we will only obtain the intervals of those models that have it implemented.\n\nap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap}, index=pd.Index([0] * ap.size, name='unique_id'))\nfcst = StatsForecast(\n    df=ap_df,\n    models=[\n        SeasonalNaive(season_length=12), \n        AutoARIMA(season_length=12)\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = fcst.forecast(12, level=(80, 95))\nap_ci.set_index('ds').plot(marker='.', figsize=(10, 6))\n\n\n#hide\ndef test_conf_intervals(n_jobs=1):\n    ap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap}, index=pd.Index([0] * ap.size, name='unique_id'))\n    fcst = StatsForecast(\n        df=ap_df,\n        models=[\n            SeasonalNaive(season_length=12), \n            AutoARIMA(season_length=12)\n        ],\n        freq='M',\n        n_jobs=n_jobs\n    )\n    ap_ci = fcst.forecast(12, level=(80, 95))\n    ap_ci.set_index('ds').plot(marker='.', figsize=(10, 6))\ntest_conf_intervals(n_jobs=1)"
  },
  {
    "objectID": "adapters.prophet.html",
    "href": "adapters.prophet.html",
    "title": "Replace FB-Prophet",
    "section": "",
    "text": "source\n\n\n\n AutoARIMAProphet (growth='linear', changepoints=None, n_changepoints=25,\n                   changepoint_range=0.8, yearly_seasonality='auto',\n                   weekly_seasonality='auto', daily_seasonality='auto',\n                   holidays=None, seasonality_mode='additive',\n                   seasonality_prior_scale=10.0,\n                   holidays_prior_scale=10.0,\n                   changepoint_prior_scale=0.05, mcmc_samples=0,\n                   interval_width=0.8, uncertainty_samples=1000,\n                   stan_backend=None, d=None, D=None, max_p=5, max_q=5,\n                   max_P=2, max_Q=2, max_order=5, max_d=2, max_D=1,\n                   start_p=2, start_q=2, start_P=1, start_Q=1,\n                   stationary=False, seasonal=True, ic='aicc',\n                   stepwise=True, nmodels=94, trace=False,\n                   approximation=False, method=None, truncate=None,\n                   test='kpss', test_kwargs=None, seasonal_test='seas',\n                   seasonal_test_kwargs=None, allowdrift=False,\n                   allowmean=False, blambda=None, biasadj=False,\n                   parallel=False, num_cores=2, period=1)\n\nAutoARIMAProphet adapter.\nReturns best ARIMA model using external variables created by the Prophet interface. This class receives as parameters the same as prophet.Prophet and uses a models.AutoARIMA backend.\nIf your forecasting pipeline uses Prophet the AutoARIMAProphet adapter helps to easily substitute Prophet with an AutoARIMA.\nParameters: growth: String ‘linear’, ‘logistic’ or ‘flat’ to specify a linear, logistic or flat trend. changepoints: List of dates of potential changepoints. Otherwise selected automatically. n_changepoints: Number of potential changepoints to include. changepoint_range: Proportion of history in which trend changepoints will be estimated. yearly_seasonality: Fit yearly seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. weekly_seasonality: Fit weekly seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. daily_seasonality: Fit daily seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. holidays: pandas.DataFrame with columns holiday (string) and ds (date type). interval_width: float, uncertainty forecast intervals width. StatsForecast’s level \nNotes: You can create automated exogenous variables from the Prophet data processing pipeline these exogenous will be included into AutoARIMA’s exogenous features. Parameters like seasonality_mode, seasonality_prior_scale, holidays_prior_scale, changepoint_prior_scale, mcmc_samples, uncertainty_samples, stan_backend are Prophet exclusive.\nReferences: Sean J. Taylor, Benjamin Letham (2017). “Prophet Forecasting at Scale”\nOskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, Ram Rajagopal (2021). “NeuralProphet: Explainable Forecasting at Scale”.\nRob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\n\nsource\n\n\n\n\n AutoARIMAProphet.fit (df, disable_seasonal_features=True, **kwargs)\n\nFit the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series. disable_seasonal_features: bool, Wheter disable Prophet’s seasonal features. kwargs: Additional arguments.\nReturns: self: AutoARIMAProphet adapter object with AutoARIMA fitted model.\n\nsource\n\n\n\n\n AutoARIMAProphet.predict (df=None)\n\nPredict using the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series.\nReturns: fcsts_df: A pandas.DataFrame with the forecast components.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "adapters.prophet.html#univariate-prophet",
    "href": "adapters.prophet.html#univariate-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.1 Univariate Prophet ",
    "text": "2.1 Univariate Prophet \nHere we forecast with Prophet without external regressors. We first instantiate a new Prophet object, and define its forecasting procedure into its constructor. After that a classic sklearn fit and predict is used to obtain the predictions.\n\nm = Prophet(daily_seasonality=False)\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nHere we forecast with AutoARIMAProphet adapter without external regressors. It inherits the Prophet constructor as well as its fit and predict methods.\nWith the class AutoARIMAProphet you can simply substitute Prophet and you’ll be training an AutoARIMA model without changing anything in your forecasting pipeline.\n\nm = AutoARIMAProphet(daily_seasonality=False)\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "adapters.prophet.html#holiday-prophet",
    "href": "adapters.prophet.html#holiday-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.2 Holiday Prophet ",
    "text": "2.2 Holiday Prophet \nUsually Prophet pipelines include the usage of external regressors such as holidays.\nSuppose you want to include holidays or other recurring calendar events, you can create a pandas.DataFrame for them. The DataFrame needs two columns [holiday, ds] and a row for each holiday. It requires all the occurrences of the holiday (as far as the historical data allows) and the future events of the holiday. If the future does not have the holidays registered, they will be modeled but not included in the forecast.\nYou can also include into the events DataFrame, lower_window and upper_window that extends the effect of the holidays through dates to [lower_window, upper_window] days around the date. For example if you wanted to account for Christmas Eve in addition to Christmas you’d include lower_window=-1,upper_window=0, or Black Friday in addition to Thanksgiving, you’d include lower_window=0,upper_window=1.\nHere we Peyton Manning’s playoff appearances dates:\n\nplayoffs = pd.DataFrame({\n  'holiday': 'playoff',\n  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n                        '2010-01-24', '2010-02-07', '2011-01-08',\n                        '2013-01-12', '2014-01-12', '2014-01-19',\n                        '2014-02-02', '2015-01-11', '2016-01-17',\n                        '2016-01-24', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nsuperbowls = pd.DataFrame({\n  'holiday': 'superbowl',\n  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nholidays = pd.concat((playoffs, superbowls))\n\n\nm = Prophet(daily_seasonality=False, holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nThe class AutoARIMAProphet adapter allows to handle these scenarios to fit an AutoARIMA model with exogenous variables.\nYou can enjoy your Prophet pipelines with the improved performance of a classic ARIMA.\n\nm = AutoARIMAProphet(daily_seasonality=False,\n                     holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  }
]